{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00a128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "import gymnasium  as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import scipy.stats as stats\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780afb92",
   "metadata": {},
   "source": [
    "<h1> Important Libraries To Install </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57601915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gym) (1.23.2)\n",
      "Requirement already satisfied: pyglet==1.5.27 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (1.5.27)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement stable-baseline3 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for stable-baseline3\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: gymnasium[all] in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (0.27.1)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (0.2.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (0.0.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (4.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (4.13.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (1.23.2)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (1.0.3)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.3.5)\n",
      "Requirement already satisfied: imageio>=2.14.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.26.0)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (3.7.1)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (0.2.1)\n",
      "Requirement already satisfied: swig==4.* in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (4.1.1)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (1.12.1)\n",
      "Requirement already satisfied: opencv-python>=3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (4.7.0.72)\n",
      "Requirement already satisfied: mujoco>=2.3.1.post1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.3.2)\n",
      "Requirement already satisfied: jax==0.3.24 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (0.3.24)\n",
      "Requirement already satisfied: mujoco-py<2.2,>=2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.1.2.14)\n",
      "Requirement already satisfied: pygame==2.1.3.dev8 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (2.1.3.dev8)\n",
      "Requirement already satisfied: jaxlib==0.3.24 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (0.3.24)\n",
      "Requirement already satisfied: lz4>=3.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from gymnasium[all]) (4.3.2)\n",
      "Requirement already satisfied: opt-einsum in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from jax==0.3.24->gymnasium[all]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from jax==0.3.24->gymnasium[all]) (1.10.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from imageio>=2.14.1->gymnasium[all]) (9.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium[all]) (3.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (23.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (0.11.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from matplotlib>=3.0->gymnasium[all]) (5.12.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from moviepy>=1.0.0->gymnasium[all]) (2.28.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from moviepy>=1.0.0->gymnasium[all]) (4.65.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.8)\n",
      "Requirement already satisfied: absl-py in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco>=2.3.1.post1->gymnasium[all]) (1.4.0)\n",
      "Requirement already satisfied: pyopengl in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco>=2.3.1.post1->gymnasium[all]) (3.1.6)\n",
      "Requirement already satisfied: glfw in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco>=2.3.1.post1->gymnasium[all]) (2.5.6)\n",
      "Requirement already satisfied: Cython>=0.27.2 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (0.29.33)\n",
      "Requirement already satisfied: cffi>=1.10 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.15.1)\n",
      "Requirement already satisfied: fasteners~=0.15 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (0.18)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (0.8.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/myenv/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install pyglet==1.5.27\n",
    "!pip install stable-baseline3\n",
    "!pip install \"gymnasium[all]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2826cd85",
   "metadata": {},
   "source": [
    "<h1> Parameter & Environment Information </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef75ca",
   "metadata": {},
   "source": [
    "<p>\n",
    "    This environment is part of the Box2D environments.\n",
    "</p>\n",
    "\n",
    "<ul>\n",
    "    <li> Action Space Discrete(4) </li>\n",
    "    <li> Observation Shape (8,) </li>\n",
    "    <li> Observation High [1.5 1.5 5. 5. 3.14 5. 1. 1. ] </li>\n",
    "    <li> Observation Low [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] </li>\n",
    "    <li> Import gymnasium.make(\"LunarLander-v2\") </li>\n",
    "</ul>\n",
    "\n",
    "<h3> Description </h3>\n",
    "<p>This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt.</p>\n",
    "\n",
    "<h3> Action Space </h3>\n",
    "<p>\n",
    "There are four discrete actions available:\n",
    "\n",
    "* 0: do nothing\n",
    "* 1: fire left orientation engine\n",
    "* 2: fire main engine\n",
    "* 3: fire right orientation engine\n",
    "\n",
    "</p>\n",
    "\n",
    "<h3> Observation Space </h3>\n",
    "<p>\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in x & y, its linear velocities in x & y, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "</p>\n",
    "\n",
    "<h3> Reward </h3>\n",
    "<p>\n",
    "After every step a reward is granted. The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "* is increased/decreased the closer/further the lander is to the landing pad.\n",
    "* is increased/decreased the slower/faster the lander is moving.\n",
    "* is decreased the more the lander is tilted (angle not horizontal).\n",
    "* is increased by 10 points for each leg that is in contact with the ground.\n",
    "* is decreased by 0.03 points each frame a side engine is firing.\n",
    "* is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
    "\n",
    "An episode is considered a solution if it scores at least 200 points.\n",
    "</p>\n",
    "\n",
    "<h3> Starting State </h3>\n",
    "\n",
    "<p>The lander starts at the top center of the viewport with a random initial force applied to its center of mass.</p>\n",
    "\n",
    "<h3> Episode Termination </h3>\n",
    "<p> The episode finishes if:<br>\n",
    "    \n",
    "1. the lander crashes (the lander body gets in contact with the moon);<br>\n",
    "2. the lander gets outside of the viewport (x coordinate is greater than 1);<br>\n",
    "3. the lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:<br>\n",
    "\n",
    "When Box2D determines that a body (or group of bodies) has come to rest, the body enters a sleep state which has very little CPU overhead. If a body is awake and collides with a sleeping body, then the sleeping body wakes up. Bodies will also wake up if a joint or contact attached to them is destroyed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0543a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fb45b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Action inter is descrete 4\n",
      "Shape of Observation is (8,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Action inter is descrete {}\".format(env.action_space.n))\n",
    "print(\"Shape of Observation is {}\".format(env.observation_space.sample().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ff9c9",
   "metadata": {},
   "source": [
    "<h1> Baseline Model. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35101af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward after 10 max run is -1.2346880943500231\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "obs = env.reset()\n",
    "done = False\n",
    "MAX_RUN = 10\n",
    "\n",
    "for i in range(MAX_RUN):\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action_sample = env.action_space.sample()\n",
    "        # let's take a step in the environment \n",
    "        obs, rwd, done, info ,_  = env.step(action_sample)\n",
    "        rewards.append(rwd)\n",
    "env.close()\n",
    "print(\"Mean Reward after {} max run is {}\".format(MAX_RUN, np.mean(np.array(rewards))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac737551",
   "metadata": {},
   "source": [
    "<h1> Reinforcement Learning For Training The Model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaa0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d311d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./TensorBoardLog/A2C_7\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -205.89\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -205.89 - Last mean reward per episode: -199.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -199.48 - Last mean reward per episode: -194.56\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -194.56 - Last mean reward per episode: -182.98\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -182.98 - Last mean reward per episode: -181.72\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -181.72 - Last mean reward per episode: -172.56\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -172.56 - Last mean reward per episode: -173.52\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -172.56 - Last mean reward per episode: -172.41\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -172.41 - Last mean reward per episode: -173.99\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -172.41 - Last mean reward per episode: -172.22\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 92.3     |\n",
      "|    ep_rew_mean        | -172     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.0141  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -93.7    |\n",
      "|    value_loss         | 5.31e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -172.22 - Last mean reward per episode: -163.61\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -166.89\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -165.44\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -169.57\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -169.65\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -179.72\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -186.89\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -183.18\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -178.16\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -180.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91       |\n",
      "|    ep_rew_mean        | -180     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1805     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.00828 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -125     |\n",
      "|    value_loss         | 9.59e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -179.30\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -182.30\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -175.74\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -170.65\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -167.34\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -163.61 - Last mean reward per episode: -158.54\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -158.54 - Last mean reward per episode: -157.67\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -162.01\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -164.03\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -165.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 97.7     |\n",
      "|    ep_rew_mean        | -166     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2063     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.0169  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -6.71    |\n",
      "|    value_loss         | 330      |\n",
      "------------------------------------\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -169.81\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -164.05\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -169.75\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -176.05\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -175.43\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -181.32\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -177.49\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -179.05\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -171.76\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -171.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 101      |\n",
      "|    ep_rew_mean        | -171     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2219     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 0.000592 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -103     |\n",
      "|    value_loss         | 6.78e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -171.30\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -166.57\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -166.68\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -161.18\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -160.22\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -157.67 - Last mean reward per episode: -155.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -155.69 - Last mean reward per episode: -152.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -152.02 - Last mean reward per episode: -147.77\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -147.77 - Last mean reward per episode: -146.78\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -146.78 - Last mean reward per episode: -146.06\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 101      |\n",
      "|    ep_rew_mean        | -146     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2320     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.35    |\n",
      "|    explained_variance | -0.0029  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.38     |\n",
      "|    value_loss         | 66.6     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 51000\n",
      "Best mean reward: -146.06 - Last mean reward per episode: -146.16\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -146.06 - Last mean reward per episode: -143.67\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -143.67 - Last mean reward per episode: -145.25\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -143.67 - Last mean reward per episode: -144.89\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -143.67 - Last mean reward per episode: -144.89\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -143.67 - Last mean reward per episode: -138.36\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -138.36 - Last mean reward per episode: -136.44\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -138.49\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -141.03\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -139.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 102      |\n",
      "|    ep_rew_mean        | -140     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2392     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | -0.00345 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -14.5    |\n",
      "|    value_loss         | 716      |\n",
      "------------------------------------\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -141.83\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -141.30\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -136.44 - Last mean reward per episode: -133.46\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -134.47\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -135.56\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -138.67\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -142.39\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.22\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -142.39\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -138.93\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 107      |\n",
      "|    ep_rew_mean        | -139     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2441     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | -0.00391 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -58.4    |\n",
      "|    value_loss         | 3.22e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -137.34\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -138.19\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -140.73\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -143.76\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -142.16\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.28\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -145.22\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -141.50\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -140.64\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -140.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 109      |\n",
      "|    ep_rew_mean        | -140     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2476     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | -0.00166 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -42.6    |\n",
      "|    value_loss         | 2.3e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.46\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -148.37\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -143.77\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -139.96\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -143.69\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -145.71\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -140.91\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -142.30\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -143.20\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 111      |\n",
      "|    ep_rew_mean        | -145     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2503     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 1.81e-05 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -77      |\n",
      "|    value_loss         | 8.58e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.86\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -145.69\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.68\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -145.66\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -149.33\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -144.29\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -142.35\n",
      "Num timesteps: 98000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -141.48\n",
      "Num timesteps: 99000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -135.24\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -135.64\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 118       |\n",
      "|    ep_rew_mean        | -136      |\n",
      "| time/                 |           |\n",
      "|    fps                | 2518      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.28     |\n",
      "|    explained_variance | -0.000943 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 6.38      |\n",
      "|    value_loss         | 86        |\n",
      "-------------------------------------\n",
      "Num timesteps: 101000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -135.46\n",
      "Num timesteps: 102000\n",
      "Best mean reward: -133.46 - Last mean reward per episode: -131.73\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 103000\n",
      "Best mean reward: -131.73 - Last mean reward per episode: -124.61\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 104000\n",
      "Best mean reward: -124.61 - Last mean reward per episode: -127.35\n",
      "Num timesteps: 105000\n",
      "Best mean reward: -124.61 - Last mean reward per episode: -124.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 106000\n",
      "Best mean reward: -124.02 - Last mean reward per episode: -124.21\n",
      "Num timesteps: 107000\n",
      "Best mean reward: -124.02 - Last mean reward per episode: -124.12\n",
      "Num timesteps: 108000\n",
      "Best mean reward: -124.02 - Last mean reward per episode: -125.00\n",
      "Num timesteps: 109000\n",
      "Best mean reward: -124.02 - Last mean reward per episode: -127.07\n",
      "Num timesteps: 110000\n",
      "Best mean reward: -124.02 - Last mean reward per episode: -123.95\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 125       |\n",
      "|    ep_rew_mean        | -124      |\n",
      "| time/                 |           |\n",
      "|    fps                | 2528      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 110000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.2      |\n",
      "|    explained_variance | -6.06e-05 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -11.5     |\n",
      "|    value_loss         | 1.67e+03  |\n",
      "-------------------------------------\n",
      "Num timesteps: 111000\n",
      "Best mean reward: -123.95 - Last mean reward per episode: -124.83\n",
      "Num timesteps: 112000\n",
      "Best mean reward: -123.95 - Last mean reward per episode: -122.39\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 113000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -124.12\n",
      "Num timesteps: 114000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -129.03\n",
      "Num timesteps: 115000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -127.13\n",
      "Num timesteps: 116000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -125.84\n",
      "Num timesteps: 117000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -124.56\n",
      "Num timesteps: 118000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -122.90\n",
      "Num timesteps: 119000\n",
      "Best mean reward: -122.39 - Last mean reward per episode: -120.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 120000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -123.34\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 123       |\n",
      "|    ep_rew_mean        | -123      |\n",
      "| time/                 |           |\n",
      "|    fps                | 2539      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 120000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.26     |\n",
      "|    explained_variance | -5.36e-05 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -60.4     |\n",
      "|    value_loss         | 3.61e+03  |\n",
      "-------------------------------------\n",
      "Num timesteps: 121000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -122.00\n",
      "Num timesteps: 122000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -130.68\n",
      "Num timesteps: 123000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -129.78\n",
      "Num timesteps: 124000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -128.04\n",
      "Num timesteps: 125000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -126.27\n",
      "Num timesteps: 126000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -126.25\n",
      "Num timesteps: 127000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -125.78\n",
      "Num timesteps: 128000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -124.27\n",
      "Num timesteps: 129000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -125.06\n",
      "Num timesteps: 130000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -121.75\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 135      |\n",
      "|    ep_rew_mean        | -122     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2499     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 130000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | -0.00123 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -22.5    |\n",
      "|    value_loss         | 1.15e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 131000\n",
      "Best mean reward: -120.74 - Last mean reward per episode: -115.73\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 132000\n",
      "Best mean reward: -115.73 - Last mean reward per episode: -116.01\n",
      "Num timesteps: 133000\n",
      "Best mean reward: -115.73 - Last mean reward per episode: -113.73\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 134000\n",
      "Best mean reward: -113.73 - Last mean reward per episode: -113.84\n",
      "Num timesteps: 135000\n",
      "Best mean reward: -113.73 - Last mean reward per episode: -108.04\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 136000\n",
      "Best mean reward: -108.04 - Last mean reward per episode: -110.58\n",
      "Num timesteps: 137000\n",
      "Best mean reward: -108.04 - Last mean reward per episode: -105.49\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 138000\n",
      "Best mean reward: -105.49 - Last mean reward per episode: -107.88\n",
      "Num timesteps: 139000\n",
      "Best mean reward: -105.49 - Last mean reward per episode: -107.76\n",
      "Num timesteps: 140000\n",
      "Best mean reward: -105.49 - Last mean reward per episode: -104.34\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 126      |\n",
      "|    ep_rew_mean        | -104     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2503     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 140000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.25    |\n",
      "|    explained_variance | 3.52e-05 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -4.58    |\n",
      "|    value_loss         | 232      |\n",
      "------------------------------------\n",
      "Num timesteps: 141000\n",
      "Best mean reward: -104.34 - Last mean reward per episode: -102.75\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 142000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -106.53\n",
      "Num timesteps: 143000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -105.84\n",
      "Num timesteps: 144000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -109.73\n",
      "Num timesteps: 145000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -112.23\n",
      "Num timesteps: 146000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -108.07\n",
      "Num timesteps: 147000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -105.56\n",
      "Num timesteps: 148000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -102.88\n",
      "Num timesteps: 149000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -104.01\n",
      "Num timesteps: 150000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -102.91\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 138       |\n",
      "|    ep_rew_mean        | -103      |\n",
      "| time/                 |           |\n",
      "|    fps                | 2503      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 150000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.21     |\n",
      "|    explained_variance | -0.000415 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.52      |\n",
      "|    value_loss         | 890       |\n",
      "-------------------------------------\n",
      "Num timesteps: 151000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -104.73\n",
      "Num timesteps: 152000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -103.02\n",
      "Num timesteps: 153000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -103.17\n",
      "Num timesteps: 154000\n",
      "Best mean reward: -102.75 - Last mean reward per episode: -99.55\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 155000\n",
      "Best mean reward: -99.55 - Last mean reward per episode: -101.75\n",
      "Num timesteps: 156000\n",
      "Best mean reward: -99.55 - Last mean reward per episode: -99.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 157000\n",
      "Best mean reward: -99.55 - Last mean reward per episode: -98.41\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 158000\n",
      "Best mean reward: -98.41 - Last mean reward per episode: -98.84\n",
      "Num timesteps: 159000\n",
      "Best mean reward: -98.41 - Last mean reward per episode: -98.17\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 160000\n",
      "Best mean reward: -98.17 - Last mean reward per episode: -96.81\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 155       |\n",
      "|    ep_rew_mean        | -96.8     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2429      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 160000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.11     |\n",
      "|    explained_variance | -9.95e-05 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 26.9      |\n",
      "|    value_loss         | 1.95e+03  |\n",
      "-------------------------------------\n",
      "Num timesteps: 161000\n",
      "Best mean reward: -96.81 - Last mean reward per episode: -93.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 162000\n",
      "Best mean reward: -93.48 - Last mean reward per episode: -92.31\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 163000\n",
      "Best mean reward: -92.31 - Last mean reward per episode: -92.37\n",
      "Num timesteps: 164000\n",
      "Best mean reward: -92.31 - Last mean reward per episode: -94.06\n",
      "Num timesteps: 165000\n",
      "Best mean reward: -92.31 - Last mean reward per episode: -90.97\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 166000\n",
      "Best mean reward: -90.97 - Last mean reward per episode: -88.24\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 167000\n",
      "Best mean reward: -88.24 - Last mean reward per episode: -88.65\n",
      "Num timesteps: 168000\n",
      "Best mean reward: -88.24 - Last mean reward per episode: -81.34\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 169000\n",
      "Best mean reward: -81.34 - Last mean reward per episode: -80.97\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 170000\n",
      "Best mean reward: -80.97 - Last mean reward per episode: -78.37\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 171      |\n",
      "|    ep_rew_mean        | -78.4    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2419     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.15    |\n",
      "|    explained_variance | 0.000129 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -14.2    |\n",
      "|    value_loss         | 1.03e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 171000\n",
      "Best mean reward: -78.37 - Last mean reward per episode: -78.30\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 172000\n",
      "Best mean reward: -78.30 - Last mean reward per episode: -75.21\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 173000\n",
      "Best mean reward: -75.21 - Last mean reward per episode: -72.39\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 174000\n",
      "Best mean reward: -72.39 - Last mean reward per episode: -71.44\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 175000\n",
      "Best mean reward: -71.44 - Last mean reward per episode: -70.40\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 176000\n",
      "Best mean reward: -70.40 - Last mean reward per episode: -71.35\n",
      "Num timesteps: 177000\n",
      "Best mean reward: -70.40 - Last mean reward per episode: -72.72\n",
      "Num timesteps: 178000\n",
      "Best mean reward: -70.40 - Last mean reward per episode: -69.99\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 179000\n",
      "Best mean reward: -69.99 - Last mean reward per episode: -68.06\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 180000\n",
      "Best mean reward: -68.06 - Last mean reward per episode: -64.79\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 158      |\n",
      "|    ep_rew_mean        | -64.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2395     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.21    |\n",
      "|    explained_variance | -0.00422 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 7.58     |\n",
      "|    value_loss         | 1.12e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 181000\n",
      "Best mean reward: -64.79 - Last mean reward per episode: -63.64\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 182000\n",
      "Best mean reward: -63.64 - Last mean reward per episode: -59.47\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 183000\n",
      "Best mean reward: -59.47 - Last mean reward per episode: -56.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 184000\n",
      "Best mean reward: -56.69 - Last mean reward per episode: -59.05\n",
      "Num timesteps: 185000\n",
      "Best mean reward: -56.69 - Last mean reward per episode: -59.22\n",
      "Num timesteps: 186000\n",
      "Best mean reward: -56.69 - Last mean reward per episode: -56.38\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 187000\n",
      "Best mean reward: -56.38 - Last mean reward per episode: -54.75\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 188000\n",
      "Best mean reward: -54.75 - Last mean reward per episode: -51.82\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 189000\n",
      "Best mean reward: -51.82 - Last mean reward per episode: -50.21\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 190000\n",
      "Best mean reward: -50.21 - Last mean reward per episode: -48.84\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 175      |\n",
      "|    ep_rew_mean        | -48.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2354     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 190000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | -0.00239 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 4.57     |\n",
      "|    value_loss         | 1.27e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 191000\n",
      "Best mean reward: -48.84 - Last mean reward per episode: -46.82\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 192000\n",
      "Best mean reward: -46.82 - Last mean reward per episode: -44.15\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 193000\n",
      "Best mean reward: -44.15 - Last mean reward per episode: -42.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 194000\n",
      "Best mean reward: -42.48 - Last mean reward per episode: -40.93\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 195000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -42.25\n",
      "Num timesteps: 196000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -42.36\n",
      "Num timesteps: 197000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -46.23\n",
      "Num timesteps: 198000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -46.65\n",
      "Num timesteps: 199000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -47.45\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -46.60\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 179       |\n",
      "|    ep_rew_mean        | -46.6     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2348      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 200000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.15     |\n",
      "|    explained_variance | -0.000399 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -7.11     |\n",
      "|    value_loss         | 829       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 201000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -44.30\n",
      "Num timesteps: 202000\n",
      "Best mean reward: -40.93 - Last mean reward per episode: -39.86\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 203000\n",
      "Best mean reward: -39.86 - Last mean reward per episode: -38.20\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 204000\n",
      "Best mean reward: -38.20 - Last mean reward per episode: -36.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 205000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -39.00\n",
      "Num timesteps: 206000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -36.99\n",
      "Num timesteps: 207000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -37.76\n",
      "Num timesteps: 208000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -40.13\n",
      "Num timesteps: 209000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -40.03\n",
      "Num timesteps: 210000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -39.61\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 175       |\n",
      "|    ep_rew_mean        | -39.6     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2341      |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 210000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.12     |\n",
      "|    explained_variance | -6.19e-05 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    value_loss         | 739       |\n",
      "-------------------------------------\n",
      "Num timesteps: 211000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -39.41\n",
      "Num timesteps: 212000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -39.64\n",
      "Num timesteps: 213000\n",
      "Best mean reward: -36.02 - Last mean reward per episode: -33.52\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 214000\n",
      "Best mean reward: -33.52 - Last mean reward per episode: -32.56\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 215000\n",
      "Best mean reward: -32.56 - Last mean reward per episode: -29.58\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 216000\n",
      "Best mean reward: -29.58 - Last mean reward per episode: -28.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 217000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -31.75\n",
      "Num timesteps: 218000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -31.38\n",
      "Num timesteps: 219000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -32.41\n",
      "Num timesteps: 220000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -32.50\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 175       |\n",
      "|    ep_rew_mean        | -32.5     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2336      |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 220000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.14     |\n",
      "|    explained_variance | -0.000282 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 22.6      |\n",
      "|    value_loss         | 943       |\n",
      "-------------------------------------\n",
      "Num timesteps: 221000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -33.30\n",
      "Num timesteps: 222000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -35.22\n",
      "Num timesteps: 223000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -33.44\n",
      "Num timesteps: 224000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -31.67\n",
      "Num timesteps: 225000\n",
      "Best mean reward: -28.42 - Last mean reward per episode: -27.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 226000\n",
      "Best mean reward: -27.69 - Last mean reward per episode: -27.61\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 227000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -28.22\n",
      "Num timesteps: 228000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -28.04\n",
      "Num timesteps: 229000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -29.13\n",
      "Num timesteps: 230000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -29.44\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 178       |\n",
      "|    ep_rew_mean        | -29.4     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2329      |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 230000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.13     |\n",
      "|    explained_variance | -0.000419 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 43.7      |\n",
      "|    value_loss         | 2.09e+03  |\n",
      "-------------------------------------\n",
      "Num timesteps: 231000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -30.30\n",
      "Num timesteps: 232000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -29.98\n",
      "Num timesteps: 233000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -31.14\n",
      "Num timesteps: 234000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -30.14\n",
      "Num timesteps: 235000\n",
      "Best mean reward: -27.61 - Last mean reward per episode: -26.94\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 236000\n",
      "Best mean reward: -26.94 - Last mean reward per episode: -27.01\n",
      "Num timesteps: 237000\n",
      "Best mean reward: -26.94 - Last mean reward per episode: -27.56\n",
      "Num timesteps: 238000\n",
      "Best mean reward: -26.94 - Last mean reward per episode: -25.46\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 239000\n",
      "Best mean reward: -25.46 - Last mean reward per episode: -23.37\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 240000\n",
      "Best mean reward: -23.37 - Last mean reward per episode: -23.90\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 189       |\n",
      "|    ep_rew_mean        | -23.9     |\n",
      "| time/                 |           |\n",
      "|    fps                | 2305      |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 240000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.12     |\n",
      "|    explained_variance | -0.000452 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 23        |\n",
      "|    value_loss         | 1.1e+03   |\n",
      "-------------------------------------\n",
      "Num timesteps: 241000\n",
      "Best mean reward: -23.37 - Last mean reward per episode: -23.82\n",
      "Num timesteps: 242000\n",
      "Best mean reward: -23.37 - Last mean reward per episode: -23.72\n",
      "Num timesteps: 243000\n",
      "Best mean reward: -23.37 - Last mean reward per episode: -23.96\n",
      "Num timesteps: 244000\n",
      "Best mean reward: -23.37 - Last mean reward per episode: -21.78\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 245000\n",
      "Best mean reward: -21.78 - Last mean reward per episode: -20.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 246000\n",
      "Best mean reward: -20.74 - Last mean reward per episode: -20.57\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 247000\n",
      "Best mean reward: -20.57 - Last mean reward per episode: -19.93\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 248000\n",
      "Best mean reward: -19.93 - Last mean reward per episode: -20.45\n",
      "Num timesteps: 249000\n",
      "Best mean reward: -19.93 - Last mean reward per episode: -22.42\n",
      "Num timesteps: 250000\n",
      "Best mean reward: -19.93 - Last mean reward per episode: -20.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 220      |\n",
      "|    ep_rew_mean        | -20.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2252     |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.3     |\n",
      "|    explained_variance | 0.0314   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 34.8     |\n",
      "|    value_loss         | 916      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 251000\n",
      "Best mean reward: -19.93 - Last mean reward per episode: -20.08\n",
      "Num timesteps: 252000\n",
      "Best mean reward: -19.93 - Last mean reward per episode: -19.45\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 253000\n",
      "Best mean reward: -19.45 - Last mean reward per episode: -18.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 254000\n",
      "Best mean reward: -18.74 - Last mean reward per episode: -17.80\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 255000\n",
      "Best mean reward: -17.80 - Last mean reward per episode: -16.80\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 256000\n",
      "Best mean reward: -16.80 - Last mean reward per episode: -15.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 257000\n",
      "Best mean reward: -15.42 - Last mean reward per episode: -15.66\n",
      "Num timesteps: 258000\n",
      "Best mean reward: -15.42 - Last mean reward per episode: -14.52\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 259000\n",
      "Best mean reward: -14.52 - Last mean reward per episode: -13.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 260000\n",
      "Best mean reward: -13.69 - Last mean reward per episode: -12.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 273      |\n",
      "|    ep_rew_mean        | -12.7    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2138     |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 260000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | -0.00839 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 12.5     |\n",
      "|    value_loss         | 399      |\n",
      "------------------------------------\n",
      "Num timesteps: 261000\n",
      "Best mean reward: -12.74 - Last mean reward per episode: -12.74\n",
      "Num timesteps: 262000\n",
      "Best mean reward: -12.74 - Last mean reward per episode: -12.28\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 263000\n",
      "Best mean reward: -12.28 - Last mean reward per episode: -11.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 264000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -11.68\n",
      "Num timesteps: 265000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -13.18\n",
      "Num timesteps: 266000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -12.18\n",
      "Num timesteps: 267000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -12.03\n",
      "Num timesteps: 268000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.77\n",
      "Num timesteps: 269000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.82\n",
      "Num timesteps: 270000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -13.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | -14      |\n",
      "| time/                 |          |\n",
      "|    fps                | 2051     |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 270000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.31    |\n",
      "|    explained_variance | 0.734    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 3.49     |\n",
      "|    value_loss         | 28.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 271000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -13.35\n",
      "Num timesteps: 272000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -12.32\n",
      "Num timesteps: 273000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -12.71\n",
      "Num timesteps: 274000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.01\n",
      "Num timesteps: 275000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.33\n",
      "Num timesteps: 276000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.40\n",
      "Num timesteps: 277000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -17.53\n",
      "Num timesteps: 278000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -17.17\n",
      "Num timesteps: 279000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.13\n",
      "Num timesteps: 280000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 328      |\n",
      "|    ep_rew_mean        | -18.8    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1987     |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | -0.00255 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 16.8     |\n",
      "|    value_loss         | 860      |\n",
      "------------------------------------\n",
      "Num timesteps: 281000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.20\n",
      "Num timesteps: 282000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -16.35\n",
      "Num timesteps: 283000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.62\n",
      "Num timesteps: 284000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.22\n",
      "Num timesteps: 285000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -21.57\n",
      "Num timesteps: 286000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -21.35\n",
      "Num timesteps: 287000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -21.43\n",
      "Num timesteps: 288000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -22.41\n",
      "Num timesteps: 289000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -23.79\n",
      "Num timesteps: 290000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -23.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 332      |\n",
      "|    ep_rew_mean        | -23.9    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1930     |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 150      |\n",
      "|    total_timesteps    | 290000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | -0.0139  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 29       |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 291000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -24.21\n",
      "Num timesteps: 292000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -22.71\n",
      "Num timesteps: 293000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -22.17\n",
      "Num timesteps: 294000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.40\n",
      "Num timesteps: 295000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -19.74\n",
      "Num timesteps: 296000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.20\n",
      "Num timesteps: 297000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.65\n",
      "Num timesteps: 298000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.98\n",
      "Num timesteps: 299000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.13\n",
      "Num timesteps: 300000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -20.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | -20.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1869     |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.32    |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 9.16     |\n",
      "|    value_loss         | 79       |\n",
      "------------------------------------\n",
      "Num timesteps: 301000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.13\n",
      "Num timesteps: 302000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -18.14\n",
      "Num timesteps: 303000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -17.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 304000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -17.96\n",
      "Num timesteps: 305000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.65\n",
      "Num timesteps: 306000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.81\n",
      "Num timesteps: 307000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.96\n",
      "Num timesteps: 308000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.00\n",
      "Num timesteps: 309000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.16\n",
      "Num timesteps: 310000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 392      |\n",
      "|    ep_rew_mean        | -15.1    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1794     |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 310000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.613    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -8.77    |\n",
      "|    value_loss         | 73.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 311000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.38\n",
      "Num timesteps: 312000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.40\n",
      "Num timesteps: 313000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.12\n",
      "Num timesteps: 314000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -14.85\n",
      "Num timesteps: 315000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -16.90\n",
      "Num timesteps: 316000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -17.57\n",
      "Num timesteps: 317000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -16.69\n",
      "Num timesteps: 318000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.79\n",
      "Num timesteps: 319000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.64\n",
      "Num timesteps: 320000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 434      |\n",
      "|    ep_rew_mean        | -15      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1731     |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 184      |\n",
      "|    total_timesteps    | 320000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | -0.0191  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -6.21    |\n",
      "|    value_loss         | 899      |\n",
      "------------------------------------\n",
      "Num timesteps: 321000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -15.30\n",
      "Num timesteps: 322000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -13.38\n",
      "Num timesteps: 323000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -12.13\n",
      "Num timesteps: 324000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -11.66\n",
      "Num timesteps: 325000\n",
      "Best mean reward: -11.42 - Last mean reward per episode: -11.28\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 326000\n",
      "Best mean reward: -11.28 - Last mean reward per episode: -10.44\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 327000\n",
      "Best mean reward: -10.44 - Last mean reward per episode: -8.91\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 328000\n",
      "Best mean reward: -8.91 - Last mean reward per episode: -7.67\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 329000\n",
      "Best mean reward: -7.67 - Last mean reward per episode: -9.31\n",
      "Num timesteps: 330000\n",
      "Best mean reward: -7.67 - Last mean reward per episode: -10.49\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 482      |\n",
      "|    ep_rew_mean        | -10.5    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1663     |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 198      |\n",
      "|    total_timesteps    | 330000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | 0.56     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 3.67     |\n",
      "|    value_loss         | 16.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 331000\n",
      "Best mean reward: -7.67 - Last mean reward per episode: -6.60\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 332000\n",
      "Best mean reward: -6.60 - Last mean reward per episode: -4.20\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 333000\n",
      "Best mean reward: -4.20 - Last mean reward per episode: -2.18\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 334000\n",
      "Best mean reward: -2.18 - Last mean reward per episode: -3.27\n",
      "Num timesteps: 335000\n",
      "Best mean reward: -2.18 - Last mean reward per episode: -2.86\n",
      "Num timesteps: 336000\n",
      "Best mean reward: -2.18 - Last mean reward per episode: -2.60\n",
      "Num timesteps: 337000\n",
      "Best mean reward: -2.18 - Last mean reward per episode: -2.38\n",
      "Num timesteps: 338000\n",
      "Best mean reward: -2.18 - Last mean reward per episode: -0.15\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 339000\n",
      "Best mean reward: -0.15 - Last mean reward per episode: 0.63\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 0.63 - Last mean reward per episode: 0.93\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 516      |\n",
      "|    ep_rew_mean        | 0.929    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1626     |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 209      |\n",
      "|    total_timesteps    | 340000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | -0.202   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 6.1      |\n",
      "|    value_loss         | 121      |\n",
      "------------------------------------\n",
      "Num timesteps: 341000\n",
      "Best mean reward: 0.93 - Last mean reward per episode: 2.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 342000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 1.35\n",
      "Num timesteps: 343000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 344000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -0.43\n",
      "Num timesteps: 345000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -3.36\n",
      "Num timesteps: 346000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -4.95\n",
      "Num timesteps: 347000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -5.99\n",
      "Num timesteps: 348000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -5.34\n",
      "Num timesteps: 349000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -6.92\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -6.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 557      |\n",
      "|    ep_rew_mean        | -6.54    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1569     |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 223      |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | -0.0314  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 10.3     |\n",
      "|    value_loss         | 284      |\n",
      "------------------------------------\n",
      "Num timesteps: 351000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -6.78\n",
      "Num timesteps: 352000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -5.67\n",
      "Num timesteps: 353000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -5.19\n",
      "Num timesteps: 354000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.99\n",
      "Num timesteps: 355000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.59\n",
      "Num timesteps: 356000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -1.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 357000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -0.01\n",
      "Num timesteps: 358000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 0.35\n",
      "Num timesteps: 359000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 1.11\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 597      |\n",
      "|    ep_rew_mean        | -2.94    |\n",
      "| time/                 |          |\n",
      "|    fps                | 1530     |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 235      |\n",
      "|    total_timesteps    | 360000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | 0.0208   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -4.94    |\n",
      "|    value_loss         | 138      |\n",
      "------------------------------------\n",
      "Num timesteps: 361000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.80\n",
      "Num timesteps: 362000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -3.29\n",
      "Num timesteps: 363000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.47\n",
      "Num timesteps: 364000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -3.46\n",
      "Num timesteps: 365000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.89\n",
      "Num timesteps: 366000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.07\n",
      "Num timesteps: 367000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.04\n",
      "Num timesteps: 368000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -2.31\n",
      "Num timesteps: 369000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -1.32\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: -0.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 641      |\n",
      "|    ep_rew_mean        | -0.0204  |\n",
      "| time/                 |          |\n",
      "|    fps                | 1486     |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 248      |\n",
      "|    total_timesteps    | 370000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.596    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.243    |\n",
      "|    value_loss         | 10.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 371000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 1.64\n",
      "Num timesteps: 372000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 2.39\n",
      "Num timesteps: 373000\n",
      "Best mean reward: 2.42 - Last mean reward per episode: 3.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 374000\n",
      "Best mean reward: 3.42 - Last mean reward per episode: 3.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 375000\n",
      "Best mean reward: 3.74 - Last mean reward per episode: 4.36\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 376000\n",
      "Best mean reward: 4.36 - Last mean reward per episode: 3.91\n",
      "Num timesteps: 377000\n",
      "Best mean reward: 4.36 - Last mean reward per episode: 4.61\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 378000\n",
      "Best mean reward: 4.61 - Last mean reward per episode: 4.84\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 379000\n",
      "Best mean reward: 4.84 - Last mean reward per episode: 6.39\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 6.39 - Last mean reward per episode: 8.23\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 678      |\n",
      "|    ep_rew_mean        | 8.23     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1432     |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 265      |\n",
      "|    total_timesteps    | 380000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | -0.0312  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 11.7     |\n",
      "|    value_loss         | 292      |\n",
      "------------------------------------\n",
      "Num timesteps: 381000\n",
      "Best mean reward: 8.23 - Last mean reward per episode: 9.96\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 382000\n",
      "Best mean reward: 9.96 - Last mean reward per episode: 9.70\n",
      "Num timesteps: 383000\n",
      "Best mean reward: 9.96 - Last mean reward per episode: 12.63\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 384000\n",
      "Best mean reward: 12.63 - Last mean reward per episode: 16.45\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 385000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.54\n",
      "Num timesteps: 386000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.40\n",
      "Num timesteps: 387000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.66\n",
      "Num timesteps: 388000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.84\n",
      "Num timesteps: 389000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.87\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 700      |\n",
      "|    ep_rew_mean        | 13       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1406     |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 277      |\n",
      "|    total_timesteps    | 390000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.936   |\n",
      "|    explained_variance | -0.0389  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 1.4      |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "Num timesteps: 391000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 12.02\n",
      "Num timesteps: 392000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 12.75\n",
      "Num timesteps: 393000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 10.48\n",
      "Num timesteps: 394000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 10.98\n",
      "Num timesteps: 395000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 12.29\n",
      "Num timesteps: 396000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.18\n",
      "Num timesteps: 397000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.27\n",
      "Num timesteps: 398000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 12.77\n",
      "Num timesteps: 399000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.50\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 722      |\n",
      "|    ep_rew_mean        | 13.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 292      |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.49     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.594    |\n",
      "|    value_loss         | 9.91     |\n",
      "------------------------------------\n",
      "Num timesteps: 401000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 11.91\n",
      "Num timesteps: 402000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.92\n",
      "Num timesteps: 403000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.10\n",
      "Num timesteps: 404000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.44\n",
      "Num timesteps: 405000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.09\n",
      "Num timesteps: 406000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.69\n",
      "Num timesteps: 407000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.00\n",
      "Num timesteps: 408000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.44\n",
      "Num timesteps: 409000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.67\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 747      |\n",
      "|    ep_rew_mean        | 15.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 305      |\n",
      "|    total_timesteps    | 410000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | -0.0498  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.482   |\n",
      "|    value_loss         | 23.8     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 411000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.12\n",
      "Num timesteps: 412000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 14.86\n",
      "Num timesteps: 413000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 13.47\n",
      "Num timesteps: 414000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.52\n",
      "Num timesteps: 415000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 16.38\n",
      "Num timesteps: 416000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.82\n",
      "Num timesteps: 417000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 15.74\n",
      "Num timesteps: 418000\n",
      "Best mean reward: 16.45 - Last mean reward per episode: 18.61\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 419000\n",
      "Best mean reward: 18.61 - Last mean reward per episode: 21.15\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 21.15 - Last mean reward per episode: 23.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 757      |\n",
      "|    ep_rew_mean        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 317      |\n",
      "|    total_timesteps    | 420000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.707   |\n",
      "|    explained_variance | 0.0189   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.0305  |\n",
      "|    value_loss         | 0.0795   |\n",
      "------------------------------------\n",
      "Num timesteps: 421000\n",
      "Best mean reward: 23.42 - Last mean reward per episode: 26.85\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 422000\n",
      "Best mean reward: 26.85 - Last mean reward per episode: 28.66\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 423000\n",
      "Best mean reward: 28.66 - Last mean reward per episode: 31.99\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 424000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 31.53\n",
      "Num timesteps: 425000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 31.25\n",
      "Num timesteps: 426000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 31.91\n",
      "Num timesteps: 427000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 31.43\n",
      "Num timesteps: 428000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 30.99\n",
      "Num timesteps: 429000\n",
      "Best mean reward: 31.99 - Last mean reward per episode: 33.00\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 33.00 - Last mean reward per episode: 34.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 766      |\n",
      "|    ep_rew_mean        | 34.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 331      |\n",
      "|    total_timesteps    | 430000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.659   |\n",
      "|    explained_variance | 0.00653  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.00927 |\n",
      "|    value_loss         | 0.026    |\n",
      "------------------------------------\n",
      "Num timesteps: 431000\n",
      "Best mean reward: 34.69 - Last mean reward per episode: 37.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 432000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 34.63\n",
      "Num timesteps: 433000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 34.56\n",
      "Num timesteps: 434000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 34.91\n",
      "Num timesteps: 435000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 34.11\n",
      "Num timesteps: 436000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 34.40\n",
      "Num timesteps: 437000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 35.59\n",
      "Num timesteps: 438000\n",
      "Best mean reward: 37.02 - Last mean reward per episode: 37.98\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 439000\n",
      "Best mean reward: 37.98 - Last mean reward per episode: 38.71\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 38.71 - Last mean reward per episode: 38.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 751      |\n",
      "|    ep_rew_mean        | 38.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 342      |\n",
      "|    total_timesteps    | 440000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.01     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -4.59    |\n",
      "|    value_loss         | 25.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 441000\n",
      "Best mean reward: 38.71 - Last mean reward per episode: 39.12\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 442000\n",
      "Best mean reward: 39.12 - Last mean reward per episode: 39.96\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 443000\n",
      "Best mean reward: 39.96 - Last mean reward per episode: 40.11\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 444000\n",
      "Best mean reward: 40.11 - Last mean reward per episode: 40.87\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 445000\n",
      "Best mean reward: 40.87 - Last mean reward per episode: 42.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 446000\n",
      "Best mean reward: 42.74 - Last mean reward per episode: 44.31\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 447000\n",
      "Best mean reward: 44.31 - Last mean reward per episode: 41.50\n",
      "Num timesteps: 448000\n",
      "Best mean reward: 44.31 - Last mean reward per episode: 44.29\n",
      "Num timesteps: 449000\n",
      "Best mean reward: 44.31 - Last mean reward per episode: 46.03\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 46.03 - Last mean reward per episode: 43.98\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 711      |\n",
      "|    ep_rew_mean        | 44       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1263     |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 356      |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | 0.187    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 5.93     |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "Num timesteps: 451000\n",
      "Best mean reward: 46.03 - Last mean reward per episode: 41.74\n",
      "Num timesteps: 452000\n",
      "Best mean reward: 46.03 - Last mean reward per episode: 43.22\n",
      "Num timesteps: 453000\n",
      "Best mean reward: 46.03 - Last mean reward per episode: 45.31\n",
      "Num timesteps: 454000\n",
      "Best mean reward: 46.03 - Last mean reward per episode: 46.47\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 455000\n",
      "Best mean reward: 46.47 - Last mean reward per episode: 51.67\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 456000\n",
      "Best mean reward: 51.67 - Last mean reward per episode: 51.85\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 457000\n",
      "Best mean reward: 51.85 - Last mean reward per episode: 57.20\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 458000\n",
      "Best mean reward: 57.20 - Last mean reward per episode: 59.23\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 459000\n",
      "Best mean reward: 59.23 - Last mean reward per episode: 60.45\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 60.45 - Last mean reward per episode: 62.80\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 708       |\n",
      "|    ep_rew_mean        | 62.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1250      |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 367       |\n",
      "|    total_timesteps    | 460000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.521    |\n",
      "|    explained_variance | -0.000463 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -0.733    |\n",
      "|    value_loss         | 2.19      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 461000\n",
      "Best mean reward: 62.80 - Last mean reward per episode: 69.28\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 462000\n",
      "Best mean reward: 69.28 - Last mean reward per episode: 71.41\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 463000\n",
      "Best mean reward: 71.41 - Last mean reward per episode: 72.51\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 464000\n",
      "Best mean reward: 72.51 - Last mean reward per episode: 76.33\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 465000\n",
      "Best mean reward: 76.33 - Last mean reward per episode: 77.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 466000\n",
      "Best mean reward: 77.02 - Last mean reward per episode: 80.32\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 467000\n",
      "Best mean reward: 80.32 - Last mean reward per episode: 81.35\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 468000\n",
      "Best mean reward: 81.35 - Last mean reward per episode: 83.80\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 469000\n",
      "Best mean reward: 83.80 - Last mean reward per episode: 85.42\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 85.42 - Last mean reward per episode: 84.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 695      |\n",
      "|    ep_rew_mean        | 84.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1233     |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 381      |\n",
      "|    total_timesteps    | 470000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | 0.0715   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -1.8     |\n",
      "|    value_loss         | 40.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 471000\n",
      "Best mean reward: 85.42 - Last mean reward per episode: 88.46\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 472000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 87.63\n",
      "Num timesteps: 473000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 86.95\n",
      "Num timesteps: 474000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 85.43\n",
      "Num timesteps: 475000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 83.74\n",
      "Num timesteps: 476000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 79.49\n",
      "Num timesteps: 477000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 79.63\n",
      "Num timesteps: 478000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 80.66\n",
      "Num timesteps: 479000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 80.37\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 77.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 699      |\n",
      "|    ep_rew_mean        | 77.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1211     |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 396      |\n",
      "|    total_timesteps    | 480000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.116    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 3.15     |\n",
      "|    value_loss         | 31       |\n",
      "------------------------------------\n",
      "Num timesteps: 481000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 77.48\n",
      "Num timesteps: 482000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 78.04\n",
      "Num timesteps: 483000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 80.38\n",
      "Num timesteps: 484000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 81.32\n",
      "Num timesteps: 485000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 81.39\n",
      "Num timesteps: 486000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 82.73\n",
      "Num timesteps: 487000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 84.87\n",
      "Num timesteps: 488000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 86.16\n",
      "Num timesteps: 489000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 84.38\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 80.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 713      |\n",
      "|    ep_rew_mean        | 80.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 411      |\n",
      "|    total_timesteps    | 490000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.1     |\n",
      "|    explained_variance | -0.0341  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -12.8    |\n",
      "|    value_loss         | 187      |\n",
      "------------------------------------\n",
      "Num timesteps: 491000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 78.02\n",
      "Num timesteps: 492000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 75.35\n",
      "Num timesteps: 493000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 75.99\n",
      "Num timesteps: 494000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 74.62\n",
      "Num timesteps: 495000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 71.90\n",
      "Num timesteps: 496000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 73.98\n",
      "Num timesteps: 497000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 73.61\n",
      "Num timesteps: 498000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 77.44\n",
      "Num timesteps: 499000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 81.57\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 82.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 717      |\n",
      "|    ep_rew_mean        | 82.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1170     |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 427      |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.149    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -7.48    |\n",
      "|    value_loss         | 112      |\n",
      "------------------------------------\n",
      "Num timesteps: 501000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 80.08\n",
      "Num timesteps: 502000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 79.80\n",
      "Num timesteps: 503000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 81.00\n",
      "Num timesteps: 504000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 81.57\n",
      "Num timesteps: 505000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 83.95\n",
      "Num timesteps: 506000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 85.87\n",
      "Num timesteps: 507000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 85.27\n",
      "Num timesteps: 508000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 84.48\n",
      "Num timesteps: 509000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 85.94\n",
      "Num timesteps: 510000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 86.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 708      |\n",
      "|    ep_rew_mean        | 86.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1159     |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 439      |\n",
      "|    total_timesteps    | 510000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.779   |\n",
      "|    explained_variance | 0.341    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 1.36     |\n",
      "|    value_loss         | 33       |\n",
      "------------------------------------\n",
      "Num timesteps: 511000\n",
      "Best mean reward: 88.46 - Last mean reward per episode: 89.16\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 512000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 86.63\n",
      "Num timesteps: 513000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 87.04\n",
      "Num timesteps: 514000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 88.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 515000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 84.50\n",
      "Num timesteps: 516000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 83.35\n",
      "Num timesteps: 517000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 80.80\n",
      "Num timesteps: 518000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 80.41\n",
      "Num timesteps: 519000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 82.76\n",
      "Num timesteps: 520000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 82.98\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 727      |\n",
      "|    ep_rew_mean        | 83       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1137     |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 457      |\n",
      "|    total_timesteps    | 520000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.12    |\n",
      "|    explained_variance | -0.189   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -4.23    |\n",
      "|    value_loss         | 59.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 521000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 80.54\n",
      "Num timesteps: 522000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 78.41\n",
      "Num timesteps: 523000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 81.67\n",
      "Num timesteps: 524000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 79.54\n",
      "Num timesteps: 525000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 82.06\n",
      "Num timesteps: 526000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 84.46\n",
      "Num timesteps: 527000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 84.24\n",
      "Num timesteps: 528000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 83.30\n",
      "Num timesteps: 529000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 82.85\n",
      "Num timesteps: 530000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 80.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 741      |\n",
      "|    ep_rew_mean        | 80.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1120     |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 472      |\n",
      "|    total_timesteps    | 530000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.0935   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -6.17    |\n",
      "|    value_loss         | 41.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 531000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 78.42\n",
      "Num timesteps: 532000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 77.91\n",
      "Num timesteps: 533000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 75.68\n",
      "Num timesteps: 534000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 73.69\n",
      "Num timesteps: 535000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 73.03\n",
      "Num timesteps: 536000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 72.89\n",
      "Num timesteps: 537000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 72.57\n",
      "Num timesteps: 538000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 70.44\n",
      "Num timesteps: 539000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 69.87\n",
      "Num timesteps: 540000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 66.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 737      |\n",
      "|    ep_rew_mean        | 66.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1110     |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 486      |\n",
      "|    total_timesteps    | 540000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | -0.185   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -9.23    |\n",
      "|    value_loss         | 99.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 541000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 64.42\n",
      "Num timesteps: 542000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 61.67\n",
      "Num timesteps: 543000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 58.31\n",
      "Num timesteps: 544000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 56.53\n",
      "Num timesteps: 545000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 56.18\n",
      "Num timesteps: 546000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 53.78\n",
      "Num timesteps: 547000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 52.22\n",
      "Num timesteps: 548000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 52.83\n",
      "Num timesteps: 549000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 55.38\n",
      "Num timesteps: 550000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 58.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 755      |\n",
      "|    ep_rew_mean        | 58.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1088     |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 505      |\n",
      "|    total_timesteps    | 550000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | 0.108    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 19.1     |\n",
      "|    value_loss         | 644      |\n",
      "------------------------------------\n",
      "Num timesteps: 551000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 61.54\n",
      "Num timesteps: 552000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 62.84\n",
      "Num timesteps: 553000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 65.17\n",
      "Num timesteps: 554000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 66.12\n",
      "Num timesteps: 555000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 67.93\n",
      "Num timesteps: 556000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 71.22\n",
      "Num timesteps: 557000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 74.28\n",
      "Num timesteps: 558000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 73.61\n",
      "Num timesteps: 559000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 71.17\n",
      "Num timesteps: 560000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 71.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 758      |\n",
      "|    ep_rew_mean        | 71.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1072     |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 521      |\n",
      "|    total_timesteps    | 560000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.48    |\n",
      "|    explained_variance | 0.185    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 21.6     |\n",
      "|    value_loss         | 4e+03    |\n",
      "------------------------------------\n",
      "Num timesteps: 561000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 73.77\n",
      "Num timesteps: 562000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 71.49\n",
      "Num timesteps: 563000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 72.67\n",
      "Num timesteps: 564000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 76.99\n",
      "Num timesteps: 565000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 83.81\n",
      "Num timesteps: 566000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 87.43\n",
      "Num timesteps: 567000\n",
      "Best mean reward: 89.16 - Last mean reward per episode: 91.19\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 568000\n",
      "Best mean reward: 91.19 - Last mean reward per episode: 93.75\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 569000\n",
      "Best mean reward: 93.75 - Last mean reward per episode: 94.19\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 570000\n",
      "Best mean reward: 94.19 - Last mean reward per episode: 94.90\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 725      |\n",
      "|    ep_rew_mean        | 94.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1066     |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 534      |\n",
      "|    total_timesteps    | 570000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | 0.00459  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -1.27    |\n",
      "|    value_loss         | 14.5     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 571000\n",
      "Best mean reward: 94.90 - Last mean reward per episode: 93.59\n",
      "Num timesteps: 572000\n",
      "Best mean reward: 94.90 - Last mean reward per episode: 91.93\n",
      "Num timesteps: 573000\n",
      "Best mean reward: 94.90 - Last mean reward per episode: 91.67\n",
      "Num timesteps: 574000\n",
      "Best mean reward: 94.90 - Last mean reward per episode: 93.96\n",
      "Num timesteps: 575000\n",
      "Best mean reward: 94.90 - Last mean reward per episode: 94.92\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 576000\n",
      "Best mean reward: 94.92 - Last mean reward per episode: 92.16\n",
      "Num timesteps: 577000\n",
      "Best mean reward: 94.92 - Last mean reward per episode: 93.97\n",
      "Num timesteps: 578000\n",
      "Best mean reward: 94.92 - Last mean reward per episode: 96.31\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 579000\n",
      "Best mean reward: 96.31 - Last mean reward per episode: 96.19\n",
      "Num timesteps: 580000\n",
      "Best mean reward: 96.31 - Last mean reward per episode: 97.95\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 744      |\n",
      "|    ep_rew_mean        | 97.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1058     |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 548      |\n",
      "|    total_timesteps    | 580000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.391    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -8.55    |\n",
      "|    value_loss         | 77       |\n",
      "------------------------------------\n",
      "Num timesteps: 581000\n",
      "Best mean reward: 97.95 - Last mean reward per episode: 99.47\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 582000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 96.82\n",
      "Num timesteps: 583000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 97.88\n",
      "Num timesteps: 584000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 98.49\n",
      "Num timesteps: 585000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 95.87\n",
      "Num timesteps: 586000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 95.22\n",
      "Num timesteps: 587000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 97.80\n",
      "Num timesteps: 588000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 97.30\n",
      "Num timesteps: 589000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 96.09\n",
      "Num timesteps: 590000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 95.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 755      |\n",
      "|    ep_rew_mean        | 95.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1047     |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 563      |\n",
      "|    total_timesteps    | 590000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.264    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.926   |\n",
      "|    value_loss         | 5.12     |\n",
      "------------------------------------\n",
      "Num timesteps: 591000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 98.49\n",
      "Num timesteps: 592000\n",
      "Best mean reward: 99.47 - Last mean reward per episode: 101.33\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 593000\n",
      "Best mean reward: 101.33 - Last mean reward per episode: 99.48\n",
      "Num timesteps: 594000\n",
      "Best mean reward: 101.33 - Last mean reward per episode: 102.98\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 595000\n",
      "Best mean reward: 102.98 - Last mean reward per episode: 105.99\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 596000\n",
      "Best mean reward: 105.99 - Last mean reward per episode: 105.89\n",
      "Num timesteps: 597000\n",
      "Best mean reward: 105.99 - Last mean reward per episode: 107.35\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 598000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 103.90\n",
      "Num timesteps: 599000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 104.89\n",
      "Num timesteps: 600000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 102.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 728      |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1044     |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 574      |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.585    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -5.65    |\n",
      "|    value_loss         | 43.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 601000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 103.19\n",
      "Num timesteps: 602000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 102.41\n",
      "Num timesteps: 603000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 102.72\n",
      "Num timesteps: 604000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 102.75\n",
      "Num timesteps: 605000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 101.24\n",
      "Num timesteps: 606000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 98.98\n",
      "Num timesteps: 607000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 99.18\n",
      "Num timesteps: 608000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 94.05\n",
      "Num timesteps: 609000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 94.35\n",
      "Num timesteps: 610000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 96.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 716      |\n",
      "|    ep_rew_mean        | 96.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1039     |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 586      |\n",
      "|    total_timesteps    | 610000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | -2.49    |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 611000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 100.66\n",
      "Num timesteps: 612000\n",
      "Best mean reward: 107.35 - Last mean reward per episode: 107.87\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 613000\n",
      "Best mean reward: 107.87 - Last mean reward per episode: 113.36\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 614000\n",
      "Best mean reward: 113.36 - Last mean reward per episode: 118.95\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 615000\n",
      "Best mean reward: 118.95 - Last mean reward per episode: 119.59\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 616000\n",
      "Best mean reward: 119.59 - Last mean reward per episode: 120.69\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 617000\n",
      "Best mean reward: 120.69 - Last mean reward per episode: 121.81\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 618000\n",
      "Best mean reward: 121.81 - Last mean reward per episode: 123.14\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 619000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 121.91\n",
      "Num timesteps: 620000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 117.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 621      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1040     |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 595      |\n",
      "|    total_timesteps    | 620000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.453    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 5.63     |\n",
      "|    value_loss         | 126      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 621000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 120.72\n",
      "Num timesteps: 622000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 122.27\n",
      "Num timesteps: 623000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 119.66\n",
      "Num timesteps: 624000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 118.38\n",
      "Num timesteps: 625000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 118.82\n",
      "Num timesteps: 626000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 116.44\n",
      "Num timesteps: 627000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 119.09\n",
      "Num timesteps: 628000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 117.32\n",
      "Num timesteps: 629000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 119.70\n",
      "Num timesteps: 630000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 120.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 568      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1041     |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 604      |\n",
      "|    total_timesteps    | 630000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.754   |\n",
      "|    explained_variance | 0.00208  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 4.7      |\n",
      "|    value_loss         | 1.22e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 631000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 115.91\n",
      "Num timesteps: 632000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 119.98\n",
      "Num timesteps: 633000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 120.43\n",
      "Num timesteps: 634000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 116.90\n",
      "Num timesteps: 635000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 119.22\n",
      "Num timesteps: 636000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 116.16\n",
      "Num timesteps: 637000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 120.73\n",
      "Num timesteps: 638000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 121.88\n",
      "Num timesteps: 639000\n",
      "Best mean reward: 123.14 - Last mean reward per episode: 124.70\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 640000\n",
      "Best mean reward: 124.70 - Last mean reward per episode: 127.37\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 496      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1042     |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 613      |\n",
      "|    total_timesteps    | 640000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.276    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 3.02     |\n",
      "|    value_loss         | 38.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 641000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 125.43\n",
      "Num timesteps: 642000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 127.00\n",
      "Num timesteps: 643000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 127.08\n",
      "Num timesteps: 644000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 124.40\n",
      "Num timesteps: 645000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 127.16\n",
      "Num timesteps: 646000\n",
      "Best mean reward: 127.37 - Last mean reward per episode: 128.19\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 647000\n",
      "Best mean reward: 128.19 - Last mean reward per episode: 128.91\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 648000\n",
      "Best mean reward: 128.91 - Last mean reward per episode: 130.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 649000\n",
      "Best mean reward: 130.48 - Last mean reward per episode: 132.12\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 650000\n",
      "Best mean reward: 132.12 - Last mean reward per episode: 130.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 474      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1040     |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 624      |\n",
      "|    total_timesteps    | 650000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.629    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | -11.4    |\n",
      "|    value_loss         | 119      |\n",
      "------------------------------------\n",
      "Num timesteps: 651000\n",
      "Best mean reward: 132.12 - Last mean reward per episode: 132.20\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 652000\n",
      "Best mean reward: 132.20 - Last mean reward per episode: 131.40\n",
      "Num timesteps: 653000\n",
      "Best mean reward: 132.20 - Last mean reward per episode: 134.53\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 654000\n",
      "Best mean reward: 134.53 - Last mean reward per episode: 137.21\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 655000\n",
      "Best mean reward: 137.21 - Last mean reward per episode: 142.99\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 656000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 140.99\n",
      "Num timesteps: 657000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 138.42\n",
      "Num timesteps: 658000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 138.05\n",
      "Num timesteps: 659000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 138.09\n",
      "Num timesteps: 660000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 471      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1037     |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 635      |\n",
      "|    total_timesteps    | 660000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.26    |\n",
      "|    explained_variance | -2.95    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 4.18     |\n",
      "|    value_loss         | 86.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 661000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 136.13\n",
      "Num timesteps: 662000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.28\n",
      "Num timesteps: 663000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.47\n",
      "Num timesteps: 664000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.83\n",
      "Num timesteps: 665000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.18\n",
      "Num timesteps: 666000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.52\n",
      "Num timesteps: 667000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.26\n",
      "Num timesteps: 668000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.06\n",
      "Num timesteps: 669000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.62\n",
      "Num timesteps: 670000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 467      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1037     |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 645      |\n",
      "|    total_timesteps    | 670000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.23    |\n",
      "|    explained_variance | -0.0314  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 4.42     |\n",
      "|    value_loss         | 1.95e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 671000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.17\n",
      "Num timesteps: 672000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.10\n",
      "Num timesteps: 673000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 674000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.52\n",
      "Num timesteps: 675000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.90\n",
      "Num timesteps: 676000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.75\n",
      "Num timesteps: 677000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.84\n",
      "Num timesteps: 678000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.08\n",
      "Num timesteps: 679000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.18\n",
      "Num timesteps: 680000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 136.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 484      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1036     |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 655      |\n",
      "|    total_timesteps    | 680000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.182    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 16       |\n",
      "|    value_loss         | 349      |\n",
      "------------------------------------\n",
      "Num timesteps: 681000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.48\n",
      "Num timesteps: 682000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.76\n",
      "Num timesteps: 683000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.97\n",
      "Num timesteps: 684000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.73\n",
      "Num timesteps: 685000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.64\n",
      "Num timesteps: 686000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.86\n",
      "Num timesteps: 687000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.31\n",
      "Num timesteps: 688000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.83\n",
      "Num timesteps: 689000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.21\n",
      "Num timesteps: 690000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 496      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1036     |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 665      |\n",
      "|    total_timesteps    | 690000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 1.85     |\n",
      "|    value_loss         | 9.28     |\n",
      "------------------------------------\n",
      "Num timesteps: 691000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.58\n",
      "Num timesteps: 692000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.56\n",
      "Num timesteps: 693000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.52\n",
      "Num timesteps: 694000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.60\n",
      "Num timesteps: 695000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.84\n",
      "Num timesteps: 696000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.30\n",
      "Num timesteps: 697000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 128.38\n",
      "Num timesteps: 698000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.29\n",
      "Num timesteps: 699000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.69\n",
      "Num timesteps: 700000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 478      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1037     |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 675      |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -1.99    |\n",
      "|    value_loss         | 33.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 701000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 127.41\n",
      "Num timesteps: 702000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 127.06\n",
      "Num timesteps: 703000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.48\n",
      "Num timesteps: 704000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.16\n",
      "Num timesteps: 705000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.21\n",
      "Num timesteps: 706000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.39\n",
      "Num timesteps: 707000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 128.18\n",
      "Num timesteps: 708000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 128.63\n",
      "Num timesteps: 709000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.88\n",
      "Num timesteps: 710000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 477      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1036     |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 684      |\n",
      "|    total_timesteps    | 710000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.0659   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 18.8     |\n",
      "|    value_loss         | 743      |\n",
      "------------------------------------\n",
      "Num timesteps: 711000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.90\n",
      "Num timesteps: 712000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.48\n",
      "Num timesteps: 713000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.65\n",
      "Num timesteps: 714000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.55\n",
      "Num timesteps: 715000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 138.82\n",
      "Num timesteps: 716000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 142.99\n",
      "Num timesteps: 717000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 139.70\n",
      "Num timesteps: 718000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.23\n",
      "Num timesteps: 719000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 132.61\n",
      "Num timesteps: 720000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 136.08\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 486      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1036     |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 694      |\n",
      "|    total_timesteps    | 720000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.662   |\n",
      "|    explained_variance | 0.215    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 0.159    |\n",
      "|    value_loss         | 5.73     |\n",
      "------------------------------------\n",
      "Num timesteps: 721000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 133.86\n",
      "Num timesteps: 722000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.66\n",
      "Num timesteps: 723000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 121.76\n",
      "Num timesteps: 724000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 118.84\n",
      "Num timesteps: 725000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 112.89\n",
      "Num timesteps: 726000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 115.28\n",
      "Num timesteps: 727000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 113.98\n",
      "Num timesteps: 728000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 111.12\n",
      "Num timesteps: 729000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 115.15\n",
      "Num timesteps: 730000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 113.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 456      |\n",
      "|    ep_rew_mean        | 114      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1038     |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 702      |\n",
      "|    total_timesteps    | 730000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.469   |\n",
      "|    explained_variance | -1.48    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 14.9     |\n",
      "|    value_loss         | 483      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 731000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 113.05\n",
      "Num timesteps: 732000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 113.57\n",
      "Num timesteps: 733000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 111.18\n",
      "Num timesteps: 734000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 111.44\n",
      "Num timesteps: 735000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 111.18\n",
      "Num timesteps: 736000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 106.96\n",
      "Num timesteps: 737000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 103.43\n",
      "Num timesteps: 738000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 106.77\n",
      "Num timesteps: 739000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 103.28\n",
      "Num timesteps: 740000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 98.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | 98.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1042     |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 709      |\n",
      "|    total_timesteps    | 740000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.816   |\n",
      "|    explained_variance | -0.0407  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | -73.7    |\n",
      "|    value_loss         | 1.21e+04 |\n",
      "------------------------------------\n",
      "Num timesteps: 741000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 98.75\n",
      "Num timesteps: 742000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 96.47\n",
      "Num timesteps: 743000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 92.47\n",
      "Num timesteps: 744000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.63\n",
      "Num timesteps: 745000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.65\n",
      "Num timesteps: 746000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 91.42\n",
      "Num timesteps: 747000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 85.23\n",
      "Num timesteps: 748000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 86.51\n",
      "Num timesteps: 749000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 89.41\n",
      "Num timesteps: 750000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 90.72\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 90.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1044     |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 717      |\n",
      "|    total_timesteps    | 750000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.944   |\n",
      "|    explained_variance | 0.37     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -5       |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "Num timesteps: 751000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 85.91\n",
      "Num timesteps: 752000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 85.02\n",
      "Num timesteps: 753000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 83.33\n",
      "Num timesteps: 754000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 87.97\n",
      "Num timesteps: 755000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 90.87\n",
      "Num timesteps: 756000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 89.36\n",
      "Num timesteps: 757000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 87.48\n",
      "Num timesteps: 758000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 90.63\n",
      "Num timesteps: 759000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.44\n",
      "Num timesteps: 760000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 99.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 359      |\n",
      "|    ep_rew_mean        | 99.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1046     |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 726      |\n",
      "|    total_timesteps    | 760000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 1.25     |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "Num timesteps: 761000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 95.07\n",
      "Num timesteps: 762000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.08\n",
      "Num timesteps: 763000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 94.13\n",
      "Num timesteps: 764000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 92.61\n",
      "Num timesteps: 765000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 95.91\n",
      "Num timesteps: 766000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 97.47\n",
      "Num timesteps: 767000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 97.98\n",
      "Num timesteps: 768000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 92.00\n",
      "Num timesteps: 769000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.05\n",
      "Num timesteps: 770000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 93.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 93.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1048     |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 734      |\n",
      "|    total_timesteps    | 770000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.755   |\n",
      "|    explained_variance | 0.133    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -39.4    |\n",
      "|    value_loss         | 6.96e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 771000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 94.54\n",
      "Num timesteps: 772000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 96.94\n",
      "Num timesteps: 773000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 102.42\n",
      "Num timesteps: 774000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 102.31\n",
      "Num timesteps: 775000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 104.22\n",
      "Num timesteps: 776000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 108.07\n",
      "Num timesteps: 777000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 110.84\n",
      "Num timesteps: 778000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 114.55\n",
      "Num timesteps: 779000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 119.65\n",
      "Num timesteps: 780000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 119.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 386      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1050     |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 742      |\n",
      "|    total_timesteps    | 780000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.14    |\n",
      "|    explained_variance | 0.36     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -5.85    |\n",
      "|    value_loss         | 38.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 781000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.20\n",
      "Num timesteps: 782000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 125.60\n",
      "Num timesteps: 783000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 123.15\n",
      "Num timesteps: 784000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 122.75\n",
      "Num timesteps: 785000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.40\n",
      "Num timesteps: 786000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.55\n",
      "Num timesteps: 787000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.10\n",
      "Num timesteps: 788000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 126.13\n",
      "Num timesteps: 789000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.53\n",
      "Num timesteps: 790000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 128.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 384      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1053     |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 749      |\n",
      "|    total_timesteps    | 790000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.528    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 4.22     |\n",
      "|    value_loss         | 70.6     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 791000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.00\n",
      "Num timesteps: 792000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 130.24\n",
      "Num timesteps: 793000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 129.37\n",
      "Num timesteps: 794000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.45\n",
      "Num timesteps: 795000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 135.67\n",
      "Num timesteps: 796000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 131.80\n",
      "Num timesteps: 797000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 136.98\n",
      "Num timesteps: 798000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 138.75\n",
      "Num timesteps: 799000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 141.94\n",
      "Num timesteps: 800000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 142.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 366      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1057     |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 756      |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 2.5      |\n",
      "|    value_loss         | 33       |\n",
      "------------------------------------\n",
      "Num timesteps: 801000\n",
      "Best mean reward: 142.99 - Last mean reward per episode: 147.02\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 802000\n",
      "Best mean reward: 147.02 - Last mean reward per episode: 142.41\n",
      "Num timesteps: 803000\n",
      "Best mean reward: 147.02 - Last mean reward per episode: 144.55\n",
      "Num timesteps: 804000\n",
      "Best mean reward: 147.02 - Last mean reward per episode: 145.52\n",
      "Num timesteps: 805000\n",
      "Best mean reward: 147.02 - Last mean reward per episode: 146.48\n",
      "Num timesteps: 806000\n",
      "Best mean reward: 147.02 - Last mean reward per episode: 151.91\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 807000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 146.70\n",
      "Num timesteps: 808000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 140.98\n",
      "Num timesteps: 809000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 140.98\n",
      "Num timesteps: 810000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 140.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1060     |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 763      |\n",
      "|    total_timesteps    | 810000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.544    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | -1.94    |\n",
      "|    value_loss         | 27.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 811000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 143.93\n",
      "Num timesteps: 812000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 145.63\n",
      "Num timesteps: 813000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 145.34\n",
      "Num timesteps: 814000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 142.00\n",
      "Num timesteps: 815000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 136.05\n",
      "Num timesteps: 816000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 137.86\n",
      "Num timesteps: 817000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.83\n",
      "Num timesteps: 818000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.00\n",
      "Num timesteps: 819000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.39\n",
      "Num timesteps: 820000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 328      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1064     |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 770      |\n",
      "|    total_timesteps    | 820000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.876   |\n",
      "|    explained_variance | 0.0917   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -39.6    |\n",
      "|    value_loss         | 7.15e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 821000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.48\n",
      "Num timesteps: 822000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.23\n",
      "Num timesteps: 823000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.04\n",
      "Num timesteps: 824000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.72\n",
      "Num timesteps: 825000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.59\n",
      "Num timesteps: 826000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.48\n",
      "Num timesteps: 827000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.01\n",
      "Num timesteps: 828000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.50\n",
      "Num timesteps: 829000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.46\n",
      "Num timesteps: 830000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 343      |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1067     |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 777      |\n",
      "|    total_timesteps    | 830000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -7.34    |\n",
      "|    value_loss         | 52.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 831000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.60\n",
      "Num timesteps: 832000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.89\n",
      "Num timesteps: 833000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.27\n",
      "Num timesteps: 834000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.18\n",
      "Num timesteps: 835000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.19\n",
      "Num timesteps: 836000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.47\n",
      "Num timesteps: 837000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.16\n",
      "Num timesteps: 838000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.28\n",
      "Num timesteps: 839000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.72\n",
      "Num timesteps: 840000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 337      |\n",
      "|    ep_rew_mean        | 98.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1069     |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 785      |\n",
      "|    total_timesteps    | 840000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | -1.55    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 2.64     |\n",
      "|    value_loss         | 91.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 841000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.14\n",
      "Num timesteps: 842000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.60\n",
      "Num timesteps: 843000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.16\n",
      "Num timesteps: 844000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.11\n",
      "Num timesteps: 845000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.67\n",
      "Num timesteps: 846000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.69\n",
      "Num timesteps: 847000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.25\n",
      "Num timesteps: 848000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 849000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.84\n",
      "Num timesteps: 850000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 93.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1073     |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 792      |\n",
      "|    total_timesteps    | 850000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.238    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 13       |\n",
      "|    value_loss         | 453      |\n",
      "------------------------------------\n",
      "Num timesteps: 851000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.96\n",
      "Num timesteps: 852000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.98\n",
      "Num timesteps: 853000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.04\n",
      "Num timesteps: 854000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.51\n",
      "Num timesteps: 855000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.84\n",
      "Num timesteps: 856000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.92\n",
      "Num timesteps: 857000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.28\n",
      "Num timesteps: 858000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 76.48\n",
      "Num timesteps: 859000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 74.55\n",
      "Num timesteps: 860000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 72.74\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 72.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1077     |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 798      |\n",
      "|    total_timesteps    | 860000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.225    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -5.64    |\n",
      "|    value_loss         | 3.18e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 861000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.06\n",
      "Num timesteps: 862000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 74.80\n",
      "Num timesteps: 863000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.34\n",
      "Num timesteps: 864000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 73.73\n",
      "Num timesteps: 865000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 71.78\n",
      "Num timesteps: 866000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.26\n",
      "Num timesteps: 867000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 71.76\n",
      "Num timesteps: 868000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 67.29\n",
      "Num timesteps: 869000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 63.11\n",
      "Num timesteps: 870000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 66.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1081     |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 804      |\n",
      "|    total_timesteps    | 870000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.956   |\n",
      "|    explained_variance | 0.499    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -13.6    |\n",
      "|    value_loss         | 280      |\n",
      "------------------------------------\n",
      "Num timesteps: 871000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 64.10\n",
      "Num timesteps: 872000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 67.63\n",
      "Num timesteps: 873000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 68.08\n",
      "Num timesteps: 874000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.03\n",
      "Num timesteps: 875000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 63.75\n",
      "Num timesteps: 876000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 64.77\n",
      "Num timesteps: 877000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.71\n",
      "Num timesteps: 878000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.79\n",
      "Num timesteps: 879000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 61.90\n",
      "Num timesteps: 880000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 256      |\n",
      "|    ep_rew_mean        | 62.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1086     |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 809      |\n",
      "|    total_timesteps    | 880000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.688   |\n",
      "|    explained_variance | 0.0772   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 12.4     |\n",
      "|    value_loss         | 938      |\n",
      "------------------------------------\n",
      "Num timesteps: 881000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 61.12\n",
      "Num timesteps: 882000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.33\n",
      "Num timesteps: 883000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 70.80\n",
      "Num timesteps: 884000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.95\n",
      "Num timesteps: 885000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.40\n",
      "Num timesteps: 886000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.91\n",
      "Num timesteps: 887000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.60\n",
      "Num timesteps: 888000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.85\n",
      "Num timesteps: 889000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.67\n",
      "Num timesteps: 890000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 259      |\n",
      "|    ep_rew_mean        | 90.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1090     |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 816      |\n",
      "|    total_timesteps    | 890000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.74    |\n",
      "|    explained_variance | -2.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 29       |\n",
      "|    value_loss         | 1.36e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 891000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.99\n",
      "Num timesteps: 892000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.59\n",
      "Num timesteps: 893000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.73\n",
      "Num timesteps: 894000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.63\n",
      "Num timesteps: 895000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.40\n",
      "Num timesteps: 896000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.07\n",
      "Num timesteps: 897000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.97\n",
      "Num timesteps: 898000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.25\n",
      "Num timesteps: 899000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.95\n",
      "Num timesteps: 900000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 270      |\n",
      "|    ep_rew_mean        | 97.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1094     |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 822      |\n",
      "|    total_timesteps    | 900000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.609   |\n",
      "|    explained_variance | 3.56e-05 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -6.27    |\n",
      "|    value_loss         | 155      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 901000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.87\n",
      "Num timesteps: 902000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.12\n",
      "Num timesteps: 903000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.36\n",
      "Num timesteps: 904000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.88\n",
      "Num timesteps: 905000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.08\n",
      "Num timesteps: 906000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.98\n",
      "Num timesteps: 907000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.03\n",
      "Num timesteps: 908000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.46\n",
      "Num timesteps: 909000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.07\n",
      "Num timesteps: 910000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1097     |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 828      |\n",
      "|    total_timesteps    | 910000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.266    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 6.9      |\n",
      "|    value_loss         | 231      |\n",
      "------------------------------------\n",
      "Num timesteps: 911000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.32\n",
      "Num timesteps: 912000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.93\n",
      "Num timesteps: 913000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.35\n",
      "Num timesteps: 914000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.20\n",
      "Num timesteps: 915000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.31\n",
      "Num timesteps: 916000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.18\n",
      "Num timesteps: 917000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.99\n",
      "Num timesteps: 918000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.47\n",
      "Num timesteps: 919000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.33\n",
      "Num timesteps: 920000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1099     |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 836      |\n",
      "|    total_timesteps    | 920000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 4.23     |\n",
      "|    value_loss         | 56.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 921000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.58\n",
      "Num timesteps: 922000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.16\n",
      "Num timesteps: 923000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.28\n",
      "Num timesteps: 924000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.74\n",
      "Num timesteps: 925000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.84\n",
      "Num timesteps: 926000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.98\n",
      "Num timesteps: 927000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.15\n",
      "Num timesteps: 928000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.79\n",
      "Num timesteps: 929000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.52\n",
      "Num timesteps: 930000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 105      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1104     |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 842      |\n",
      "|    total_timesteps    | 930000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | -10.9    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 9.36     |\n",
      "|    value_loss         | 570      |\n",
      "------------------------------------\n",
      "Num timesteps: 931000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.53\n",
      "Num timesteps: 932000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.72\n",
      "Num timesteps: 933000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.92\n",
      "Num timesteps: 934000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.38\n",
      "Num timesteps: 935000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.87\n",
      "Num timesteps: 936000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.24\n",
      "Num timesteps: 937000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.20\n",
      "Num timesteps: 938000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.98\n",
      "Num timesteps: 939000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.48\n",
      "Num timesteps: 940000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 293      |\n",
      "|    ep_rew_mean        | 96.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1108     |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 848      |\n",
      "|    total_timesteps    | 940000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.977   |\n",
      "|    explained_variance | -0.0206  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 10.4     |\n",
      "|    value_loss         | 948      |\n",
      "------------------------------------\n",
      "Num timesteps: 941000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.39\n",
      "Num timesteps: 942000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.27\n",
      "Num timesteps: 943000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.46\n",
      "Num timesteps: 944000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.24\n",
      "Num timesteps: 945000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.94\n",
      "Num timesteps: 946000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 73.94\n",
      "Num timesteps: 947000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 74.04\n",
      "Num timesteps: 948000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 74.73\n",
      "Num timesteps: 949000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 72.95\n",
      "Num timesteps: 950000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 68.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 243      |\n",
      "|    ep_rew_mean        | 68.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1113     |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 853      |\n",
      "|    total_timesteps    | 950000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.723   |\n",
      "|    explained_variance | -1.17    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -2.72    |\n",
      "|    value_loss         | 143      |\n",
      "------------------------------------\n",
      "Num timesteps: 951000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.83\n",
      "Num timesteps: 952000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 70.61\n",
      "Num timesteps: 953000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.53\n",
      "Num timesteps: 954000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 68.22\n",
      "Num timesteps: 955000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 68.08\n",
      "Num timesteps: 956000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 76.15\n",
      "Num timesteps: 957000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.49\n",
      "Num timesteps: 958000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.13\n",
      "Num timesteps: 959000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.82\n",
      "Num timesteps: 960000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 271      |\n",
      "|    ep_rew_mean        | 88       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1115     |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 860      |\n",
      "|    total_timesteps    | 960000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.248   |\n",
      "|    explained_variance | 0.287    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -1.27    |\n",
      "|    value_loss         | 949      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 961000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.55\n",
      "Num timesteps: 962000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.39\n",
      "Num timesteps: 963000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.08\n",
      "Num timesteps: 964000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.46\n",
      "Num timesteps: 965000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.06\n",
      "Num timesteps: 966000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.60\n",
      "Num timesteps: 967000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.23\n",
      "Num timesteps: 968000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.20\n",
      "Num timesteps: 969000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.37\n",
      "Num timesteps: 970000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 271      |\n",
      "|    ep_rew_mean        | 95.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1119     |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 866      |\n",
      "|    total_timesteps    | 970000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.653   |\n",
      "|    explained_variance | -0.698   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -2.62    |\n",
      "|    value_loss         | 23.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 971000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.16\n",
      "Num timesteps: 972000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.20\n",
      "Num timesteps: 973000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.27\n",
      "Num timesteps: 974000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.96\n",
      "Num timesteps: 975000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.54\n",
      "Num timesteps: 976000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.29\n",
      "Num timesteps: 977000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.69\n",
      "Num timesteps: 978000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.78\n",
      "Num timesteps: 979000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.40\n",
      "Num timesteps: 980000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 281      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1123     |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 872      |\n",
      "|    total_timesteps    | 980000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.998   |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -11.2    |\n",
      "|    value_loss         | 135      |\n",
      "------------------------------------\n",
      "Num timesteps: 981000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.95\n",
      "Num timesteps: 982000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.95\n",
      "Num timesteps: 983000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.70\n",
      "Num timesteps: 984000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.61\n",
      "Num timesteps: 985000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.65\n",
      "Num timesteps: 986000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.40\n",
      "Num timesteps: 987000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.87\n",
      "Num timesteps: 988000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.73\n",
      "Num timesteps: 989000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.03\n",
      "Num timesteps: 990000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 246      |\n",
      "|    ep_rew_mean        | 90.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1128     |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 877      |\n",
      "|    total_timesteps    | 990000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.639   |\n",
      "|    value_loss         | 122      |\n",
      "------------------------------------\n",
      "Num timesteps: 991000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.61\n",
      "Num timesteps: 992000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.62\n",
      "Num timesteps: 993000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.26\n",
      "Num timesteps: 994000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.96\n",
      "Num timesteps: 995000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.16\n",
      "Num timesteps: 996000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.90\n",
      "Num timesteps: 997000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.48\n",
      "Num timesteps: 998000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.75\n",
      "Num timesteps: 999000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.02\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 226      |\n",
      "|    ep_rew_mean        | 85.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1133     |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 882      |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.445    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -15.6    |\n",
      "|    value_loss         | 309      |\n",
      "------------------------------------\n",
      "Num timesteps: 1001000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.14\n",
      "Num timesteps: 1002000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.21\n",
      "Num timesteps: 1003000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.13\n",
      "Num timesteps: 1004000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.69\n",
      "Num timesteps: 1005000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.88\n",
      "Num timesteps: 1006000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.71\n",
      "Num timesteps: 1007000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.62\n",
      "Num timesteps: 1008000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.95\n",
      "Num timesteps: 1009000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.25\n",
      "Num timesteps: 1010000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 234      |\n",
      "|    ep_rew_mean        | 86.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1137     |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 887      |\n",
      "|    total_timesteps    | 1010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.323    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | -39      |\n",
      "|    value_loss         | 5.82e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1011000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.21\n",
      "Num timesteps: 1012000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.69\n",
      "Num timesteps: 1013000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.18\n",
      "Num timesteps: 1014000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.26\n",
      "Num timesteps: 1015000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.15\n",
      "Num timesteps: 1016000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.53\n",
      "Num timesteps: 1017000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.23\n",
      "Num timesteps: 1018000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.32\n",
      "Num timesteps: 1019000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1020000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 242      |\n",
      "|    ep_rew_mean        | 84.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1141     |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 893      |\n",
      "|    total_timesteps    | 1020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.18     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -17.4    |\n",
      "|    value_loss         | 350      |\n",
      "------------------------------------\n",
      "Num timesteps: 1021000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.80\n",
      "Num timesteps: 1022000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.08\n",
      "Num timesteps: 1023000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.80\n",
      "Num timesteps: 1024000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.01\n",
      "Num timesteps: 1025000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.64\n",
      "Num timesteps: 1026000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.78\n",
      "Num timesteps: 1027000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.80\n",
      "Num timesteps: 1028000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.43\n",
      "Num timesteps: 1029000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.63\n",
      "Num timesteps: 1030000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 241      |\n",
      "|    ep_rew_mean        | 75.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1146     |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 898      |\n",
      "|    total_timesteps    | 1030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.227    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | -31.5    |\n",
      "|    value_loss         | 3e+03    |\n",
      "------------------------------------\n",
      "Num timesteps: 1031000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.04\n",
      "Num timesteps: 1032000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 68.26\n",
      "Num timesteps: 1033000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 71.31\n",
      "Num timesteps: 1034000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.87\n",
      "Num timesteps: 1035000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 71.75\n",
      "Num timesteps: 1036000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.74\n",
      "Num timesteps: 1037000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 64.17\n",
      "Num timesteps: 1038000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 63.37\n",
      "Num timesteps: 1039000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 59.12\n",
      "Num timesteps: 1040000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 59.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 226      |\n",
      "|    ep_rew_mean        | 59.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1151     |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 903      |\n",
      "|    total_timesteps    | 1040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.993   |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 1.15     |\n",
      "|    value_loss         | 3.75     |\n",
      "------------------------------------\n",
      "Num timesteps: 1041000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 64.27\n",
      "Num timesteps: 1042000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.05\n",
      "Num timesteps: 1043000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 56.85\n",
      "Num timesteps: 1044000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 63.75\n",
      "Num timesteps: 1045000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 65.02\n",
      "Num timesteps: 1046000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 64.77\n",
      "Num timesteps: 1047000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.44\n",
      "Num timesteps: 1048000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.45\n",
      "Num timesteps: 1049000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 54.62\n",
      "Num timesteps: 1050000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 58.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | 58.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1155     |\n",
      "|    iterations         | 10500    |\n",
      "|    time_elapsed       | 908      |\n",
      "|    total_timesteps    | 1050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | 0.168    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10499    |\n",
      "|    policy_loss        | -3.09    |\n",
      "|    value_loss         | 804      |\n",
      "------------------------------------\n",
      "Num timesteps: 1051000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 57.96\n",
      "Num timesteps: 1052000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 60.28\n",
      "Num timesteps: 1053000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 61.10\n",
      "Num timesteps: 1054000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 57.54\n",
      "Num timesteps: 1055000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 58.00\n",
      "Num timesteps: 1056000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.27\n",
      "Num timesteps: 1057000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.45\n",
      "Num timesteps: 1058000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.36\n",
      "Num timesteps: 1059000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 56.81\n",
      "Num timesteps: 1060000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.91\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | 55.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1160     |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 913      |\n",
      "|    total_timesteps    | 1060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | 0.186    |\n",
      "|    value_loss         | 10       |\n",
      "------------------------------------\n",
      "Num timesteps: 1061000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 58.58\n",
      "Num timesteps: 1062000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 51.87\n",
      "Num timesteps: 1063000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 52.55\n",
      "Num timesteps: 1064000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 54.99\n",
      "Num timesteps: 1065000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 57.80\n",
      "Num timesteps: 1066000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 56.93\n",
      "Num timesteps: 1067000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 59.49\n",
      "Num timesteps: 1068000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 57.23\n",
      "Num timesteps: 1069000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 55.50\n",
      "Num timesteps: 1070000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 56.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 231      |\n",
      "|    ep_rew_mean        | 57       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1164     |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 918      |\n",
      "|    total_timesteps    | 1070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | -1.08    |\n",
      "|    value_loss         | 9.44     |\n",
      "------------------------------------\n",
      "Num timesteps: 1071000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 51.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1072000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 53.83\n",
      "Num timesteps: 1073000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 50.14\n",
      "Num timesteps: 1074000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 50.53\n",
      "Num timesteps: 1075000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 46.13\n",
      "Num timesteps: 1076000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 46.38\n",
      "Num timesteps: 1077000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 52.88\n",
      "Num timesteps: 1078000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 51.68\n",
      "Num timesteps: 1079000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 58.07\n",
      "Num timesteps: 1080000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 58.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 245      |\n",
      "|    ep_rew_mean        | 58.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1168     |\n",
      "|    iterations         | 10800    |\n",
      "|    time_elapsed       | 924      |\n",
      "|    total_timesteps    | 1080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.887   |\n",
      "|    explained_variance | -0.102   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | -0.982   |\n",
      "|    value_loss         | 1.04e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1081000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.66\n",
      "Num timesteps: 1082000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 62.12\n",
      "Num timesteps: 1083000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 66.94\n",
      "Num timesteps: 1084000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 70.79\n",
      "Num timesteps: 1085000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 76.07\n",
      "Num timesteps: 1086000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.05\n",
      "Num timesteps: 1087000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.35\n",
      "Num timesteps: 1088000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.76\n",
      "Num timesteps: 1089000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.36\n",
      "Num timesteps: 1090000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 268      |\n",
      "|    ep_rew_mean        | 78.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1170     |\n",
      "|    iterations         | 10900    |\n",
      "|    time_elapsed       | 931      |\n",
      "|    total_timesteps    | 1090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.97    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | 0.135    |\n",
      "|    value_loss         | 12.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 1091000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.51\n",
      "Num timesteps: 1092000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.99\n",
      "Num timesteps: 1093000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.06\n",
      "Num timesteps: 1094000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.52\n",
      "Num timesteps: 1095000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.49\n",
      "Num timesteps: 1096000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.75\n",
      "Num timesteps: 1097000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.55\n",
      "Num timesteps: 1098000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.30\n",
      "Num timesteps: 1099000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.82\n",
      "Num timesteps: 1100000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 83.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1172     |\n",
      "|    iterations         | 11000    |\n",
      "|    time_elapsed       | 937      |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0247  |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | -0.0704  |\n",
      "|    value_loss         | 564      |\n",
      "------------------------------------\n",
      "Num timesteps: 1101000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.63\n",
      "Num timesteps: 1102000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.01\n",
      "Num timesteps: 1103000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.67\n",
      "Num timesteps: 1104000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.33\n",
      "Num timesteps: 1105000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.58\n",
      "Num timesteps: 1106000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.62\n",
      "Num timesteps: 1107000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.28\n",
      "Num timesteps: 1108000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.71\n",
      "Num timesteps: 1109000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.36\n",
      "Num timesteps: 1110000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 92.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1174     |\n",
      "|    iterations         | 11100    |\n",
      "|    time_elapsed       | 944      |\n",
      "|    total_timesteps    | 1110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11099    |\n",
      "|    policy_loss        | -2.08    |\n",
      "|    value_loss         | 16.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 1111000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.22\n",
      "Num timesteps: 1112000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.87\n",
      "Num timesteps: 1113000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.88\n",
      "Num timesteps: 1114000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.54\n",
      "Num timesteps: 1115000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.75\n",
      "Num timesteps: 1116000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.71\n",
      "Num timesteps: 1117000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.50\n",
      "Num timesteps: 1118000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.76\n",
      "Num timesteps: 1119000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.47\n",
      "Num timesteps: 1120000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1177     |\n",
      "|    iterations         | 11200    |\n",
      "|    time_elapsed       | 951      |\n",
      "|    total_timesteps    | 1120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | -0.514   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11199    |\n",
      "|    policy_loss        | 21.7     |\n",
      "|    value_loss         | 891      |\n",
      "------------------------------------\n",
      "Num timesteps: 1121000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.09\n",
      "Num timesteps: 1122000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.94\n",
      "Num timesteps: 1123000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.57\n",
      "Num timesteps: 1124000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.32\n",
      "Num timesteps: 1125000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.79\n",
      "Num timesteps: 1126000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.54\n",
      "Num timesteps: 1127000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.57\n",
      "Num timesteps: 1128000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.72\n",
      "Num timesteps: 1129000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.61\n",
      "Num timesteps: 1130000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.47\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 299      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 957      |\n",
      "|    total_timesteps    | 1130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.654   |\n",
      "|    explained_variance | -0.00204 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | -0.927   |\n",
      "|    value_loss         | 2.09     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1131000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.28\n",
      "Num timesteps: 1132000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.39\n",
      "Num timesteps: 1133000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.10\n",
      "Num timesteps: 1134000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.65\n",
      "Num timesteps: 1135000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.89\n",
      "Num timesteps: 1136000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.23\n",
      "Num timesteps: 1137000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.22\n",
      "Num timesteps: 1138000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.26\n",
      "Num timesteps: 1139000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.68\n",
      "Num timesteps: 1140000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 11400    |\n",
      "|    time_elapsed       | 965      |\n",
      "|    total_timesteps    | 1140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.657   |\n",
      "|    explained_variance | -0.333   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11399    |\n",
      "|    policy_loss        | 0.00336  |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "Num timesteps: 1141000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.96\n",
      "Num timesteps: 1142000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.84\n",
      "Num timesteps: 1143000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.63\n",
      "Num timesteps: 1144000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.50\n",
      "Num timesteps: 1145000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.54\n",
      "Num timesteps: 1146000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.34\n",
      "Num timesteps: 1147000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.66\n",
      "Num timesteps: 1148000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.81\n",
      "Num timesteps: 1149000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.66\n",
      "Num timesteps: 1150000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 330      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 11500    |\n",
      "|    time_elapsed       | 972      |\n",
      "|    total_timesteps    | 1150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.637   |\n",
      "|    explained_variance | -7.6     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11499    |\n",
      "|    policy_loss        | 33.4     |\n",
      "|    value_loss         | 2.27e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1151000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.71\n",
      "Num timesteps: 1152000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.74\n",
      "Num timesteps: 1153000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.90\n",
      "Num timesteps: 1154000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.27\n",
      "Num timesteps: 1155000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.38\n",
      "Num timesteps: 1156000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.95\n",
      "Num timesteps: 1157000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.05\n",
      "Num timesteps: 1158000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.82\n",
      "Num timesteps: 1159000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.62\n",
      "Num timesteps: 1160000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 978      |\n",
      "|    total_timesteps    | 1160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.533   |\n",
      "|    explained_variance | 0.201    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | -0.0942  |\n",
      "|    value_loss         | 584      |\n",
      "------------------------------------\n",
      "Num timesteps: 1161000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.58\n",
      "Num timesteps: 1162000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.47\n",
      "Num timesteps: 1163000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.72\n",
      "Num timesteps: 1164000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.45\n",
      "Num timesteps: 1165000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.06\n",
      "Num timesteps: 1166000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.12\n",
      "Num timesteps: 1167000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.32\n",
      "Num timesteps: 1168000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.25\n",
      "Num timesteps: 1169000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.68\n",
      "Num timesteps: 1170000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 11700    |\n",
      "|    time_elapsed       | 985      |\n",
      "|    total_timesteps    | 1170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.804   |\n",
      "|    explained_variance | -2.35    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | 8.44     |\n",
      "|    value_loss         | 505      |\n",
      "------------------------------------\n",
      "Num timesteps: 1171000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.70\n",
      "Num timesteps: 1172000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.89\n",
      "Num timesteps: 1173000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.08\n",
      "Num timesteps: 1174000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.51\n",
      "Num timesteps: 1175000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.18\n",
      "Num timesteps: 1176000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.56\n",
      "Num timesteps: 1177000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.76\n",
      "Num timesteps: 1178000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.99\n",
      "Num timesteps: 1179000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.46\n",
      "Num timesteps: 1180000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.66\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 350      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 11800    |\n",
      "|    time_elapsed       | 994      |\n",
      "|    total_timesteps    | 1180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.839   |\n",
      "|    explained_variance | -14.4    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | 50.9     |\n",
      "|    value_loss         | 3.4e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 1181000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.13\n",
      "Num timesteps: 1182000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.23\n",
      "Num timesteps: 1183000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.18\n",
      "Num timesteps: 1184000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.86\n",
      "Num timesteps: 1185000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.30\n",
      "Num timesteps: 1186000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.05\n",
      "Num timesteps: 1187000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.35\n",
      "Num timesteps: 1188000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.51\n",
      "Num timesteps: 1189000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1190000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.28\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 100      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 1002     |\n",
      "|    total_timesteps    | 1190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.979   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | 1.01     |\n",
      "|    value_loss         | 9.9      |\n",
      "------------------------------------\n",
      "Num timesteps: 1191000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.70\n",
      "Num timesteps: 1192000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.68\n",
      "Num timesteps: 1193000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.53\n",
      "Num timesteps: 1194000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.79\n",
      "Num timesteps: 1195000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.17\n",
      "Num timesteps: 1196000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.91\n",
      "Num timesteps: 1197000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.77\n",
      "Num timesteps: 1198000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.52\n",
      "Num timesteps: 1199000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.49\n",
      "Num timesteps: 1200000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 374      |\n",
      "|    ep_rew_mean        | 97.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 12000    |\n",
      "|    time_elapsed       | 1010     |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.422    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | -14.3    |\n",
      "|    value_loss         | 2.32e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1201000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.92\n",
      "Num timesteps: 1202000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.41\n",
      "Num timesteps: 1203000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.93\n",
      "Num timesteps: 1204000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.10\n",
      "Num timesteps: 1205000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.84\n",
      "Num timesteps: 1206000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.21\n",
      "Num timesteps: 1207000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.90\n",
      "Num timesteps: 1208000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.27\n",
      "Num timesteps: 1209000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.11\n",
      "Num timesteps: 1210000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 401      |\n",
      "|    ep_rew_mean        | 89.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 1018     |\n",
      "|    total_timesteps    | 1210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | -0.41    |\n",
      "|    value_loss         | 3.48     |\n",
      "------------------------------------\n",
      "Num timesteps: 1211000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.47\n",
      "Num timesteps: 1212000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.75\n",
      "Num timesteps: 1213000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.57\n",
      "Num timesteps: 1214000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.33\n",
      "Num timesteps: 1215000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.16\n",
      "Num timesteps: 1216000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 79.31\n",
      "Num timesteps: 1217000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.97\n",
      "Num timesteps: 1218000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.16\n",
      "Num timesteps: 1219000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.48\n",
      "Num timesteps: 1220000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 72.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 421      |\n",
      "|    ep_rew_mean        | 72.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 1028     |\n",
      "|    total_timesteps    | 1220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | 0.317    |\n",
      "|    value_loss         | 5.53     |\n",
      "------------------------------------\n",
      "Num timesteps: 1221000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.24\n",
      "Num timesteps: 1222000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.38\n",
      "Num timesteps: 1223000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 72.79\n",
      "Num timesteps: 1224000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 73.10\n",
      "Num timesteps: 1225000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 71.29\n",
      "Num timesteps: 1226000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 76.16\n",
      "Num timesteps: 1227000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.10\n",
      "Num timesteps: 1228000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.78\n",
      "Num timesteps: 1229000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.45\n",
      "Num timesteps: 1230000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.66\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 88.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 1037     |\n",
      "|    total_timesteps    | 1230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -44      |\n",
      "|    value_loss         | 4.82e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1231000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.19\n",
      "Num timesteps: 1232000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.16\n",
      "Num timesteps: 1233000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.93\n",
      "Num timesteps: 1234000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.17\n",
      "Num timesteps: 1235000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.27\n",
      "Num timesteps: 1236000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.72\n",
      "Num timesteps: 1237000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.09\n",
      "Num timesteps: 1238000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.98\n",
      "Num timesteps: 1239000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.62\n",
      "Num timesteps: 1240000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 451      |\n",
      "|    ep_rew_mean        | 96.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 1046     |\n",
      "|    total_timesteps    | 1240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.844   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 2.29     |\n",
      "|    value_loss         | 10.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 1241000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1242000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.77\n",
      "Num timesteps: 1243000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.69\n",
      "Num timesteps: 1244000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.47\n",
      "Num timesteps: 1245000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.49\n",
      "Num timesteps: 1246000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.61\n",
      "Num timesteps: 1247000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.22\n",
      "Num timesteps: 1248000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.53\n",
      "Num timesteps: 1249000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.90\n",
      "Num timesteps: 1250000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.15\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 465      |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 1055     |\n",
      "|    total_timesteps    | 1250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -2.92    |\n",
      "|    value_loss         | 10.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1251000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.08\n",
      "Num timesteps: 1252000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.69\n",
      "Num timesteps: 1253000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.11\n",
      "Num timesteps: 1254000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.50\n",
      "Num timesteps: 1255000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.06\n",
      "Num timesteps: 1256000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.13\n",
      "Num timesteps: 1257000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.36\n",
      "Num timesteps: 1258000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.53\n",
      "Num timesteps: 1259000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.66\n",
      "Num timesteps: 1260000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 469      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 12600    |\n",
      "|    time_elapsed       | 1065     |\n",
      "|    total_timesteps    | 1260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.964   |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | -3.05    |\n",
      "|    value_loss         | 11.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 1261000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.81\n",
      "Num timesteps: 1262000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.00\n",
      "Num timesteps: 1263000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.68\n",
      "Num timesteps: 1264000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.91\n",
      "Num timesteps: 1265000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.48\n",
      "Num timesteps: 1266000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.03\n",
      "Num timesteps: 1267000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.83\n",
      "Num timesteps: 1268000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.18\n",
      "Num timesteps: 1269000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.89\n",
      "Num timesteps: 1270000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 475      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1181     |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 1074     |\n",
      "|    total_timesteps    | 1270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.216    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | -69      |\n",
      "|    value_loss         | 7.48e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1271000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.86\n",
      "Num timesteps: 1272000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.09\n",
      "Num timesteps: 1273000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.94\n",
      "Num timesteps: 1274000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.69\n",
      "Num timesteps: 1275000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.72\n",
      "Num timesteps: 1276000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.33\n",
      "Num timesteps: 1277000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.62\n",
      "Num timesteps: 1278000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.71\n",
      "Num timesteps: 1279000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.79\n",
      "Num timesteps: 1280000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 477      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 12800    |\n",
      "|    time_elapsed       | 1084     |\n",
      "|    total_timesteps    | 1280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.975   |\n",
      "|    explained_variance | 0.372    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | -7.5     |\n",
      "|    value_loss         | 96.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1281000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.15\n",
      "Num timesteps: 1282000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.30\n",
      "Num timesteps: 1283000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.81\n",
      "Num timesteps: 1284000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.67\n",
      "Num timesteps: 1285000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.70\n",
      "Num timesteps: 1286000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.54\n",
      "Num timesteps: 1287000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.59\n",
      "Num timesteps: 1288000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.29\n",
      "Num timesteps: 1289000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.15\n",
      "Num timesteps: 1290000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 476      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1178     |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 1094     |\n",
      "|    total_timesteps    | 1290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.867   |\n",
      "|    explained_variance | 0.767    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 9.55     |\n",
      "------------------------------------\n",
      "Num timesteps: 1291000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.89\n",
      "Num timesteps: 1292000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.37\n",
      "Num timesteps: 1293000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.11\n",
      "Num timesteps: 1294000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.76\n",
      "Num timesteps: 1295000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.94\n",
      "Num timesteps: 1296000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.99\n",
      "Num timesteps: 1297000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.17\n",
      "Num timesteps: 1298000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.32\n",
      "Num timesteps: 1299000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.82\n",
      "Num timesteps: 1300000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 487      |\n",
      "|    ep_rew_mean        | 92.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1176     |\n",
      "|    iterations         | 13000    |\n",
      "|    time_elapsed       | 1104     |\n",
      "|    total_timesteps    | 1300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -1.11    |\n",
      "|    value_loss         | 6.44     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1301000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.71\n",
      "Num timesteps: 1302000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.88\n",
      "Num timesteps: 1303000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.45\n",
      "Num timesteps: 1304000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.84\n",
      "Num timesteps: 1305000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.63\n",
      "Num timesteps: 1306000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.59\n",
      "Num timesteps: 1307000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.59\n",
      "Num timesteps: 1308000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.71\n",
      "Num timesteps: 1309000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.21\n",
      "Num timesteps: 1310000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 476      |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 1114     |\n",
      "|    total_timesteps    | 1310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.59    |\n",
      "|    explained_variance | -3.71    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 45.3     |\n",
      "|    value_loss         | 5.24e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1311000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.22\n",
      "Num timesteps: 1312000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.13\n",
      "Num timesteps: 1313000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.02\n",
      "Num timesteps: 1314000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.60\n",
      "Num timesteps: 1315000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.30\n",
      "Num timesteps: 1316000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.80\n",
      "Num timesteps: 1317000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.35\n",
      "Num timesteps: 1318000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.90\n",
      "Num timesteps: 1319000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.59\n",
      "Num timesteps: 1320000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 462      |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 1122     |\n",
      "|    total_timesteps    | 1320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.937   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -5.9     |\n",
      "|    value_loss         | 49.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1321000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.70\n",
      "Num timesteps: 1322000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.46\n",
      "Num timesteps: 1323000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.54\n",
      "Num timesteps: 1324000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.36\n",
      "Num timesteps: 1325000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.05\n",
      "Num timesteps: 1326000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.75\n",
      "Num timesteps: 1327000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.40\n",
      "Num timesteps: 1328000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.33\n",
      "Num timesteps: 1329000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.07\n",
      "Num timesteps: 1330000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | 98.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 1131     |\n",
      "|    total_timesteps    | 1330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.248    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | 17.2     |\n",
      "|    value_loss         | 487      |\n",
      "------------------------------------\n",
      "Num timesteps: 1331000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.88\n",
      "Num timesteps: 1332000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.45\n",
      "Num timesteps: 1333000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.86\n",
      "Num timesteps: 1334000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.00\n",
      "Num timesteps: 1335000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.30\n",
      "Num timesteps: 1336000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.64\n",
      "Num timesteps: 1337000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.37\n",
      "Num timesteps: 1338000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.67\n",
      "Num timesteps: 1339000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.90\n",
      "Num timesteps: 1340000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 451      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1174     |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 1140     |\n",
      "|    total_timesteps    | 1340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.173   |\n",
      "|    explained_variance | -2.14    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | 3.65     |\n",
      "|    value_loss         | 201      |\n",
      "------------------------------------\n",
      "Num timesteps: 1341000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.61\n",
      "Num timesteps: 1342000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.74\n",
      "Num timesteps: 1343000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.89\n",
      "Num timesteps: 1344000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.93\n",
      "Num timesteps: 1345000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.34\n",
      "Num timesteps: 1346000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.99\n",
      "Num timesteps: 1347000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.49\n",
      "Num timesteps: 1348000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.21\n",
      "Num timesteps: 1349000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.72\n",
      "Num timesteps: 1350000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 431      |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1174     |\n",
      "|    iterations         | 13500    |\n",
      "|    time_elapsed       | 1149     |\n",
      "|    total_timesteps    | 1350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | -1.05    |\n",
      "|    value_loss         | 2.73     |\n",
      "------------------------------------\n",
      "Num timesteps: 1351000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.05\n",
      "Num timesteps: 1352000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.00\n",
      "Num timesteps: 1353000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.69\n",
      "Num timesteps: 1354000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.72\n",
      "Num timesteps: 1355000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.99\n",
      "Num timesteps: 1356000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.87\n",
      "Num timesteps: 1357000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.66\n",
      "Num timesteps: 1358000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.57\n",
      "Num timesteps: 1359000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1360000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 439      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1172     |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 1159     |\n",
      "|    total_timesteps    | 1360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.611    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | -2.85    |\n",
      "|    value_loss         | 13.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1361000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.40\n",
      "Num timesteps: 1362000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.07\n",
      "Num timesteps: 1363000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.79\n",
      "Num timesteps: 1364000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.36\n",
      "Num timesteps: 1365000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.98\n",
      "Num timesteps: 1366000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.08\n",
      "Num timesteps: 1367000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.21\n",
      "Num timesteps: 1368000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.36\n",
      "Num timesteps: 1369000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.83\n",
      "Num timesteps: 1370000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 432      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1172     |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 1168     |\n",
      "|    total_timesteps    | 1370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -3.72    |\n",
      "|    value_loss         | 18.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 1371000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.52\n",
      "Num timesteps: 1372000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.07\n",
      "Num timesteps: 1373000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.08\n",
      "Num timesteps: 1374000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.05\n",
      "Num timesteps: 1375000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.85\n",
      "Num timesteps: 1376000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.39\n",
      "Num timesteps: 1377000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.03\n",
      "Num timesteps: 1378000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.14\n",
      "Num timesteps: 1379000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.73\n",
      "Num timesteps: 1380000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 415      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1172     |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 1177     |\n",
      "|    total_timesteps    | 1380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -0.134   |\n",
      "|    value_loss         | 9.9      |\n",
      "------------------------------------\n",
      "Num timesteps: 1381000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.41\n",
      "Num timesteps: 1382000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.48\n",
      "Num timesteps: 1383000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.64\n",
      "Num timesteps: 1384000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.14\n",
      "Num timesteps: 1385000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.66\n",
      "Num timesteps: 1386000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.37\n",
      "Num timesteps: 1387000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.20\n",
      "Num timesteps: 1388000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.99\n",
      "Num timesteps: 1389000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.21\n",
      "Num timesteps: 1390000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1173     |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 1184     |\n",
      "|    total_timesteps    | 1390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | -2.48    |\n",
      "|    value_loss         | 11.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1391000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.17\n",
      "Num timesteps: 1392000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.41\n",
      "Num timesteps: 1393000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.04\n",
      "Num timesteps: 1394000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 136.81\n",
      "Num timesteps: 1395000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 136.19\n",
      "Num timesteps: 1396000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.38\n",
      "Num timesteps: 1397000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.19\n",
      "Num timesteps: 1398000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.21\n",
      "Num timesteps: 1399000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.73\n",
      "Num timesteps: 1400000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 135.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 349      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1175     |\n",
      "|    iterations         | 14000    |\n",
      "|    time_elapsed       | 1191     |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | 3.52     |\n",
      "|    value_loss         | 60.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 1401000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.18\n",
      "Num timesteps: 1402000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 137.01\n",
      "Num timesteps: 1403000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.71\n",
      "Num timesteps: 1404000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.69\n",
      "Num timesteps: 1405000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.96\n",
      "Num timesteps: 1406000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.25\n",
      "Num timesteps: 1407000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.97\n",
      "Num timesteps: 1408000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.91\n",
      "Num timesteps: 1409000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.27\n",
      "Num timesteps: 1410000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1177     |\n",
      "|    iterations         | 14100    |\n",
      "|    time_elapsed       | 1197     |\n",
      "|    total_timesteps    | 1410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.613   |\n",
      "|    explained_variance | 0.508    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | 9.78     |\n",
      "|    value_loss         | 725      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1411000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.13\n",
      "Num timesteps: 1412000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.79\n",
      "Num timesteps: 1413000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.43\n",
      "Num timesteps: 1414000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.15\n",
      "Num timesteps: 1415000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.05\n",
      "Num timesteps: 1416000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.67\n",
      "Num timesteps: 1417000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.30\n",
      "Num timesteps: 1418000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.25\n",
      "Num timesteps: 1419000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.93\n",
      "Num timesteps: 1420000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 324      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1178     |\n",
      "|    iterations         | 14200    |\n",
      "|    time_elapsed       | 1204     |\n",
      "|    total_timesteps    | 1420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.792   |\n",
      "|    explained_variance | 0.494    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | 4.08     |\n",
      "|    value_loss         | 321      |\n",
      "------------------------------------\n",
      "Num timesteps: 1421000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.23\n",
      "Num timesteps: 1422000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.64\n",
      "Num timesteps: 1423000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.22\n",
      "Num timesteps: 1424000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.43\n",
      "Num timesteps: 1425000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.84\n",
      "Num timesteps: 1426000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.99\n",
      "Num timesteps: 1427000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.76\n",
      "Num timesteps: 1428000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.61\n",
      "Num timesteps: 1429000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.98\n",
      "Num timesteps: 1430000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 92.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1181     |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 1210     |\n",
      "|    total_timesteps    | 1430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.968   |\n",
      "|    explained_variance | 0.899    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 0.654    |\n",
      "|    value_loss         | 15.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 1431000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.94\n",
      "Num timesteps: 1432000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.72\n",
      "Num timesteps: 1433000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.29\n",
      "Num timesteps: 1434000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.75\n",
      "Num timesteps: 1435000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.99\n",
      "Num timesteps: 1436000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.34\n",
      "Num timesteps: 1437000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.31\n",
      "Num timesteps: 1438000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.45\n",
      "Num timesteps: 1439000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.56\n",
      "Num timesteps: 1440000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 304      |\n",
      "|    ep_rew_mean        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 1217     |\n",
      "|    total_timesteps    | 1440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.927   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | -0.389   |\n",
      "|    value_loss         | 2.76     |\n",
      "------------------------------------\n",
      "Num timesteps: 1441000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.75\n",
      "Num timesteps: 1442000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.61\n",
      "Num timesteps: 1443000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.36\n",
      "Num timesteps: 1444000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.65\n",
      "Num timesteps: 1445000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.01\n",
      "Num timesteps: 1446000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.57\n",
      "Num timesteps: 1447000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.61\n",
      "Num timesteps: 1448000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.89\n",
      "Num timesteps: 1449000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.03\n",
      "Num timesteps: 1450000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 106      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 1225     |\n",
      "|    total_timesteps    | 1450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.645   |\n",
      "|    explained_variance | 0.704    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | 6.78     |\n",
      "|    value_loss         | 399      |\n",
      "------------------------------------\n",
      "Num timesteps: 1451000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.58\n",
      "Num timesteps: 1452000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.53\n",
      "Num timesteps: 1453000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.45\n",
      "Num timesteps: 1454000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.62\n",
      "Num timesteps: 1455000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.92\n",
      "Num timesteps: 1456000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.58\n",
      "Num timesteps: 1457000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.01\n",
      "Num timesteps: 1458000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.01\n",
      "Num timesteps: 1459000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.50\n",
      "Num timesteps: 1460000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 14600    |\n",
      "|    time_elapsed       | 1233     |\n",
      "|    total_timesteps    | 1460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.698   |\n",
      "|    explained_variance | -3.88    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 25.4     |\n",
      "|    value_loss         | 1.28e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1461000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.30\n",
      "Num timesteps: 1462000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.28\n",
      "Num timesteps: 1463000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.09\n",
      "Num timesteps: 1464000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.49\n",
      "Num timesteps: 1465000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.30\n",
      "Num timesteps: 1466000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.73\n",
      "Num timesteps: 1467000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.95\n",
      "Num timesteps: 1468000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.81\n",
      "Num timesteps: 1469000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1470000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 355      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 1241     |\n",
      "|    total_timesteps    | 1470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.826   |\n",
      "|    explained_variance | 0.217    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | -18.3    |\n",
      "|    value_loss         | 5.28e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1471000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.76\n",
      "Num timesteps: 1472000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.82\n",
      "Num timesteps: 1473000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.21\n",
      "Num timesteps: 1474000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.47\n",
      "Num timesteps: 1475000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.76\n",
      "Num timesteps: 1476000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.41\n",
      "Num timesteps: 1477000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.11\n",
      "Num timesteps: 1478000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.21\n",
      "Num timesteps: 1479000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.45\n",
      "Num timesteps: 1480000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 368      |\n",
      "|    ep_rew_mean        | 88.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 14800    |\n",
      "|    time_elapsed       | 1249     |\n",
      "|    total_timesteps    | 1480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.962   |\n",
      "|    explained_variance | -4.15    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 51.1     |\n",
      "|    value_loss         | 3.61e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1481000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.87\n",
      "Num timesteps: 1482000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.37\n",
      "Num timesteps: 1483000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.91\n",
      "Num timesteps: 1484000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.89\n",
      "Num timesteps: 1485000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.77\n",
      "Num timesteps: 1486000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.76\n",
      "Num timesteps: 1487000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.13\n",
      "Num timesteps: 1488000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.96\n",
      "Num timesteps: 1489000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 79.28\n",
      "Num timesteps: 1490000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 82.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 1257     |\n",
      "|    total_timesteps    | 1490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.915   |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | -2.5     |\n",
      "|    value_loss         | 19.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1491000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.04\n",
      "Num timesteps: 1492000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.18\n",
      "Num timesteps: 1493000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.97\n",
      "Num timesteps: 1494000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.11\n",
      "Num timesteps: 1495000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.46\n",
      "Num timesteps: 1496000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 83.88\n",
      "Num timesteps: 1497000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.36\n",
      "Num timesteps: 1498000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.44\n",
      "Num timesteps: 1499000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.17\n",
      "Num timesteps: 1500000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 88.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 15000    |\n",
      "|    time_elapsed       | 1265     |\n",
      "|    total_timesteps    | 1500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.513   |\n",
      "|    explained_variance | -10      |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 22.8     |\n",
      "|    value_loss         | 1.04e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1501000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.59\n",
      "Num timesteps: 1502000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.08\n",
      "Num timesteps: 1503000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.24\n",
      "Num timesteps: 1504000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.99\n",
      "Num timesteps: 1505000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.37\n",
      "Num timesteps: 1506000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.70\n",
      "Num timesteps: 1507000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.08\n",
      "Num timesteps: 1508000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.19\n",
      "Num timesteps: 1509000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.16\n",
      "Num timesteps: 1510000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 372      |\n",
      "|    ep_rew_mean        | 93.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 15100    |\n",
      "|    time_elapsed       | 1274     |\n",
      "|    total_timesteps    | 1510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | -3.26    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15099    |\n",
      "|    policy_loss        | -5.34    |\n",
      "|    value_loss         | 378      |\n",
      "------------------------------------\n",
      "Num timesteps: 1511000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.80\n",
      "Num timesteps: 1512000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.62\n",
      "Num timesteps: 1513000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.79\n",
      "Num timesteps: 1514000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.43\n",
      "Num timesteps: 1515000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.66\n",
      "Num timesteps: 1516000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.25\n",
      "Num timesteps: 1517000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.63\n",
      "Num timesteps: 1518000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.93\n",
      "Num timesteps: 1519000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.88\n",
      "Num timesteps: 1520000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 1282     |\n",
      "|    total_timesteps    | 1520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.602    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | 3.49     |\n",
      "|    value_loss         | 21.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 1521000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1522000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.89\n",
      "Num timesteps: 1523000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.35\n",
      "Num timesteps: 1524000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.90\n",
      "Num timesteps: 1525000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.51\n",
      "Num timesteps: 1526000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.33\n",
      "Num timesteps: 1527000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.13\n",
      "Num timesteps: 1528000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.74\n",
      "Num timesteps: 1529000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.73\n",
      "Num timesteps: 1530000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 1292     |\n",
      "|    total_timesteps    | 1530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | -1.43    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | 56.9     |\n",
      "|    value_loss         | 3.82e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1531000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.55\n",
      "Num timesteps: 1532000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.46\n",
      "Num timesteps: 1533000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.53\n",
      "Num timesteps: 1534000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.83\n",
      "Num timesteps: 1535000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.44\n",
      "Num timesteps: 1536000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.70\n",
      "Num timesteps: 1537000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.34\n",
      "Num timesteps: 1538000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.78\n",
      "Num timesteps: 1539000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.63\n",
      "Num timesteps: 1540000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 422      |\n",
      "|    ep_rew_mean        | 98.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 15400    |\n",
      "|    time_elapsed       | 1302     |\n",
      "|    total_timesteps    | 1540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | -0.29    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | 1.55     |\n",
      "|    value_loss         | 16.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 1541000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.05\n",
      "Num timesteps: 1542000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.66\n",
      "Num timesteps: 1543000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.13\n",
      "Num timesteps: 1544000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.20\n",
      "Num timesteps: 1545000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.38\n",
      "Num timesteps: 1546000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.45\n",
      "Num timesteps: 1547000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.29\n",
      "Num timesteps: 1548000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.18\n",
      "Num timesteps: 1549000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.12\n",
      "Num timesteps: 1550000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 437      |\n",
      "|    ep_rew_mean        | 90.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 1311     |\n",
      "|    total_timesteps    | 1550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.448   |\n",
      "|    explained_variance | 0.349    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.651   |\n",
      "|    value_loss         | 434      |\n",
      "------------------------------------\n",
      "Num timesteps: 1551000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.80\n",
      "Num timesteps: 1552000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.47\n",
      "Num timesteps: 1553000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 91.32\n",
      "Num timesteps: 1554000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.51\n",
      "Num timesteps: 1555000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.03\n",
      "Num timesteps: 1556000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.70\n",
      "Num timesteps: 1557000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.09\n",
      "Num timesteps: 1558000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.19\n",
      "Num timesteps: 1559000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.18\n",
      "Num timesteps: 1560000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 452      |\n",
      "|    ep_rew_mean        | 84.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 15600    |\n",
      "|    time_elapsed       | 1321     |\n",
      "|    total_timesteps    | 1560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.84    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15599    |\n",
      "|    policy_loss        | 0.971    |\n",
      "|    value_loss         | 3.96     |\n",
      "------------------------------------\n",
      "Num timesteps: 1561000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 81.99\n",
      "Num timesteps: 1562000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 82.37\n",
      "Num timesteps: 1563000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.50\n",
      "Num timesteps: 1564000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.68\n",
      "Num timesteps: 1565000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.60\n",
      "Num timesteps: 1566000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.17\n",
      "Num timesteps: 1567000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 79.46\n",
      "Num timesteps: 1568000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 74.84\n",
      "Num timesteps: 1569000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 72.25\n",
      "Num timesteps: 1570000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 75.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 459      |\n",
      "|    ep_rew_mean        | 75.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 15700    |\n",
      "|    time_elapsed       | 1331     |\n",
      "|    total_timesteps    | 1570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.928   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | -0.236   |\n",
      "|    value_loss         | 8.94     |\n",
      "------------------------------------\n",
      "Num timesteps: 1571000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.84\n",
      "Num timesteps: 1572000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 77.05\n",
      "Num timesteps: 1573000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 78.07\n",
      "Num timesteps: 1574000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 76.55\n",
      "Num timesteps: 1575000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.75\n",
      "Num timesteps: 1576000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 69.87\n",
      "Num timesteps: 1577000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.07\n",
      "Num timesteps: 1578000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 80.40\n",
      "Num timesteps: 1579000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 84.74\n",
      "Num timesteps: 1580000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 85.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 85.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 15800    |\n",
      "|    time_elapsed       | 1338     |\n",
      "|    total_timesteps    | 1580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.743   |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15799    |\n",
      "|    policy_loss        | 5.26     |\n",
      "|    value_loss         | 68       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1581000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.16\n",
      "Num timesteps: 1582000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.00\n",
      "Num timesteps: 1583000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.88\n",
      "Num timesteps: 1584000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.58\n",
      "Num timesteps: 1585000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 86.82\n",
      "Num timesteps: 1586000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.62\n",
      "Num timesteps: 1587000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.25\n",
      "Num timesteps: 1588000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.38\n",
      "Num timesteps: 1589000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 89.49\n",
      "Num timesteps: 1590000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 87.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 396      |\n",
      "|    ep_rew_mean        | 87       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 15900    |\n",
      "|    time_elapsed       | 1347     |\n",
      "|    total_timesteps    | 1590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | 4.18     |\n",
      "|    value_loss         | 58       |\n",
      "------------------------------------\n",
      "Num timesteps: 1591000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 88.23\n",
      "Num timesteps: 1592000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 92.49\n",
      "Num timesteps: 1593000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.38\n",
      "Num timesteps: 1594000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.77\n",
      "Num timesteps: 1595000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.00\n",
      "Num timesteps: 1596000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.72\n",
      "Num timesteps: 1597000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.93\n",
      "Num timesteps: 1598000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.24\n",
      "Num timesteps: 1599000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.55\n",
      "Num timesteps: 1600000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.70\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 1355     |\n",
      "|    total_timesteps    | 1600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.776   |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -1.38    |\n",
      "|    value_loss         | 51.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1601000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.00\n",
      "Num timesteps: 1602000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.01\n",
      "Num timesteps: 1603000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.48\n",
      "Num timesteps: 1604000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.08\n",
      "Num timesteps: 1605000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.42\n",
      "Num timesteps: 1606000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.73\n",
      "Num timesteps: 1607000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.12\n",
      "Num timesteps: 1608000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.71\n",
      "Num timesteps: 1609000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.44\n",
      "Num timesteps: 1610000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 378      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1179     |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 1365     |\n",
      "|    total_timesteps    | 1610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | -0.714   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | -1.39    |\n",
      "|    value_loss         | 22.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1611000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.13\n",
      "Num timesteps: 1612000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.37\n",
      "Num timesteps: 1613000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.02\n",
      "Num timesteps: 1614000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.27\n",
      "Num timesteps: 1615000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.07\n",
      "Num timesteps: 1616000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.20\n",
      "Num timesteps: 1617000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.38\n",
      "Num timesteps: 1618000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.01\n",
      "Num timesteps: 1619000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.53\n",
      "Num timesteps: 1620000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 391      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1178     |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 1374     |\n",
      "|    total_timesteps    | 1620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.831   |\n",
      "|    explained_variance | 0.385    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | 6.81     |\n",
      "|    value_loss         | 302      |\n",
      "------------------------------------\n",
      "Num timesteps: 1621000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.21\n",
      "Num timesteps: 1622000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.64\n",
      "Num timesteps: 1623000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.04\n",
      "Num timesteps: 1624000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.25\n",
      "Num timesteps: 1625000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.06\n",
      "Num timesteps: 1626000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.41\n",
      "Num timesteps: 1627000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.43\n",
      "Num timesteps: 1628000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.95\n",
      "Num timesteps: 1629000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.66\n",
      "Num timesteps: 1630000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 367      |\n",
      "|    ep_rew_mean        | 94.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1178     |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 1383     |\n",
      "|    total_timesteps    | 1630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | -0.463   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -4.37    |\n",
      "|    value_loss         | 36.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 1631000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.27\n",
      "Num timesteps: 1632000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.96\n",
      "Num timesteps: 1633000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.16\n",
      "Num timesteps: 1634000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.78\n",
      "Num timesteps: 1635000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.04\n",
      "Num timesteps: 1636000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.11\n",
      "Num timesteps: 1637000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 90.80\n",
      "Num timesteps: 1638000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.07\n",
      "Num timesteps: 1639000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1640000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 97.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 1389     |\n",
      "|    total_timesteps    | 1640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.571   |\n",
      "|    value_loss         | 14.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 1641000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.08\n",
      "Num timesteps: 1642000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.44\n",
      "Num timesteps: 1643000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.03\n",
      "Num timesteps: 1644000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.07\n",
      "Num timesteps: 1645000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.30\n",
      "Num timesteps: 1646000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.17\n",
      "Num timesteps: 1647000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.57\n",
      "Num timesteps: 1648000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.32\n",
      "Num timesteps: 1649000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.95\n",
      "Num timesteps: 1650000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1180     |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 1397     |\n",
      "|    total_timesteps    | 1650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.922   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -0.732   |\n",
      "|    value_loss         | 2.27     |\n",
      "------------------------------------\n",
      "Num timesteps: 1651000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.82\n",
      "Num timesteps: 1652000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.16\n",
      "Num timesteps: 1653000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.86\n",
      "Num timesteps: 1654000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.81\n",
      "Num timesteps: 1655000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.69\n",
      "Num timesteps: 1656000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.00\n",
      "Num timesteps: 1657000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.93\n",
      "Num timesteps: 1658000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.11\n",
      "Num timesteps: 1659000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.08\n",
      "Num timesteps: 1660000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1181     |\n",
      "|    iterations         | 16600    |\n",
      "|    time_elapsed       | 1404     |\n",
      "|    total_timesteps    | 1660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.316    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | -2.54    |\n",
      "|    value_loss         | 10.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1661000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.78\n",
      "Num timesteps: 1662000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.07\n",
      "Num timesteps: 1663000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.67\n",
      "Num timesteps: 1664000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.50\n",
      "Num timesteps: 1665000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.80\n",
      "Num timesteps: 1666000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.56\n",
      "Num timesteps: 1667000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.04\n",
      "Num timesteps: 1668000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.25\n",
      "Num timesteps: 1669000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.01\n",
      "Num timesteps: 1670000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 318      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 1411     |\n",
      "|    total_timesteps    | 1670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.63    |\n",
      "|    explained_variance | 0.438    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -10.9    |\n",
      "|    value_loss         | 3.98e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1671000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.05\n",
      "Num timesteps: 1672000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.17\n",
      "Num timesteps: 1673000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.08\n",
      "Num timesteps: 1674000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.04\n",
      "Num timesteps: 1675000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.35\n",
      "Num timesteps: 1676000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.18\n",
      "Num timesteps: 1677000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.08\n",
      "Num timesteps: 1678000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.78\n",
      "Num timesteps: 1679000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.66\n",
      "Num timesteps: 1680000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 347      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 16800    |\n",
      "|    time_elapsed       | 1420     |\n",
      "|    total_timesteps    | 1680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.224   |\n",
      "|    explained_variance | -0.33    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16799    |\n",
      "|    policy_loss        | -0.626   |\n",
      "|    value_loss         | 41.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 1681000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.53\n",
      "Num timesteps: 1682000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.86\n",
      "Num timesteps: 1683000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.01\n",
      "Num timesteps: 1684000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.11\n",
      "Num timesteps: 1685000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.18\n",
      "Num timesteps: 1686000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.78\n",
      "Num timesteps: 1687000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.78\n",
      "Num timesteps: 1688000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.58\n",
      "Num timesteps: 1689000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.12\n",
      "Num timesteps: 1690000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.13\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 1428     |\n",
      "|    total_timesteps    | 1690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | -3.17    |\n",
      "|    value_loss         | 23       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1691000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.69\n",
      "Num timesteps: 1692000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.86\n",
      "Num timesteps: 1693000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.02\n",
      "Num timesteps: 1694000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.90\n",
      "Num timesteps: 1695000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.12\n",
      "Num timesteps: 1696000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.66\n",
      "Num timesteps: 1697000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.22\n",
      "Num timesteps: 1698000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.88\n",
      "Num timesteps: 1699000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.42\n",
      "Num timesteps: 1700000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 355      |\n",
      "|    ep_rew_mean        | 103      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 17000    |\n",
      "|    time_elapsed       | 1436     |\n",
      "|    total_timesteps    | 1700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | 3.76     |\n",
      "|    value_loss         | 61.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 1701000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.67\n",
      "Num timesteps: 1702000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.54\n",
      "Num timesteps: 1703000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.73\n",
      "Num timesteps: 1704000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.04\n",
      "Num timesteps: 1705000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.65\n",
      "Num timesteps: 1706000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.31\n",
      "Num timesteps: 1707000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.61\n",
      "Num timesteps: 1708000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.04\n",
      "Num timesteps: 1709000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.51\n",
      "Num timesteps: 1710000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 380      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 1445     |\n",
      "|    total_timesteps    | 1710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.706   |\n",
      "|    explained_variance | 0.531    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -1.46    |\n",
      "|    value_loss         | 1.54e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1711000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.17\n",
      "Num timesteps: 1712000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.04\n",
      "Num timesteps: 1713000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.74\n",
      "Num timesteps: 1714000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.48\n",
      "Num timesteps: 1715000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.42\n",
      "Num timesteps: 1716000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.42\n",
      "Num timesteps: 1717000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.24\n",
      "Num timesteps: 1718000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.59\n",
      "Num timesteps: 1719000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.66\n",
      "Num timesteps: 1720000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 1453     |\n",
      "|    total_timesteps    | 1720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | -0.0364  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 2.1      |\n",
      "|    value_loss         | 550      |\n",
      "------------------------------------\n",
      "Num timesteps: 1721000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.38\n",
      "Num timesteps: 1722000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.44\n",
      "Num timesteps: 1723000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.79\n",
      "Num timesteps: 1724000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.06\n",
      "Num timesteps: 1725000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.22\n",
      "Num timesteps: 1726000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.92\n",
      "Num timesteps: 1727000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.40\n",
      "Num timesteps: 1728000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.20\n",
      "Num timesteps: 1729000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.22\n",
      "Num timesteps: 1730000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 1462     |\n",
      "|    total_timesteps    | 1730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.06    |\n",
      "|    explained_variance | -0.344   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -5.09    |\n",
      "|    value_loss         | 37.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1731000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.56\n",
      "Num timesteps: 1732000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.33\n",
      "Num timesteps: 1733000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.22\n",
      "Num timesteps: 1734000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.07\n",
      "Num timesteps: 1735000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.08\n",
      "Num timesteps: 1736000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.97\n",
      "Num timesteps: 1737000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.11\n",
      "Num timesteps: 1738000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.26\n",
      "Num timesteps: 1739000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.50\n",
      "Num timesteps: 1740000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 104.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 401      |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1181     |\n",
      "|    iterations         | 17400    |\n",
      "|    time_elapsed       | 1472     |\n",
      "|    total_timesteps    | 1740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.785   |\n",
      "|    explained_variance | 0.78     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | 3.35     |\n",
      "|    value_loss         | 68.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 1741000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.47\n",
      "Num timesteps: 1742000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.76\n",
      "Num timesteps: 1743000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.33\n",
      "Num timesteps: 1744000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.83\n",
      "Num timesteps: 1745000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.47\n",
      "Num timesteps: 1746000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.09\n",
      "Num timesteps: 1747000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.84\n",
      "Num timesteps: 1748000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.52\n",
      "Num timesteps: 1749000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1750000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 378      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1182     |\n",
      "|    iterations         | 17500    |\n",
      "|    time_elapsed       | 1479     |\n",
      "|    total_timesteps    | 1750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.676    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | -4.72    |\n",
      "|    value_loss         | 27.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 1751000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.80\n",
      "Num timesteps: 1752000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.85\n",
      "Num timesteps: 1753000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.92\n",
      "Num timesteps: 1754000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.03\n",
      "Num timesteps: 1755000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.17\n",
      "Num timesteps: 1756000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 106.98\n",
      "Num timesteps: 1757000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.02\n",
      "Num timesteps: 1758000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.09\n",
      "Num timesteps: 1759000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.37\n",
      "Num timesteps: 1760000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 369      |\n",
      "|    ep_rew_mean        | 110      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 17600    |\n",
      "|    time_elapsed       | 1486     |\n",
      "|    total_timesteps    | 1760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.49     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -47.5    |\n",
      "|    value_loss         | 5.18e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1761000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.35\n",
      "Num timesteps: 1762000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.89\n",
      "Num timesteps: 1763000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.18\n",
      "Num timesteps: 1764000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.86\n",
      "Num timesteps: 1765000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.65\n",
      "Num timesteps: 1766000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.91\n",
      "Num timesteps: 1767000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.76\n",
      "Num timesteps: 1768000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.34\n",
      "Num timesteps: 1769000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.06\n",
      "Num timesteps: 1770000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.28\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 329      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 1493     |\n",
      "|    total_timesteps    | 1770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | -5.7     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | 13.2     |\n",
      "|    value_loss         | 491      |\n",
      "------------------------------------\n",
      "Num timesteps: 1771000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.71\n",
      "Num timesteps: 1772000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.11\n",
      "Num timesteps: 1773000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.91\n",
      "Num timesteps: 1774000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.71\n",
      "Num timesteps: 1775000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.08\n",
      "Num timesteps: 1776000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.91\n",
      "Num timesteps: 1777000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.08\n",
      "Num timesteps: 1778000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.40\n",
      "Num timesteps: 1779000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.55\n",
      "Num timesteps: 1780000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 1500     |\n",
      "|    total_timesteps    | 1780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.859   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -2.71    |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1781000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.16\n",
      "Num timesteps: 1782000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.30\n",
      "Num timesteps: 1783000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.49\n",
      "Num timesteps: 1784000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.82\n",
      "Num timesteps: 1785000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.12\n",
      "Num timesteps: 1786000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.79\n",
      "Num timesteps: 1787000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.28\n",
      "Num timesteps: 1788000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.48\n",
      "Num timesteps: 1789000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.13\n",
      "Num timesteps: 1790000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 1506     |\n",
      "|    total_timesteps    | 1790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.494   |\n",
      "|    explained_variance | -4.94    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | 20.7     |\n",
      "|    value_loss         | 2.93e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1791000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.69\n",
      "Num timesteps: 1792000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.52\n",
      "Num timesteps: 1793000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.14\n",
      "Num timesteps: 1794000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.25\n",
      "Num timesteps: 1795000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.09\n",
      "Num timesteps: 1796000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.92\n",
      "Num timesteps: 1797000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.34\n",
      "Num timesteps: 1798000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.55\n",
      "Num timesteps: 1799000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.25\n",
      "Num timesteps: 1800000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.81\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 18000    |\n",
      "|    time_elapsed       | 1515     |\n",
      "|    total_timesteps    | 1800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.997   |\n",
      "|    explained_variance | 0.0566   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 17999    |\n",
      "|    policy_loss        | -8.17    |\n",
      "|    value_loss         | 82       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1801000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.87\n",
      "Num timesteps: 1802000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.88\n",
      "Num timesteps: 1803000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.91\n",
      "Num timesteps: 1804000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.11\n",
      "Num timesteps: 1805000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.27\n",
      "Num timesteps: 1806000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.49\n",
      "Num timesteps: 1807000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.54\n",
      "Num timesteps: 1808000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.82\n",
      "Num timesteps: 1809000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.36\n",
      "Num timesteps: 1810000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.08\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 350      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 18100    |\n",
      "|    time_elapsed       | 1523     |\n",
      "|    total_timesteps    | 1810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18099    |\n",
      "|    policy_loss        | 1.58     |\n",
      "|    value_loss         | 54.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 1811000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.93\n",
      "Num timesteps: 1812000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.59\n",
      "Num timesteps: 1813000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.94\n",
      "Num timesteps: 1814000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.30\n",
      "Num timesteps: 1815000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.81\n",
      "Num timesteps: 1816000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.56\n",
      "Num timesteps: 1817000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.30\n",
      "Num timesteps: 1818000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.77\n",
      "Num timesteps: 1819000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.04\n",
      "Num timesteps: 1820000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 1533     |\n",
      "|    total_timesteps    | 1820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.507    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | -2.89    |\n",
      "|    value_loss         | 29.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1821000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.21\n",
      "Num timesteps: 1822000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.92\n",
      "Num timesteps: 1823000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.70\n",
      "Num timesteps: 1824000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.43\n",
      "Num timesteps: 1825000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.28\n",
      "Num timesteps: 1826000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.87\n",
      "Num timesteps: 1827000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.62\n",
      "Num timesteps: 1828000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.90\n",
      "Num timesteps: 1829000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.42\n",
      "Num timesteps: 1830000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 408      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 18300    |\n",
      "|    time_elapsed       | 1542     |\n",
      "|    total_timesteps    | 1830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.873    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | -9.03    |\n",
      "|    value_loss         | 120      |\n",
      "------------------------------------\n",
      "Num timesteps: 1831000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.70\n",
      "Num timesteps: 1832000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 113.21\n",
      "Num timesteps: 1833000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.59\n",
      "Num timesteps: 1834000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.40\n",
      "Num timesteps: 1835000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.44\n",
      "Num timesteps: 1836000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.86\n",
      "Num timesteps: 1837000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.70\n",
      "Num timesteps: 1838000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.74\n",
      "Num timesteps: 1839000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.85\n",
      "Num timesteps: 1840000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 409      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 1551     |\n",
      "|    total_timesteps    | 1840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.748   |\n",
      "|    explained_variance | 0.05     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | -0.611   |\n",
      "|    value_loss         | 426      |\n",
      "------------------------------------\n",
      "Num timesteps: 1841000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.71\n",
      "Num timesteps: 1842000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.21\n",
      "Num timesteps: 1843000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.01\n",
      "Num timesteps: 1844000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.77\n",
      "Num timesteps: 1845000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.01\n",
      "Num timesteps: 1846000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.81\n",
      "Num timesteps: 1847000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.96\n",
      "Num timesteps: 1848000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.91\n",
      "Num timesteps: 1849000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.37\n",
      "Num timesteps: 1850000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 379      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 18500    |\n",
      "|    time_elapsed       | 1558     |\n",
      "|    total_timesteps    | 1850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.999   |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | -5.96    |\n",
      "|    value_loss         | 47.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 1851000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.12\n",
      "Num timesteps: 1852000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.97\n",
      "Num timesteps: 1853000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.60\n",
      "Num timesteps: 1854000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 133.09\n",
      "Num timesteps: 1855000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.69\n",
      "Num timesteps: 1856000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.61\n",
      "Num timesteps: 1857000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.61\n",
      "Num timesteps: 1858000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.15\n",
      "Num timesteps: 1859000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1860000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 381      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 1566     |\n",
      "|    total_timesteps    | 1860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | -0.094   |\n",
      "|    value_loss         | 1.39     |\n",
      "------------------------------------\n",
      "Num timesteps: 1861000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.59\n",
      "Num timesteps: 1862000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 128.39\n",
      "Num timesteps: 1863000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.50\n",
      "Num timesteps: 1864000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.56\n",
      "Num timesteps: 1865000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.12\n",
      "Num timesteps: 1866000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.31\n",
      "Num timesteps: 1867000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.44\n",
      "Num timesteps: 1868000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 129.69\n",
      "Num timesteps: 1869000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.63\n",
      "Num timesteps: 1870000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 134.91\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 381      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 1575     |\n",
      "|    total_timesteps    | 1870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.858   |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 2.93     |\n",
      "|    value_loss         | 51.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 1871000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 135.85\n",
      "Num timesteps: 1872000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 130.10\n",
      "Num timesteps: 1873000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.16\n",
      "Num timesteps: 1874000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 126.88\n",
      "Num timesteps: 1875000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.78\n",
      "Num timesteps: 1876000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.43\n",
      "Num timesteps: 1877000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.02\n",
      "Num timesteps: 1878000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.99\n",
      "Num timesteps: 1879000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.21\n",
      "Num timesteps: 1880000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 119.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 383      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 1585     |\n",
      "|    total_timesteps    | 1880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.907   |\n",
      "|    explained_variance | 0.592    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 8.93     |\n",
      "------------------------------------\n",
      "Num timesteps: 1881000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.13\n",
      "Num timesteps: 1882000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.17\n",
      "Num timesteps: 1883000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.29\n",
      "Num timesteps: 1884000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 116.76\n",
      "Num timesteps: 1885000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 114.41\n",
      "Num timesteps: 1886000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.94\n",
      "Num timesteps: 1887000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 109.45\n",
      "Num timesteps: 1888000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 112.79\n",
      "Num timesteps: 1889000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.50\n",
      "Num timesteps: 1890000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 371      |\n",
      "|    ep_rew_mean        | 111      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 18900    |\n",
      "|    time_elapsed       | 1592     |\n",
      "|    total_timesteps    | 1890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.984   |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | 3.73     |\n",
      "|    value_loss         | 21.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 1891000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.82\n",
      "Num timesteps: 1892000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 111.04\n",
      "Num timesteps: 1893000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.01\n",
      "Num timesteps: 1894000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.18\n",
      "Num timesteps: 1895000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.36\n",
      "Num timesteps: 1896000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 105.50\n",
      "Num timesteps: 1897000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.30\n",
      "Num timesteps: 1898000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.15\n",
      "Num timesteps: 1899000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.94\n",
      "Num timesteps: 1900000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 94.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 374      |\n",
      "|    ep_rew_mean        | 94.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 19000    |\n",
      "|    time_elapsed       | 1600     |\n",
      "|    total_timesteps    | 1900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.996   |\n",
      "|    explained_variance | -0.114   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | 8.54     |\n",
      "|    value_loss         | 114      |\n",
      "------------------------------------\n",
      "Num timesteps: 1901000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 97.32\n",
      "Num timesteps: 1902000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.32\n",
      "Num timesteps: 1903000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.13\n",
      "Num timesteps: 1904000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.09\n",
      "Num timesteps: 1905000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.94\n",
      "Num timesteps: 1906000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.43\n",
      "Num timesteps: 1907000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.94\n",
      "Num timesteps: 1908000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 93.87\n",
      "Num timesteps: 1909000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.32\n",
      "Num timesteps: 1910000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 363      |\n",
      "|    ep_rew_mean        | 101      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 19100    |\n",
      "|    time_elapsed       | 1607     |\n",
      "|    total_timesteps    | 1910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | -0.473   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19099    |\n",
      "|    policy_loss        | 3.35     |\n",
      "|    value_loss         | 676      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1911000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.04\n",
      "Num timesteps: 1912000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.46\n",
      "Num timesteps: 1913000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.95\n",
      "Num timesteps: 1914000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 103.86\n",
      "Num timesteps: 1915000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 101.72\n",
      "Num timesteps: 1916000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.02\n",
      "Num timesteps: 1917000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.33\n",
      "Num timesteps: 1918000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 95.38\n",
      "Num timesteps: 1919000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 98.05\n",
      "Num timesteps: 1920000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 96       |\n",
      "| time/                 |          |\n",
      "|    fps                | 1189     |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 1614     |\n",
      "|    total_timesteps    | 1920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 0.811    |\n",
      "|    value_loss         | 9.27     |\n",
      "------------------------------------\n",
      "Num timesteps: 1921000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.61\n",
      "Num timesteps: 1922000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.31\n",
      "Num timesteps: 1923000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 96.41\n",
      "Num timesteps: 1924000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 102.35\n",
      "Num timesteps: 1925000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 100.02\n",
      "Num timesteps: 1926000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.92\n",
      "Num timesteps: 1927000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.99\n",
      "Num timesteps: 1928000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.61\n",
      "Num timesteps: 1929000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 99.67\n",
      "Num timesteps: 1930000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 108.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 1624     |\n",
      "|    total_timesteps    | 1930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.548   |\n",
      "|    explained_variance | -7.61    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | 21.5     |\n",
      "|    value_loss         | 2.43e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1931000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 107.60\n",
      "Num timesteps: 1932000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 110.02\n",
      "Num timesteps: 1933000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.86\n",
      "Num timesteps: 1934000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.41\n",
      "Num timesteps: 1935000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.37\n",
      "Num timesteps: 1936000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 124.14\n",
      "Num timesteps: 1937000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.78\n",
      "Num timesteps: 1938000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.87\n",
      "Num timesteps: 1939000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.02\n",
      "Num timesteps: 1940000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 1633     |\n",
      "|    total_timesteps    | 1940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.79    |\n",
      "|    value_loss         | 9.33     |\n",
      "------------------------------------\n",
      "Num timesteps: 1941000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.78\n",
      "Num timesteps: 1942000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.54\n",
      "Num timesteps: 1943000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.09\n",
      "Num timesteps: 1944000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 125.92\n",
      "Num timesteps: 1945000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 122.47\n",
      "Num timesteps: 1946000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.54\n",
      "Num timesteps: 1947000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 115.82\n",
      "Num timesteps: 1948000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 118.93\n",
      "Num timesteps: 1949000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.33\n",
      "Num timesteps: 1950000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 120.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 19500    |\n",
      "|    time_elapsed       | 1642     |\n",
      "|    total_timesteps    | 1950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.31    |\n",
      "|    explained_variance | -10.3    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | 12.1     |\n",
      "|    value_loss         | 1.15e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 1951000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 117.99\n",
      "Num timesteps: 1952000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 121.37\n",
      "Num timesteps: 1953000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.96\n",
      "Num timesteps: 1954000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 123.63\n",
      "Num timesteps: 1955000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 127.19\n",
      "Num timesteps: 1956000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 131.45\n",
      "Num timesteps: 1957000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 132.10\n",
      "Num timesteps: 1958000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 137.67\n",
      "Num timesteps: 1959000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 142.25\n",
      "Num timesteps: 1960000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 144.19\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 383      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 1650     |\n",
      "|    total_timesteps    | 1960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | -2.73    |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 1961000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 144.05\n",
      "Num timesteps: 1962000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 139.77\n",
      "Num timesteps: 1963000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 138.99\n",
      "Num timesteps: 1964000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 144.69\n",
      "Num timesteps: 1965000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 145.28\n",
      "Num timesteps: 1966000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 150.55\n",
      "Num timesteps: 1967000\n",
      "Best mean reward: 151.91 - Last mean reward per episode: 152.75\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 1968000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 151.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1969000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 149.92\n",
      "Num timesteps: 1970000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 149.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 366      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 1657     |\n",
      "|    total_timesteps    | 1970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.487   |\n",
      "|    explained_variance | -5.01    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | 13.8     |\n",
      "|    value_loss         | 562      |\n",
      "------------------------------------\n",
      "Num timesteps: 1971000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 149.00\n",
      "Num timesteps: 1972000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 148.47\n",
      "Num timesteps: 1973000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 143.22\n",
      "Num timesteps: 1974000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 141.21\n",
      "Num timesteps: 1975000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 139.63\n",
      "Num timesteps: 1976000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 139.50\n",
      "Num timesteps: 1977000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 143.34\n",
      "Num timesteps: 1978000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 148.33\n",
      "Num timesteps: 1979000\n",
      "Best mean reward: 152.75 - Last mean reward per episode: 152.89\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 1980000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 152.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1190     |\n",
      "|    iterations         | 19800    |\n",
      "|    time_elapsed       | 1663     |\n",
      "|    total_timesteps    | 1980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.854   |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | 4.89     |\n",
      "|    value_loss         | 46.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 1981000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 149.89\n",
      "Num timesteps: 1982000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 151.47\n",
      "Num timesteps: 1983000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 148.05\n",
      "Num timesteps: 1984000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 147.24\n",
      "Num timesteps: 1985000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 145.62\n",
      "Num timesteps: 1986000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 138.48\n",
      "Num timesteps: 1987000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 140.75\n",
      "Num timesteps: 1988000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 143.12\n",
      "Num timesteps: 1989000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 145.75\n",
      "Num timesteps: 1990000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 142.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 314      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 1670     |\n",
      "|    total_timesteps    | 1990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | -1.26    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 6.4      |\n",
      "|    value_loss         | 61.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 1991000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.66\n",
      "Num timesteps: 1992000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 137.38\n",
      "Num timesteps: 1993000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 137.63\n",
      "Num timesteps: 1994000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.26\n",
      "Num timesteps: 1995000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.32\n",
      "Num timesteps: 1996000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 137.27\n",
      "Num timesteps: 1997000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.84\n",
      "Num timesteps: 1998000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.65\n",
      "Num timesteps: 1999000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.47\n",
      "Num timesteps: 2000000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20000    |\n",
      "|    time_elapsed       | 1677     |\n",
      "|    total_timesteps    | 2000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 19999    |\n",
      "|    policy_loss        | -0.469   |\n",
      "|    value_loss         | 3.33     |\n",
      "------------------------------------\n",
      "Num timesteps: 2001000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 122.92\n",
      "Num timesteps: 2002000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 119.13\n",
      "Num timesteps: 2003000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 120.80\n",
      "Num timesteps: 2004000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 123.12\n",
      "Num timesteps: 2005000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.71\n",
      "Num timesteps: 2006000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.00\n",
      "Num timesteps: 2007000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.58\n",
      "Num timesteps: 2008000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 121.74\n",
      "Num timesteps: 2009000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 121.76\n",
      "Num timesteps: 2010000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20100    |\n",
      "|    time_elapsed       | 1685     |\n",
      "|    total_timesteps    | 2010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.782   |\n",
      "|    explained_variance | 0.417    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20099    |\n",
      "|    policy_loss        | -30.4    |\n",
      "|    value_loss         | 5.18e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2011000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.35\n",
      "Num timesteps: 2012000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.18\n",
      "Num timesteps: 2013000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.69\n",
      "Num timesteps: 2014000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.32\n",
      "Num timesteps: 2015000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.96\n",
      "Num timesteps: 2016000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.34\n",
      "Num timesteps: 2017000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.22\n",
      "Num timesteps: 2018000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.27\n",
      "Num timesteps: 2019000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.57\n",
      "Num timesteps: 2020000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 114.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 350      |\n",
      "|    ep_rew_mean        | 115      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20200    |\n",
      "|    time_elapsed       | 1694     |\n",
      "|    total_timesteps    | 2020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.169    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20199    |\n",
      "|    policy_loss        | -7.04    |\n",
      "|    value_loss         | 72       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2021000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.70\n",
      "Num timesteps: 2022000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 114.32\n",
      "Num timesteps: 2023000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.64\n",
      "Num timesteps: 2024000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.11\n",
      "Num timesteps: 2025000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.17\n",
      "Num timesteps: 2026000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.67\n",
      "Num timesteps: 2027000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.68\n",
      "Num timesteps: 2028000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 110.15\n",
      "Num timesteps: 2029000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.43\n",
      "Num timesteps: 2030000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 379      |\n",
      "|    ep_rew_mean        | 114      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 20300    |\n",
      "|    time_elapsed       | 1703     |\n",
      "|    total_timesteps    | 2030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.861   |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20299    |\n",
      "|    policy_loss        | -0.00254 |\n",
      "|    value_loss         | 3.85     |\n",
      "------------------------------------\n",
      "Num timesteps: 2031000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.89\n",
      "Num timesteps: 2032000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.92\n",
      "Num timesteps: 2033000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.38\n",
      "Num timesteps: 2034000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 119.46\n",
      "Num timesteps: 2035000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.63\n",
      "Num timesteps: 2036000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.11\n",
      "Num timesteps: 2037000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.52\n",
      "Num timesteps: 2038000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.79\n",
      "Num timesteps: 2039000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.54\n",
      "Num timesteps: 2040000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20400    |\n",
      "|    time_elapsed       | 1710     |\n",
      "|    total_timesteps    | 2040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20399    |\n",
      "|    policy_loss        | 3.29     |\n",
      "|    value_loss         | 129      |\n",
      "------------------------------------\n",
      "Num timesteps: 2041000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.91\n",
      "Num timesteps: 2042000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.62\n",
      "Num timesteps: 2043000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.68\n",
      "Num timesteps: 2044000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.91\n",
      "Num timesteps: 2045000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.81\n",
      "Num timesteps: 2046000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.36\n",
      "Num timesteps: 2047000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.05\n",
      "Num timesteps: 2048000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.96\n",
      "Num timesteps: 2049000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 120.80\n",
      "Num timesteps: 2050000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 120.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 372      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20500    |\n",
      "|    time_elapsed       | 1718     |\n",
      "|    total_timesteps    | 2050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | -5.65    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20499    |\n",
      "|    policy_loss        | 6.26     |\n",
      "|    value_loss         | 4.85e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2051000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 122.51\n",
      "Num timesteps: 2052000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 120.26\n",
      "Num timesteps: 2053000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.74\n",
      "Num timesteps: 2054000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.88\n",
      "Num timesteps: 2055000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.35\n",
      "Num timesteps: 2056000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.77\n",
      "Num timesteps: 2057000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.67\n",
      "Num timesteps: 2058000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.79\n",
      "Num timesteps: 2059000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.97\n",
      "Num timesteps: 2060000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1192     |\n",
      "|    iterations         | 20600    |\n",
      "|    time_elapsed       | 1727     |\n",
      "|    total_timesteps    | 2060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20599    |\n",
      "|    policy_loss        | -1.4     |\n",
      "|    value_loss         | 2.96     |\n",
      "------------------------------------\n",
      "Num timesteps: 2061000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.45\n",
      "Num timesteps: 2062000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.62\n",
      "Num timesteps: 2063000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.39\n",
      "Num timesteps: 2064000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.60\n",
      "Num timesteps: 2065000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.83\n",
      "Num timesteps: 2066000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.33\n",
      "Num timesteps: 2067000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.64\n",
      "Num timesteps: 2068000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.20\n",
      "Num timesteps: 2069000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.71\n",
      "Num timesteps: 2070000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 362      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 20700    |\n",
      "|    time_elapsed       | 1737     |\n",
      "|    total_timesteps    | 2070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.929   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20699    |\n",
      "|    policy_loss        | 2.22     |\n",
      "|    value_loss         | 13       |\n",
      "------------------------------------\n",
      "Num timesteps: 2071000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.51\n",
      "Num timesteps: 2072000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 123.63\n",
      "Num timesteps: 2073000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.11\n",
      "Num timesteps: 2074000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.48\n",
      "Num timesteps: 2075000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.98\n",
      "Num timesteps: 2076000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.57\n",
      "Num timesteps: 2077000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 122.90\n",
      "Num timesteps: 2078000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.08\n",
      "Num timesteps: 2079000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2080000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 382      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 20800    |\n",
      "|    time_elapsed       | 1745     |\n",
      "|    total_timesteps    | 2080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.04    |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20799    |\n",
      "|    policy_loss        | -2.42    |\n",
      "|    value_loss         | 7.78     |\n",
      "------------------------------------\n",
      "Num timesteps: 2081000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.89\n",
      "Num timesteps: 2082000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.11\n",
      "Num timesteps: 2083000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.43\n",
      "Num timesteps: 2084000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.23\n",
      "Num timesteps: 2085000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.22\n",
      "Num timesteps: 2086000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.58\n",
      "Num timesteps: 2087000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 136.50\n",
      "Num timesteps: 2088000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 136.64\n",
      "Num timesteps: 2089000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.61\n",
      "Num timesteps: 2090000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 406      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1189     |\n",
      "|    iterations         | 20900    |\n",
      "|    time_elapsed       | 1756     |\n",
      "|    total_timesteps    | 2090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.08    |\n",
      "|    explained_variance | -0.0452  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20899    |\n",
      "|    policy_loss        | 2.21     |\n",
      "|    value_loss         | 24.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 2091000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 138.22\n",
      "Num timesteps: 2092000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 137.63\n",
      "Num timesteps: 2093000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 141.38\n",
      "Num timesteps: 2094000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 141.80\n",
      "Num timesteps: 2095000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 143.71\n",
      "Num timesteps: 2096000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 146.68\n",
      "Num timesteps: 2097000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 145.36\n",
      "Num timesteps: 2098000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 144.56\n",
      "Num timesteps: 2099000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 144.97\n",
      "Num timesteps: 2100000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 147.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1189     |\n",
      "|    iterations         | 21000    |\n",
      "|    time_elapsed       | 1765     |\n",
      "|    total_timesteps    | 2100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 20999    |\n",
      "|    policy_loss        | -2.03    |\n",
      "|    value_loss         | 12.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 2101000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 147.31\n",
      "Num timesteps: 2102000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 147.37\n",
      "Num timesteps: 2103000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 147.74\n",
      "Num timesteps: 2104000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 143.02\n",
      "Num timesteps: 2105000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 145.91\n",
      "Num timesteps: 2106000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 145.77\n",
      "Num timesteps: 2107000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 144.19\n",
      "Num timesteps: 2108000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 146.11\n",
      "Num timesteps: 2109000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 141.59\n",
      "Num timesteps: 2110000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 141.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 429      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 21100    |\n",
      "|    time_elapsed       | 1775     |\n",
      "|    total_timesteps    | 2110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | -0.361   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21099    |\n",
      "|    policy_loss        | -2.11    |\n",
      "|    value_loss         | 14.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 2111000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 140.08\n",
      "Num timesteps: 2112000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 139.35\n",
      "Num timesteps: 2113000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.50\n",
      "Num timesteps: 2114000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.44\n",
      "Num timesteps: 2115000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.74\n",
      "Num timesteps: 2116000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.66\n",
      "Num timesteps: 2117000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.27\n",
      "Num timesteps: 2118000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 133.70\n",
      "Num timesteps: 2119000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.09\n",
      "Num timesteps: 2120000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 438      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 21200    |\n",
      "|    time_elapsed       | 1786     |\n",
      "|    total_timesteps    | 2120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | -9.65    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21199    |\n",
      "|    policy_loss        | -10.7    |\n",
      "|    value_loss         | 3.25e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2121000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.39\n",
      "Num timesteps: 2122000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.68\n",
      "Num timesteps: 2123000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.33\n",
      "Num timesteps: 2124000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.81\n",
      "Num timesteps: 2125000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.69\n",
      "Num timesteps: 2126000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.20\n",
      "Num timesteps: 2127000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.37\n",
      "Num timesteps: 2128000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.55\n",
      "Num timesteps: 2129000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.62\n",
      "Num timesteps: 2130000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.19\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 475      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1184     |\n",
      "|    iterations         | 21300    |\n",
      "|    time_elapsed       | 1798     |\n",
      "|    total_timesteps    | 2130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.988   |\n",
      "|    explained_variance | -0.388   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21299    |\n",
      "|    policy_loss        | 3.45     |\n",
      "|    value_loss         | 16.7     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2131000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.00\n",
      "Num timesteps: 2132000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.04\n",
      "Num timesteps: 2133000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.16\n",
      "Num timesteps: 2134000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.73\n",
      "Num timesteps: 2135000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.94\n",
      "Num timesteps: 2136000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 126.59\n",
      "Num timesteps: 2137000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 123.78\n",
      "Num timesteps: 2138000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 122.13\n",
      "Num timesteps: 2139000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 122.13\n",
      "Num timesteps: 2140000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 452      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 21400    |\n",
      "|    time_elapsed       | 1807     |\n",
      "|    total_timesteps    | 2140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21399    |\n",
      "|    policy_loss        | 5.78     |\n",
      "|    value_loss         | 54.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 2141000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.39\n",
      "Num timesteps: 2142000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.33\n",
      "Num timesteps: 2143000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.95\n",
      "Num timesteps: 2144000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.91\n",
      "Num timesteps: 2145000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.72\n",
      "Num timesteps: 2146000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 108.63\n",
      "Num timesteps: 2147000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 108.03\n",
      "Num timesteps: 2148000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 106.74\n",
      "Num timesteps: 2149000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 108.70\n",
      "Num timesteps: 2150000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 109.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 453      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 21500    |\n",
      "|    time_elapsed       | 1816     |\n",
      "|    total_timesteps    | 2150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21499    |\n",
      "|    policy_loss        | 0.382    |\n",
      "|    value_loss         | 3.57     |\n",
      "------------------------------------\n",
      "Num timesteps: 2151000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 107.77\n",
      "Num timesteps: 2152000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 107.46\n",
      "Num timesteps: 2153000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 109.30\n",
      "Num timesteps: 2154000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.30\n",
      "Num timesteps: 2155000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.34\n",
      "Num timesteps: 2156000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.06\n",
      "Num timesteps: 2157000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 119.07\n",
      "Num timesteps: 2158000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.48\n",
      "Num timesteps: 2159000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.53\n",
      "Num timesteps: 2160000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 109.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 413      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1183     |\n",
      "|    iterations         | 21600    |\n",
      "|    time_elapsed       | 1824     |\n",
      "|    total_timesteps    | 2160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.98    |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21599    |\n",
      "|    policy_loss        | -2.99    |\n",
      "|    value_loss         | 15.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 2161000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 111.51\n",
      "Num timesteps: 2162000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 114.68\n",
      "Num timesteps: 2163000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.38\n",
      "Num timesteps: 2164000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 111.79\n",
      "Num timesteps: 2165000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.83\n",
      "Num timesteps: 2166000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 110.08\n",
      "Num timesteps: 2167000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 111.75\n",
      "Num timesteps: 2168000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 108.97\n",
      "Num timesteps: 2169000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 106.23\n",
      "Num timesteps: 2170000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 110.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 353      |\n",
      "|    ep_rew_mean        | 110      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1185     |\n",
      "|    iterations         | 21700    |\n",
      "|    time_elapsed       | 1831     |\n",
      "|    total_timesteps    | 2170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21699    |\n",
      "|    policy_loss        | -0.439   |\n",
      "|    value_loss         | 4.35     |\n",
      "------------------------------------\n",
      "Num timesteps: 2171000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 114.30\n",
      "Num timesteps: 2172000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.01\n",
      "Num timesteps: 2173000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.43\n",
      "Num timesteps: 2174000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.18\n",
      "Num timesteps: 2175000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.10\n",
      "Num timesteps: 2176000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 110.34\n",
      "Num timesteps: 2177000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 108.16\n",
      "Num timesteps: 2178000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 112.80\n",
      "Num timesteps: 2179000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 115.88\n",
      "Num timesteps: 2180000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 21800    |\n",
      "|    time_elapsed       | 1837     |\n",
      "|    total_timesteps    | 2180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.973   |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21799    |\n",
      "|    policy_loss        | -3.13    |\n",
      "|    value_loss         | 14       |\n",
      "------------------------------------\n",
      "Num timesteps: 2181000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.68\n",
      "Num timesteps: 2182000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 113.15\n",
      "Num timesteps: 2183000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 111.87\n",
      "Num timesteps: 2184000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 116.02\n",
      "Num timesteps: 2185000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.78\n",
      "Num timesteps: 2186000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.29\n",
      "Num timesteps: 2187000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 118.04\n",
      "Num timesteps: 2188000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 117.27\n",
      "Num timesteps: 2189000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 121.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2190000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 124.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 125      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1186     |\n",
      "|    iterations         | 21900    |\n",
      "|    time_elapsed       | 1845     |\n",
      "|    total_timesteps    | 2190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21899    |\n",
      "|    policy_loss        | 2.33     |\n",
      "|    value_loss         | 9.62     |\n",
      "------------------------------------\n",
      "Num timesteps: 2191000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.77\n",
      "Num timesteps: 2192000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 125.14\n",
      "Num timesteps: 2193000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 127.05\n",
      "Num timesteps: 2194000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.36\n",
      "Num timesteps: 2195000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 129.10\n",
      "Num timesteps: 2196000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 133.12\n",
      "Num timesteps: 2197000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 130.41\n",
      "Num timesteps: 2198000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 128.11\n",
      "Num timesteps: 2199000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 131.25\n",
      "Num timesteps: 2200000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 133.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 343      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 22000    |\n",
      "|    time_elapsed       | 1853     |\n",
      "|    total_timesteps    | 2200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | -5.73    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 21999    |\n",
      "|    policy_loss        | -13.7    |\n",
      "|    value_loss         | 1.9e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 2201000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.40\n",
      "Num timesteps: 2202000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 133.70\n",
      "Num timesteps: 2203000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.54\n",
      "Num timesteps: 2204000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 136.92\n",
      "Num timesteps: 2205000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 134.18\n",
      "Num timesteps: 2206000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 135.82\n",
      "Num timesteps: 2207000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 135.45\n",
      "Num timesteps: 2208000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.32\n",
      "Num timesteps: 2209000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 132.70\n",
      "Num timesteps: 2210000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 138.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 22100    |\n",
      "|    time_elapsed       | 1861     |\n",
      "|    total_timesteps    | 2210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | -7.6     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22099    |\n",
      "|    policy_loss        | 21       |\n",
      "|    value_loss         | 1.26e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2211000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 140.26\n",
      "Num timesteps: 2212000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 144.40\n",
      "Num timesteps: 2213000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 143.99\n",
      "Num timesteps: 2214000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 150.99\n",
      "Num timesteps: 2215000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 151.05\n",
      "Num timesteps: 2216000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 151.92\n",
      "Num timesteps: 2217000\n",
      "Best mean reward: 152.89 - Last mean reward per episode: 154.19\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2218000\n",
      "Best mean reward: 154.19 - Last mean reward per episode: 155.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2219000\n",
      "Best mean reward: 155.48 - Last mean reward per episode: 153.11\n",
      "Num timesteps: 2220000\n",
      "Best mean reward: 155.48 - Last mean reward per episode: 156.56\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 381      |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 22200    |\n",
      "|    time_elapsed       | 1868     |\n",
      "|    total_timesteps    | 2220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.473   |\n",
      "|    explained_variance | -4.18    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22199    |\n",
      "|    policy_loss        | 29.6     |\n",
      "|    value_loss         | 2.69e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2221000\n",
      "Best mean reward: 156.56 - Last mean reward per episode: 156.96\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2222000\n",
      "Best mean reward: 156.96 - Last mean reward per episode: 157.35\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2223000\n",
      "Best mean reward: 157.35 - Last mean reward per episode: 156.78\n",
      "Num timesteps: 2224000\n",
      "Best mean reward: 157.35 - Last mean reward per episode: 156.92\n",
      "Num timesteps: 2225000\n",
      "Best mean reward: 157.35 - Last mean reward per episode: 159.63\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2226000\n",
      "Best mean reward: 159.63 - Last mean reward per episode: 159.81\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2227000\n",
      "Best mean reward: 159.81 - Last mean reward per episode: 160.43\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2228000\n",
      "Best mean reward: 160.43 - Last mean reward per episode: 161.51\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2229000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 155.98\n",
      "Num timesteps: 2230000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 150.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 389      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1187     |\n",
      "|    iterations         | 22300    |\n",
      "|    time_elapsed       | 1877     |\n",
      "|    total_timesteps    | 2230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.296   |\n",
      "|    explained_variance | -1.1     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22299    |\n",
      "|    policy_loss        | -0.909   |\n",
      "|    value_loss         | 993      |\n",
      "------------------------------------\n",
      "Num timesteps: 2231000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 155.51\n",
      "Num timesteps: 2232000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 150.95\n",
      "Num timesteps: 2233000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 151.53\n",
      "Num timesteps: 2234000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 153.84\n",
      "Num timesteps: 2235000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 155.23\n",
      "Num timesteps: 2236000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 159.48\n",
      "Num timesteps: 2237000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 161.35\n",
      "Num timesteps: 2238000\n",
      "Best mean reward: 161.51 - Last mean reward per episode: 162.50\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2239000\n",
      "Best mean reward: 162.50 - Last mean reward per episode: 158.60\n",
      "Num timesteps: 2240000\n",
      "Best mean reward: 162.50 - Last mean reward per episode: 160.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 372      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1188     |\n",
      "|    iterations         | 22400    |\n",
      "|    time_elapsed       | 1885     |\n",
      "|    total_timesteps    | 2240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.00676  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22399    |\n",
      "|    policy_loss        | 5.27     |\n",
      "|    value_loss         | 1.21e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2241000\n",
      "Best mean reward: 162.50 - Last mean reward per episode: 160.40\n",
      "Num timesteps: 2242000\n",
      "Best mean reward: 162.50 - Last mean reward per episode: 162.85\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2243000\n",
      "Best mean reward: 162.85 - Last mean reward per episode: 162.80\n",
      "Num timesteps: 2244000\n",
      "Best mean reward: 162.85 - Last mean reward per episode: 164.79\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2245000\n",
      "Best mean reward: 164.79 - Last mean reward per episode: 165.48\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2246000\n",
      "Best mean reward: 165.48 - Last mean reward per episode: 165.00\n",
      "Num timesteps: 2247000\n",
      "Best mean reward: 165.48 - Last mean reward per episode: 167.40\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 2248000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 167.22\n",
      "Num timesteps: 2249000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 165.20\n",
      "Num timesteps: 2250000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 165.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1189     |\n",
      "|    iterations         | 22500    |\n",
      "|    time_elapsed       | 1891     |\n",
      "|    total_timesteps    | 2250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22499    |\n",
      "|    policy_loss        | 0.517    |\n",
      "|    value_loss         | 24       |\n",
      "------------------------------------\n",
      "Num timesteps: 2251000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 162.05\n",
      "Num timesteps: 2252000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 159.44\n",
      "Num timesteps: 2253000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.54\n",
      "Num timesteps: 2254000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.05\n",
      "Num timesteps: 2255000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.01\n",
      "Num timesteps: 2256000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.18\n",
      "Num timesteps: 2257000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.93\n",
      "Num timesteps: 2258000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.44\n",
      "Num timesteps: 2259000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.95\n",
      "Num timesteps: 2260000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.93\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 314      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1190     |\n",
      "|    iterations         | 22600    |\n",
      "|    time_elapsed       | 1897     |\n",
      "|    total_timesteps    | 2260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.945   |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22599    |\n",
      "|    policy_loss        | -0.727   |\n",
      "|    value_loss         | 4.57     |\n",
      "------------------------------------\n",
      "Num timesteps: 2261000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.15\n",
      "Num timesteps: 2262000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.14\n",
      "Num timesteps: 2263000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.45\n",
      "Num timesteps: 2264000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.68\n",
      "Num timesteps: 2265000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.38\n",
      "Num timesteps: 2266000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.19\n",
      "Num timesteps: 2267000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.94\n",
      "Num timesteps: 2268000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.20\n",
      "Num timesteps: 2269000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.85\n",
      "Num timesteps: 2270000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1191     |\n",
      "|    iterations         | 22700    |\n",
      "|    time_elapsed       | 1904     |\n",
      "|    total_timesteps    | 2270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.343   |\n",
      "|    explained_variance | 0.0306   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22699    |\n",
      "|    policy_loss        | 1.37     |\n",
      "|    value_loss         | 272      |\n",
      "------------------------------------\n",
      "Num timesteps: 2271000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.00\n",
      "Num timesteps: 2272000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.21\n",
      "Num timesteps: 2273000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.92\n",
      "Num timesteps: 2274000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.30\n",
      "Num timesteps: 2275000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.52\n",
      "Num timesteps: 2276000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.40\n",
      "Num timesteps: 2277000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.76\n",
      "Num timesteps: 2278000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.01\n",
      "Num timesteps: 2279000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.35\n",
      "Num timesteps: 2280000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 280      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1193     |\n",
      "|    iterations         | 22800    |\n",
      "|    time_elapsed       | 1910     |\n",
      "|    total_timesteps    | 2280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | -0.326   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22799    |\n",
      "|    policy_loss        | 2.32     |\n",
      "|    value_loss         | 24.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 2281000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.28\n",
      "Num timesteps: 2282000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.51\n",
      "Num timesteps: 2283000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.36\n",
      "Num timesteps: 2284000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.06\n",
      "Num timesteps: 2285000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.95\n",
      "Num timesteps: 2286000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.83\n",
      "Num timesteps: 2287000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.66\n",
      "Num timesteps: 2288000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.16\n",
      "Num timesteps: 2289000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.19\n",
      "Num timesteps: 2290000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1193     |\n",
      "|    iterations         | 22900    |\n",
      "|    time_elapsed       | 1918     |\n",
      "|    total_timesteps    | 2290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.972   |\n",
      "|    explained_variance | 0.204    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22899    |\n",
      "|    policy_loss        | -1.94    |\n",
      "|    value_loss         | 11       |\n",
      "------------------------------------\n",
      "Num timesteps: 2291000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.30\n",
      "Num timesteps: 2292000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.25\n",
      "Num timesteps: 2293000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.18\n",
      "Num timesteps: 2294000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.26\n",
      "Num timesteps: 2295000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.20\n",
      "Num timesteps: 2296000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2297000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.52\n",
      "Num timesteps: 2298000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.87\n",
      "Num timesteps: 2299000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.78\n",
      "Num timesteps: 2300000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 305      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1194     |\n",
      "|    iterations         | 23000    |\n",
      "|    time_elapsed       | 1925     |\n",
      "|    total_timesteps    | 2300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.918   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 22999    |\n",
      "|    policy_loss        | -1.63    |\n",
      "|    value_loss         | 8.08     |\n",
      "------------------------------------\n",
      "Num timesteps: 2301000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.16\n",
      "Num timesteps: 2302000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.12\n",
      "Num timesteps: 2303000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.06\n",
      "Num timesteps: 2304000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.43\n",
      "Num timesteps: 2305000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.45\n",
      "Num timesteps: 2306000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.93\n",
      "Num timesteps: 2307000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.82\n",
      "Num timesteps: 2308000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.22\n",
      "Num timesteps: 2309000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.80\n",
      "Num timesteps: 2310000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1195     |\n",
      "|    iterations         | 23100    |\n",
      "|    time_elapsed       | 1932     |\n",
      "|    total_timesteps    | 2310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23099    |\n",
      "|    policy_loss        | 1.69     |\n",
      "|    value_loss         | 5.84     |\n",
      "------------------------------------\n",
      "Num timesteps: 2311000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.67\n",
      "Num timesteps: 2312000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.42\n",
      "Num timesteps: 2313000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.86\n",
      "Num timesteps: 2314000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.09\n",
      "Num timesteps: 2315000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.05\n",
      "Num timesteps: 2316000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.13\n",
      "Num timesteps: 2317000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.72\n",
      "Num timesteps: 2318000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.44\n",
      "Num timesteps: 2319000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.97\n",
      "Num timesteps: 2320000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 337      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1195     |\n",
      "|    iterations         | 23200    |\n",
      "|    time_elapsed       | 1940     |\n",
      "|    total_timesteps    | 2320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | -8.12    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23199    |\n",
      "|    policy_loss        | 3.88     |\n",
      "|    value_loss         | 5.04e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2321000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.13\n",
      "Num timesteps: 2322000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.91\n",
      "Num timesteps: 2323000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.22\n",
      "Num timesteps: 2324000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.13\n",
      "Num timesteps: 2325000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.30\n",
      "Num timesteps: 2326000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.85\n",
      "Num timesteps: 2327000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.30\n",
      "Num timesteps: 2328000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.66\n",
      "Num timesteps: 2329000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.61\n",
      "Num timesteps: 2330000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.63\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 345      |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1195     |\n",
      "|    iterations         | 23300    |\n",
      "|    time_elapsed       | 1949     |\n",
      "|    total_timesteps    | 2330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.874   |\n",
      "|    explained_variance | 0.515    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23299    |\n",
      "|    policy_loss        | -69.8    |\n",
      "|    value_loss         | 8.85e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2331000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.36\n",
      "Num timesteps: 2332000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.40\n",
      "Num timesteps: 2333000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.80\n",
      "Num timesteps: 2334000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.64\n",
      "Num timesteps: 2335000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.06\n",
      "Num timesteps: 2336000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.67\n",
      "Num timesteps: 2337000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.21\n",
      "Num timesteps: 2338000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.22\n",
      "Num timesteps: 2339000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.87\n",
      "Num timesteps: 2340000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 344      |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1196     |\n",
      "|    iterations         | 23400    |\n",
      "|    time_elapsed       | 1956     |\n",
      "|    total_timesteps    | 2340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.661   |\n",
      "|    explained_variance | -9.11    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23399    |\n",
      "|    policy_loss        | 34.7     |\n",
      "|    value_loss         | 1.67e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2341000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 105.54\n",
      "Num timesteps: 2342000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 107.86\n",
      "Num timesteps: 2343000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 99.67\n",
      "Num timesteps: 2344000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 104.72\n",
      "Num timesteps: 2345000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 107.03\n",
      "Num timesteps: 2346000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 102.21\n",
      "Num timesteps: 2347000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 103.11\n",
      "Num timesteps: 2348000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 96.99\n",
      "Num timesteps: 2349000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 100.65\n",
      "Num timesteps: 2350000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 101.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1197     |\n",
      "|    iterations         | 23500    |\n",
      "|    time_elapsed       | 1962     |\n",
      "|    total_timesteps    | 2350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.336   |\n",
      "|    explained_variance | -0.0995  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23499    |\n",
      "|    policy_loss        | 0.282    |\n",
      "|    value_loss         | 259      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2351000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 96.76\n",
      "Num timesteps: 2352000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 96.17\n",
      "Num timesteps: 2353000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 103.71\n",
      "Num timesteps: 2354000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 104.23\n",
      "Num timesteps: 2355000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 102.62\n",
      "Num timesteps: 2356000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 103.09\n",
      "Num timesteps: 2357000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 106.77\n",
      "Num timesteps: 2358000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 104.51\n",
      "Num timesteps: 2359000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.39\n",
      "Num timesteps: 2360000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 107.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 299      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1198     |\n",
      "|    iterations         | 23600    |\n",
      "|    time_elapsed       | 1969     |\n",
      "|    total_timesteps    | 2360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23599    |\n",
      "|    policy_loss        | 1.03     |\n",
      "|    value_loss         | 20.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 2361000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.31\n",
      "Num timesteps: 2362000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.02\n",
      "Num timesteps: 2363000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.52\n",
      "Num timesteps: 2364000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.98\n",
      "Num timesteps: 2365000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.29\n",
      "Num timesteps: 2366000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.00\n",
      "Num timesteps: 2367000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.87\n",
      "Num timesteps: 2368000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.98\n",
      "Num timesteps: 2369000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.14\n",
      "Num timesteps: 2370000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1199     |\n",
      "|    iterations         | 23700    |\n",
      "|    time_elapsed       | 1976     |\n",
      "|    total_timesteps    | 2370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.571   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23699    |\n",
      "|    policy_loss        | 4.85     |\n",
      "|    value_loss         | 143      |\n",
      "------------------------------------\n",
      "Num timesteps: 2371000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.57\n",
      "Num timesteps: 2372000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.92\n",
      "Num timesteps: 2373000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.66\n",
      "Num timesteps: 2374000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.52\n",
      "Num timesteps: 2375000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.83\n",
      "Num timesteps: 2376000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.71\n",
      "Num timesteps: 2377000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.17\n",
      "Num timesteps: 2378000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.30\n",
      "Num timesteps: 2379000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.96\n",
      "Num timesteps: 2380000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 308      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1200     |\n",
      "|    iterations         | 23800    |\n",
      "|    time_elapsed       | 1982     |\n",
      "|    total_timesteps    | 2380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.802   |\n",
      "|    explained_variance | -0.995   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23799    |\n",
      "|    policy_loss        | -8.41    |\n",
      "|    value_loss         | 148      |\n",
      "------------------------------------\n",
      "Num timesteps: 2381000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.25\n",
      "Num timesteps: 2382000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.37\n",
      "Num timesteps: 2383000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.83\n",
      "Num timesteps: 2384000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.52\n",
      "Num timesteps: 2385000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.77\n",
      "Num timesteps: 2386000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.65\n",
      "Num timesteps: 2387000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.13\n",
      "Num timesteps: 2388000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.36\n",
      "Num timesteps: 2389000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.50\n",
      "Num timesteps: 2390000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 317      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1200     |\n",
      "|    iterations         | 23900    |\n",
      "|    time_elapsed       | 1990     |\n",
      "|    total_timesteps    | 2390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.602   |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23899    |\n",
      "|    policy_loss        | 1.83     |\n",
      "|    value_loss         | 1.52e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2391000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.01\n",
      "Num timesteps: 2392000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.40\n",
      "Num timesteps: 2393000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.19\n",
      "Num timesteps: 2394000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.47\n",
      "Num timesteps: 2395000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.56\n",
      "Num timesteps: 2396000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.02\n",
      "Num timesteps: 2397000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.10\n",
      "Num timesteps: 2398000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.44\n",
      "Num timesteps: 2399000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.78\n",
      "Num timesteps: 2400000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.93\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 316      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1201     |\n",
      "|    iterations         | 24000    |\n",
      "|    time_elapsed       | 1997     |\n",
      "|    total_timesteps    | 2400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 23999    |\n",
      "|    policy_loss        | 0.966    |\n",
      "|    value_loss         | 11.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 2401000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.10\n",
      "Num timesteps: 2402000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.33\n",
      "Num timesteps: 2403000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.26\n",
      "Num timesteps: 2404000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.91\n",
      "Num timesteps: 2405000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.43\n",
      "Num timesteps: 2406000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.14\n",
      "Num timesteps: 2407000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.99\n",
      "Num timesteps: 2408000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.69\n",
      "Num timesteps: 2409000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2410000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1201     |\n",
      "|    iterations         | 24100    |\n",
      "|    time_elapsed       | 2005     |\n",
      "|    total_timesteps    | 2410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.986   |\n",
      "|    explained_variance | -0.541   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24099    |\n",
      "|    policy_loss        | 20.3     |\n",
      "|    value_loss         | 640      |\n",
      "------------------------------------\n",
      "Num timesteps: 2411000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.00\n",
      "Num timesteps: 2412000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.25\n",
      "Num timesteps: 2413000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.43\n",
      "Num timesteps: 2414000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.22\n",
      "Num timesteps: 2415000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.93\n",
      "Num timesteps: 2416000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.91\n",
      "Num timesteps: 2417000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.76\n",
      "Num timesteps: 2418000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.71\n",
      "Num timesteps: 2419000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.66\n",
      "Num timesteps: 2420000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1201     |\n",
      "|    iterations         | 24200    |\n",
      "|    time_elapsed       | 2013     |\n",
      "|    total_timesteps    | 2420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.909   |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24199    |\n",
      "|    policy_loss        | -2.42    |\n",
      "|    value_loss         | 11.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 2421000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.61\n",
      "Num timesteps: 2422000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.51\n",
      "Num timesteps: 2423000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.35\n",
      "Num timesteps: 2424000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.47\n",
      "Num timesteps: 2425000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.56\n",
      "Num timesteps: 2426000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.98\n",
      "Num timesteps: 2427000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.94\n",
      "Num timesteps: 2428000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.00\n",
      "Num timesteps: 2429000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.04\n",
      "Num timesteps: 2430000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 106.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 107      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1200     |\n",
      "|    iterations         | 24300    |\n",
      "|    time_elapsed       | 2024     |\n",
      "|    total_timesteps    | 2430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.361   |\n",
      "|    explained_variance | 0.439    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24299    |\n",
      "|    policy_loss        | -3.45    |\n",
      "|    value_loss         | 88.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 2431000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.93\n",
      "Num timesteps: 2432000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 106.25\n",
      "Num timesteps: 2433000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.36\n",
      "Num timesteps: 2434000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.89\n",
      "Num timesteps: 2435000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 104.87\n",
      "Num timesteps: 2436000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.27\n",
      "Num timesteps: 2437000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.98\n",
      "Num timesteps: 2438000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.20\n",
      "Num timesteps: 2439000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.11\n",
      "Num timesteps: 2440000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 107.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1201     |\n",
      "|    iterations         | 24400    |\n",
      "|    time_elapsed       | 2031     |\n",
      "|    total_timesteps    | 2440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.95    |\n",
      "|    explained_variance | 0.77     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24399    |\n",
      "|    policy_loss        | -0.568   |\n",
      "|    value_loss         | 4.58     |\n",
      "------------------------------------\n",
      "Num timesteps: 2441000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.08\n",
      "Num timesteps: 2442000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.49\n",
      "Num timesteps: 2443000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.40\n",
      "Num timesteps: 2444000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.44\n",
      "Num timesteps: 2445000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.98\n",
      "Num timesteps: 2446000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.13\n",
      "Num timesteps: 2447000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.67\n",
      "Num timesteps: 2448000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.36\n",
      "Num timesteps: 2449000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.17\n",
      "Num timesteps: 2450000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1202     |\n",
      "|    iterations         | 24500    |\n",
      "|    time_elapsed       | 2037     |\n",
      "|    total_timesteps    | 2450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.724   |\n",
      "|    explained_variance | 0.0812   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24499    |\n",
      "|    policy_loss        | -69      |\n",
      "|    value_loss         | 7.17e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2451000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.22\n",
      "Num timesteps: 2452000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.07\n",
      "Num timesteps: 2453000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.57\n",
      "Num timesteps: 2454000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 107.36\n",
      "Num timesteps: 2455000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.38\n",
      "Num timesteps: 2456000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.05\n",
      "Num timesteps: 2457000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.40\n",
      "Num timesteps: 2458000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.34\n",
      "Num timesteps: 2459000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.94\n",
      "Num timesteps: 2460000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.78\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1203     |\n",
      "|    iterations         | 24600    |\n",
      "|    time_elapsed       | 2044     |\n",
      "|    total_timesteps    | 2460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.461   |\n",
      "|    explained_variance | -14.3    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24599    |\n",
      "|    policy_loss        | 23.3     |\n",
      "|    value_loss         | 1.2e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2461000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.77\n",
      "Num timesteps: 2462000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.05\n",
      "Num timesteps: 2463000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.80\n",
      "Num timesteps: 2464000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.95\n",
      "Num timesteps: 2465000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.86\n",
      "Num timesteps: 2466000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.26\n",
      "Num timesteps: 2467000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.99\n",
      "Num timesteps: 2468000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.18\n",
      "Num timesteps: 2469000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.82\n",
      "Num timesteps: 2470000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1203     |\n",
      "|    iterations         | 24700    |\n",
      "|    time_elapsed       | 2051     |\n",
      "|    total_timesteps    | 2470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.476   |\n",
      "|    explained_variance | -4       |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24699    |\n",
      "|    policy_loss        | 19.2     |\n",
      "|    value_loss         | 766      |\n",
      "------------------------------------\n",
      "Num timesteps: 2471000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.06\n",
      "Num timesteps: 2472000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.60\n",
      "Num timesteps: 2473000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.68\n",
      "Num timesteps: 2474000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.25\n",
      "Num timesteps: 2475000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.81\n",
      "Num timesteps: 2476000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.13\n",
      "Num timesteps: 2477000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.61\n",
      "Num timesteps: 2478000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.57\n",
      "Num timesteps: 2479000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.40\n",
      "Num timesteps: 2480000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.75\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 320      |\n",
      "|    ep_rew_mean        | 110      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 24800    |\n",
      "|    time_elapsed       | 2059     |\n",
      "|    total_timesteps    | 2480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.969   |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24799    |\n",
      "|    policy_loss        | 0.0852   |\n",
      "|    value_loss         | 4.07     |\n",
      "------------------------------------\n",
      "Num timesteps: 2481000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.82\n",
      "Num timesteps: 2482000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.11\n",
      "Num timesteps: 2483000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.53\n",
      "Num timesteps: 2484000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 106.77\n",
      "Num timesteps: 2485000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.60\n",
      "Num timesteps: 2486000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.31\n",
      "Num timesteps: 2487000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.70\n",
      "Num timesteps: 2488000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.08\n",
      "Num timesteps: 2489000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.96\n",
      "Num timesteps: 2490000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 112.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 24900    |\n",
      "|    time_elapsed       | 2067     |\n",
      "|    total_timesteps    | 2490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.835   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24899    |\n",
      "|    policy_loss        | 1.95     |\n",
      "|    value_loss         | 21.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 2491000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.14\n",
      "Num timesteps: 2492000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.26\n",
      "Num timesteps: 2493000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.52\n",
      "Num timesteps: 2494000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 115.69\n",
      "Num timesteps: 2495000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.08\n",
      "Num timesteps: 2496000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.70\n",
      "Num timesteps: 2497000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.35\n",
      "Num timesteps: 2498000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.68\n",
      "Num timesteps: 2499000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.16\n",
      "Num timesteps: 2500000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 353      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25000    |\n",
      "|    time_elapsed       | 2075     |\n",
      "|    total_timesteps    | 2500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | 0.015    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 24999    |\n",
      "|    policy_loss        | 1.5      |\n",
      "|    value_loss         | 6.14     |\n",
      "------------------------------------\n",
      "Num timesteps: 2501000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.98\n",
      "Num timesteps: 2502000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.62\n",
      "Num timesteps: 2503000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.78\n",
      "Num timesteps: 2504000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.53\n",
      "Num timesteps: 2505000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.25\n",
      "Num timesteps: 2506000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.16\n",
      "Num timesteps: 2507000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.88\n",
      "Num timesteps: 2508000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.38\n",
      "Num timesteps: 2509000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.34\n",
      "Num timesteps: 2510000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1205     |\n",
      "|    iterations         | 25100    |\n",
      "|    time_elapsed       | 2082     |\n",
      "|    total_timesteps    | 2510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25099    |\n",
      "|    policy_loss        | 1.64     |\n",
      "|    value_loss         | 14.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 2511000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.58\n",
      "Num timesteps: 2512000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.21\n",
      "Num timesteps: 2513000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.21\n",
      "Num timesteps: 2514000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.36\n",
      "Num timesteps: 2515000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.30\n",
      "Num timesteps: 2516000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.39\n",
      "Num timesteps: 2517000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.96\n",
      "Num timesteps: 2518000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.64\n",
      "Num timesteps: 2519000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2520000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25200    |\n",
      "|    time_elapsed       | 2092     |\n",
      "|    total_timesteps    | 2520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25199    |\n",
      "|    policy_loss        | -0.999   |\n",
      "|    value_loss         | 4.78     |\n",
      "------------------------------------\n",
      "Num timesteps: 2521000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.40\n",
      "Num timesteps: 2522000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.19\n",
      "Num timesteps: 2523000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.63\n",
      "Num timesteps: 2524000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.48\n",
      "Num timesteps: 2525000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.69\n",
      "Num timesteps: 2526000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.20\n",
      "Num timesteps: 2527000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.83\n",
      "Num timesteps: 2528000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.47\n",
      "Num timesteps: 2529000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.48\n",
      "Num timesteps: 2530000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 367      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25300    |\n",
      "|    time_elapsed       | 2100     |\n",
      "|    total_timesteps    | 2530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.266    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25299    |\n",
      "|    policy_loss        | -5.37    |\n",
      "|    value_loss         | 79.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 2531000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.74\n",
      "Num timesteps: 2532000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.58\n",
      "Num timesteps: 2533000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.95\n",
      "Num timesteps: 2534000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.46\n",
      "Num timesteps: 2535000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.91\n",
      "Num timesteps: 2536000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.04\n",
      "Num timesteps: 2537000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.97\n",
      "Num timesteps: 2538000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.20\n",
      "Num timesteps: 2539000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.98\n",
      "Num timesteps: 2540000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 357      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1205     |\n",
      "|    iterations         | 25400    |\n",
      "|    time_elapsed       | 2107     |\n",
      "|    total_timesteps    | 2540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | -1.34    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25399    |\n",
      "|    policy_loss        | 0.36     |\n",
      "|    value_loss         | 822      |\n",
      "------------------------------------\n",
      "Num timesteps: 2541000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.65\n",
      "Num timesteps: 2542000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.23\n",
      "Num timesteps: 2543000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.11\n",
      "Num timesteps: 2544000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.58\n",
      "Num timesteps: 2545000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.01\n",
      "Num timesteps: 2546000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.35\n",
      "Num timesteps: 2547000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.13\n",
      "Num timesteps: 2548000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.09\n",
      "Num timesteps: 2549000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.56\n",
      "Num timesteps: 2550000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 359      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25500    |\n",
      "|    time_elapsed       | 2117     |\n",
      "|    total_timesteps    | 2550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25499    |\n",
      "|    policy_loss        | -1.07    |\n",
      "|    value_loss         | 147      |\n",
      "------------------------------------\n",
      "Num timesteps: 2551000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.25\n",
      "Num timesteps: 2552000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.86\n",
      "Num timesteps: 2553000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.76\n",
      "Num timesteps: 2554000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.69\n",
      "Num timesteps: 2555000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.70\n",
      "Num timesteps: 2556000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.96\n",
      "Num timesteps: 2557000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.43\n",
      "Num timesteps: 2558000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.15\n",
      "Num timesteps: 2559000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.78\n",
      "Num timesteps: 2560000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.65\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 347      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1205     |\n",
      "|    iterations         | 25600    |\n",
      "|    time_elapsed       | 2124     |\n",
      "|    total_timesteps    | 2560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | -6.5     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25599    |\n",
      "|    policy_loss        | -1.91    |\n",
      "|    value_loss         | 182      |\n",
      "------------------------------------\n",
      "Num timesteps: 2561000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.42\n",
      "Num timesteps: 2562000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.62\n",
      "Num timesteps: 2563000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.88\n",
      "Num timesteps: 2564000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.53\n",
      "Num timesteps: 2565000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.16\n",
      "Num timesteps: 2566000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.24\n",
      "Num timesteps: 2567000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.17\n",
      "Num timesteps: 2568000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.25\n",
      "Num timesteps: 2569000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.73\n",
      "Num timesteps: 2570000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25700    |\n",
      "|    time_elapsed       | 2133     |\n",
      "|    total_timesteps    | 2570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.916   |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25699    |\n",
      "|    policy_loss        | -2.31    |\n",
      "|    value_loss         | 12.1     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2571000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.36\n",
      "Num timesteps: 2572000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.87\n",
      "Num timesteps: 2573000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.23\n",
      "Num timesteps: 2574000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.81\n",
      "Num timesteps: 2575000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.35\n",
      "Num timesteps: 2576000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.22\n",
      "Num timesteps: 2577000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.55\n",
      "Num timesteps: 2578000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.73\n",
      "Num timesteps: 2579000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.25\n",
      "Num timesteps: 2580000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25800    |\n",
      "|    time_elapsed       | 2142     |\n",
      "|    total_timesteps    | 2580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | -0.449   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25799    |\n",
      "|    policy_loss        | -5.1     |\n",
      "|    value_loss         | 40.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 2581000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.61\n",
      "Num timesteps: 2582000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.07\n",
      "Num timesteps: 2583000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.72\n",
      "Num timesteps: 2584000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.34\n",
      "Num timesteps: 2585000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.63\n",
      "Num timesteps: 2586000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.00\n",
      "Num timesteps: 2587000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.49\n",
      "Num timesteps: 2588000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.08\n",
      "Num timesteps: 2589000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.14\n",
      "Num timesteps: 2590000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 391      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 25900    |\n",
      "|    time_elapsed       | 2150     |\n",
      "|    total_timesteps    | 2590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | -0.408   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25899    |\n",
      "|    policy_loss        | 3.37     |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 2591000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.07\n",
      "Num timesteps: 2592000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.56\n",
      "Num timesteps: 2593000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.27\n",
      "Num timesteps: 2594000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.29\n",
      "Num timesteps: 2595000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.57\n",
      "Num timesteps: 2596000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.70\n",
      "Num timesteps: 2597000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.07\n",
      "Num timesteps: 2598000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.31\n",
      "Num timesteps: 2599000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.24\n",
      "Num timesteps: 2600000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 416      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 26000    |\n",
      "|    time_elapsed       | 2158     |\n",
      "|    total_timesteps    | 2600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.948   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 25999    |\n",
      "|    policy_loss        | 1.24     |\n",
      "|    value_loss         | 5.89     |\n",
      "------------------------------------\n",
      "Num timesteps: 2601000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.30\n",
      "Num timesteps: 2602000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.62\n",
      "Num timesteps: 2603000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.83\n",
      "Num timesteps: 2604000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.12\n",
      "Num timesteps: 2605000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.65\n",
      "Num timesteps: 2606000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.40\n",
      "Num timesteps: 2607000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 159.14\n",
      "Num timesteps: 2608000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.80\n",
      "Num timesteps: 2609000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.98\n",
      "Num timesteps: 2610000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 417      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 26100    |\n",
      "|    time_elapsed       | 2167     |\n",
      "|    total_timesteps    | 2610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | -0.225   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26099    |\n",
      "|    policy_loss        | 17.9     |\n",
      "|    value_loss         | 522      |\n",
      "------------------------------------\n",
      "Num timesteps: 2611000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.31\n",
      "Num timesteps: 2612000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.20\n",
      "Num timesteps: 2613000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.09\n",
      "Num timesteps: 2614000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.02\n",
      "Num timesteps: 2615000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.78\n",
      "Num timesteps: 2616000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.16\n",
      "Num timesteps: 2617000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.24\n",
      "Num timesteps: 2618000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.68\n",
      "Num timesteps: 2619000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.72\n",
      "Num timesteps: 2620000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1204     |\n",
      "|    iterations         | 26200    |\n",
      "|    time_elapsed       | 2174     |\n",
      "|    total_timesteps    | 2620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.87    |\n",
      "|    explained_variance | -2.51    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26199    |\n",
      "|    policy_loss        | 30       |\n",
      "|    value_loss         | 1.45e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2621000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.52\n",
      "Num timesteps: 2622000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.68\n",
      "Num timesteps: 2623000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.73\n",
      "Num timesteps: 2624000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.14\n",
      "Num timesteps: 2625000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.24\n",
      "Num timesteps: 2626000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.78\n",
      "Num timesteps: 2627000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.45\n",
      "Num timesteps: 2628000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.01\n",
      "Num timesteps: 2629000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2630000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 390      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1205     |\n",
      "|    iterations         | 26300    |\n",
      "|    time_elapsed       | 2181     |\n",
      "|    total_timesteps    | 2630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.05    |\n",
      "|    explained_variance | 0.000236 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26299    |\n",
      "|    policy_loss        | -3.43    |\n",
      "|    value_loss         | 25.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 2631000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.39\n",
      "Num timesteps: 2632000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.45\n",
      "Num timesteps: 2633000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.96\n",
      "Num timesteps: 2634000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.15\n",
      "Num timesteps: 2635000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.40\n",
      "Num timesteps: 2636000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.40\n",
      "Num timesteps: 2637000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.19\n",
      "Num timesteps: 2638000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.94\n",
      "Num timesteps: 2639000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.49\n",
      "Num timesteps: 2640000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 371      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1206     |\n",
      "|    iterations         | 26400    |\n",
      "|    time_elapsed       | 2188     |\n",
      "|    total_timesteps    | 2640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26399    |\n",
      "|    policy_loss        | -3.01    |\n",
      "|    value_loss         | 21.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 2641000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.69\n",
      "Num timesteps: 2642000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.38\n",
      "Num timesteps: 2643000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.99\n",
      "Num timesteps: 2644000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.12\n",
      "Num timesteps: 2645000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.13\n",
      "Num timesteps: 2646000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.27\n",
      "Num timesteps: 2647000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.18\n",
      "Num timesteps: 2648000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.99\n",
      "Num timesteps: 2649000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.62\n",
      "Num timesteps: 2650000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1207     |\n",
      "|    iterations         | 26500    |\n",
      "|    time_elapsed       | 2194     |\n",
      "|    total_timesteps    | 2650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.954   |\n",
      "|    explained_variance | 0.608    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26499    |\n",
      "|    policy_loss        | -50.6    |\n",
      "|    value_loss         | 4.26e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2651000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.03\n",
      "Num timesteps: 2652000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.84\n",
      "Num timesteps: 2653000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.49\n",
      "Num timesteps: 2654000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.27\n",
      "Num timesteps: 2655000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.04\n",
      "Num timesteps: 2656000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.49\n",
      "Num timesteps: 2657000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.35\n",
      "Num timesteps: 2658000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.14\n",
      "Num timesteps: 2659000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.84\n",
      "Num timesteps: 2660000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1208     |\n",
      "|    iterations         | 26600    |\n",
      "|    time_elapsed       | 2201     |\n",
      "|    total_timesteps    | 2660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.953   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26599    |\n",
      "|    policy_loss        | -0.104   |\n",
      "|    value_loss         | 5.76     |\n",
      "------------------------------------\n",
      "Num timesteps: 2661000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.70\n",
      "Num timesteps: 2662000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.61\n",
      "Num timesteps: 2663000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.37\n",
      "Num timesteps: 2664000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.27\n",
      "Num timesteps: 2665000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.57\n",
      "Num timesteps: 2666000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.76\n",
      "Num timesteps: 2667000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.35\n",
      "Num timesteps: 2668000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.97\n",
      "Num timesteps: 2669000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.58\n",
      "Num timesteps: 2670000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 308      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1209     |\n",
      "|    iterations         | 26700    |\n",
      "|    time_elapsed       | 2207     |\n",
      "|    total_timesteps    | 2670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26699    |\n",
      "|    policy_loss        | -7.02    |\n",
      "|    value_loss         | 63       |\n",
      "------------------------------------\n",
      "Num timesteps: 2671000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.16\n",
      "Num timesteps: 2672000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.71\n",
      "Num timesteps: 2673000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.40\n",
      "Num timesteps: 2674000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.94\n",
      "Num timesteps: 2675000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.45\n",
      "Num timesteps: 2676000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.65\n",
      "Num timesteps: 2677000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.86\n",
      "Num timesteps: 2678000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.62\n",
      "Num timesteps: 2679000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.10\n",
      "Num timesteps: 2680000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1210     |\n",
      "|    iterations         | 26800    |\n",
      "|    time_elapsed       | 2213     |\n",
      "|    total_timesteps    | 2680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | 0.511    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26799    |\n",
      "|    policy_loss        | 0.0718   |\n",
      "|    value_loss         | 180      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2681000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.87\n",
      "Num timesteps: 2682000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.64\n",
      "Num timesteps: 2683000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.62\n",
      "Num timesteps: 2684000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 151.38\n",
      "Num timesteps: 2685000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.68\n",
      "Num timesteps: 2686000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.52\n",
      "Num timesteps: 2687000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.43\n",
      "Num timesteps: 2688000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.91\n",
      "Num timesteps: 2689000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.41\n",
      "Num timesteps: 2690000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 281      |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1212     |\n",
      "|    iterations         | 26900    |\n",
      "|    time_elapsed       | 2219     |\n",
      "|    total_timesteps    | 2690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.958   |\n",
      "|    explained_variance | 0.887    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26899    |\n",
      "|    policy_loss        | -0.466   |\n",
      "|    value_loss         | 49.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 2691000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.29\n",
      "Num timesteps: 2692000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.97\n",
      "Num timesteps: 2693000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 155.86\n",
      "Num timesteps: 2694000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.86\n",
      "Num timesteps: 2695000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.79\n",
      "Num timesteps: 2696000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.77\n",
      "Num timesteps: 2697000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 160.63\n",
      "Num timesteps: 2698000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.61\n",
      "Num timesteps: 2699000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.04\n",
      "Num timesteps: 2700000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.15\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 259      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1213     |\n",
      "|    iterations         | 27000    |\n",
      "|    time_elapsed       | 2224     |\n",
      "|    total_timesteps    | 2700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.919   |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 26999    |\n",
      "|    policy_loss        | -1.1     |\n",
      "|    value_loss         | 37       |\n",
      "------------------------------------\n",
      "Num timesteps: 2701000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.38\n",
      "Num timesteps: 2702000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.89\n",
      "Num timesteps: 2703000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.40\n",
      "Num timesteps: 2704000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.95\n",
      "Num timesteps: 2705000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.98\n",
      "Num timesteps: 2706000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.91\n",
      "Num timesteps: 2707000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.79\n",
      "Num timesteps: 2708000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.43\n",
      "Num timesteps: 2709000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.23\n",
      "Num timesteps: 2710000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 255      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1215     |\n",
      "|    iterations         | 27100    |\n",
      "|    time_elapsed       | 2230     |\n",
      "|    total_timesteps    | 2710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.632   |\n",
      "|    explained_variance | 0.000771 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27099    |\n",
      "|    policy_loss        | -9.25    |\n",
      "|    value_loss         | 213      |\n",
      "------------------------------------\n",
      "Num timesteps: 2711000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.21\n",
      "Num timesteps: 2712000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.22\n",
      "Num timesteps: 2713000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.50\n",
      "Num timesteps: 2714000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.80\n",
      "Num timesteps: 2715000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.54\n",
      "Num timesteps: 2716000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.59\n",
      "Num timesteps: 2717000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.50\n",
      "Num timesteps: 2718000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.61\n",
      "Num timesteps: 2719000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.01\n",
      "Num timesteps: 2720000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 227      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1216     |\n",
      "|    iterations         | 27200    |\n",
      "|    time_elapsed       | 2235     |\n",
      "|    total_timesteps    | 2720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.869   |\n",
      "|    explained_variance | 0.362    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27199    |\n",
      "|    policy_loss        | 21.3     |\n",
      "|    value_loss         | 928      |\n",
      "------------------------------------\n",
      "Num timesteps: 2721000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.08\n",
      "Num timesteps: 2722000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.95\n",
      "Num timesteps: 2723000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.07\n",
      "Num timesteps: 2724000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 125.76\n",
      "Num timesteps: 2725000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.95\n",
      "Num timesteps: 2726000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.24\n",
      "Num timesteps: 2727000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.16\n",
      "Num timesteps: 2728000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.59\n",
      "Num timesteps: 2729000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.82\n",
      "Num timesteps: 2730000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 232      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1218     |\n",
      "|    iterations         | 27300    |\n",
      "|    time_elapsed       | 2240     |\n",
      "|    total_timesteps    | 2730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27299    |\n",
      "|    policy_loss        | -5.62    |\n",
      "|    value_loss         | 74.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 2731000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.73\n",
      "Num timesteps: 2732000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.49\n",
      "Num timesteps: 2733000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.28\n",
      "Num timesteps: 2734000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.81\n",
      "Num timesteps: 2735000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.39\n",
      "Num timesteps: 2736000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.57\n",
      "Num timesteps: 2737000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.29\n",
      "Num timesteps: 2738000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.00\n",
      "Num timesteps: 2739000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2740000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.74\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 241      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1219     |\n",
      "|    iterations         | 27400    |\n",
      "|    time_elapsed       | 2245     |\n",
      "|    total_timesteps    | 2740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.838   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27399    |\n",
      "|    policy_loss        | -0.733   |\n",
      "|    value_loss         | 5.77     |\n",
      "------------------------------------\n",
      "Num timesteps: 2741000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.07\n",
      "Num timesteps: 2742000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.68\n",
      "Num timesteps: 2743000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.51\n",
      "Num timesteps: 2744000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.61\n",
      "Num timesteps: 2745000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.22\n",
      "Num timesteps: 2746000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.89\n",
      "Num timesteps: 2747000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.76\n",
      "Num timesteps: 2748000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 156.67\n",
      "Num timesteps: 2749000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 151.78\n",
      "Num timesteps: 2750000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 257      |\n",
      "|    ep_rew_mean        | 147      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1221     |\n",
      "|    iterations         | 27500    |\n",
      "|    time_elapsed       | 2251     |\n",
      "|    total_timesteps    | 2750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.738   |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27499    |\n",
      "|    policy_loss        | -14.7    |\n",
      "|    value_loss         | 1.91e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2751000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.71\n",
      "Num timesteps: 2752000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.40\n",
      "Num timesteps: 2753000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.07\n",
      "Num timesteps: 2754000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.11\n",
      "Num timesteps: 2755000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.42\n",
      "Num timesteps: 2756000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 144.20\n",
      "Num timesteps: 2757000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.99\n",
      "Num timesteps: 2758000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.29\n",
      "Num timesteps: 2759000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.03\n",
      "Num timesteps: 2760000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.78\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 258      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1222     |\n",
      "|    iterations         | 27600    |\n",
      "|    time_elapsed       | 2257     |\n",
      "|    total_timesteps    | 2760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.726   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27599    |\n",
      "|    policy_loss        | -6.71    |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "Num timesteps: 2761000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 144.85\n",
      "Num timesteps: 2762000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.72\n",
      "Num timesteps: 2763000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 144.08\n",
      "Num timesteps: 2764000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.92\n",
      "Num timesteps: 2765000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.79\n",
      "Num timesteps: 2766000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.14\n",
      "Num timesteps: 2767000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.47\n",
      "Num timesteps: 2768000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.86\n",
      "Num timesteps: 2769000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.03\n",
      "Num timesteps: 2770000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 260      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1223     |\n",
      "|    iterations         | 27700    |\n",
      "|    time_elapsed       | 2263     |\n",
      "|    total_timesteps    | 2770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.93    |\n",
      "|    explained_variance | 0.0585   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27699    |\n",
      "|    policy_loss        | -0.936   |\n",
      "|    value_loss         | 14.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 2771000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 118.34\n",
      "Num timesteps: 2772000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.21\n",
      "Num timesteps: 2773000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.19\n",
      "Num timesteps: 2774000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.06\n",
      "Num timesteps: 2775000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.13\n",
      "Num timesteps: 2776000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.90\n",
      "Num timesteps: 2777000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 120.72\n",
      "Num timesteps: 2778000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 113.68\n",
      "Num timesteps: 2779000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.54\n",
      "Num timesteps: 2780000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 110.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 267      |\n",
      "|    ep_rew_mean        | 110      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1224     |\n",
      "|    iterations         | 27800    |\n",
      "|    time_elapsed       | 2270     |\n",
      "|    total_timesteps    | 2780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.662   |\n",
      "|    explained_variance | 0.466    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27799    |\n",
      "|    policy_loss        | -2.76    |\n",
      "|    value_loss         | 3.2e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 2781000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 109.53\n",
      "Num timesteps: 2782000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 111.18\n",
      "Num timesteps: 2783000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 106.05\n",
      "Num timesteps: 2784000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 101.22\n",
      "Num timesteps: 2785000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 99.64\n",
      "Num timesteps: 2786000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 101.79\n",
      "Num timesteps: 2787000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 100.39\n",
      "Num timesteps: 2788000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 108.30\n",
      "Num timesteps: 2789000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 114.45\n",
      "Num timesteps: 2790000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 116.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 282      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1225     |\n",
      "|    iterations         | 27900    |\n",
      "|    time_elapsed       | 2276     |\n",
      "|    total_timesteps    | 2790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.492   |\n",
      "|    explained_variance | 0.433    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27899    |\n",
      "|    policy_loss        | 9.75     |\n",
      "|    value_loss         | 1.08e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2791000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 119.86\n",
      "Num timesteps: 2792000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 117.31\n",
      "Num timesteps: 2793000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 121.22\n",
      "Num timesteps: 2794000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.01\n",
      "Num timesteps: 2795000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 122.09\n",
      "Num timesteps: 2796000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.16\n",
      "Num timesteps: 2797000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.81\n",
      "Num timesteps: 2798000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.66\n",
      "Num timesteps: 2799000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.72\n",
      "Num timesteps: 2800000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 265      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1227     |\n",
      "|    iterations         | 28000    |\n",
      "|    time_elapsed       | 2281     |\n",
      "|    total_timesteps    | 2800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.814   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 27999    |\n",
      "|    policy_loss        | 0.941    |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 2801000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 129.55\n",
      "Num timesteps: 2802000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 126.06\n",
      "Num timesteps: 2803000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.09\n",
      "Num timesteps: 2804000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.79\n",
      "Num timesteps: 2805000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 130.74\n",
      "Num timesteps: 2806000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.89\n",
      "Num timesteps: 2807000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.28\n",
      "Num timesteps: 2808000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.55\n",
      "Num timesteps: 2809000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.07\n",
      "Num timesteps: 2810000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 289      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1228     |\n",
      "|    iterations         | 28100    |\n",
      "|    time_elapsed       | 2288     |\n",
      "|    total_timesteps    | 2810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.562   |\n",
      "|    explained_variance | -5.52    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28099    |\n",
      "|    policy_loss        | 28.6     |\n",
      "|    value_loss         | 2.12e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2811000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.48\n",
      "Num timesteps: 2812000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 151.52\n",
      "Num timesteps: 2813000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.30\n",
      "Num timesteps: 2814000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.40\n",
      "Num timesteps: 2815000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.32\n",
      "Num timesteps: 2816000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.25\n",
      "Num timesteps: 2817000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.89\n",
      "Num timesteps: 2818000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.37\n",
      "Num timesteps: 2819000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.18\n",
      "Num timesteps: 2820000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1228     |\n",
      "|    iterations         | 28200    |\n",
      "|    time_elapsed       | 2295     |\n",
      "|    total_timesteps    | 2820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0533  |\n",
      "|    explained_variance | 0.644    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28199    |\n",
      "|    policy_loss        | -0.0905  |\n",
      "|    value_loss         | 115      |\n",
      "------------------------------------\n",
      "Num timesteps: 2821000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.46\n",
      "Num timesteps: 2822000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.73\n",
      "Num timesteps: 2823000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.38\n",
      "Num timesteps: 2824000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.90\n",
      "Num timesteps: 2825000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 141.84\n",
      "Num timesteps: 2826000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.96\n",
      "Num timesteps: 2827000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.91\n",
      "Num timesteps: 2828000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.73\n",
      "Num timesteps: 2829000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.81\n",
      "Num timesteps: 2830000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 325      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1229     |\n",
      "|    iterations         | 28300    |\n",
      "|    time_elapsed       | 2302     |\n",
      "|    total_timesteps    | 2830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.484   |\n",
      "|    explained_variance | 0.625    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28299    |\n",
      "|    policy_loss        | 0.168    |\n",
      "|    value_loss         | 164      |\n",
      "------------------------------------\n",
      "Num timesteps: 2831000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.10\n",
      "Num timesteps: 2832000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.30\n",
      "Num timesteps: 2833000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 151.18\n",
      "Num timesteps: 2834000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.58\n",
      "Num timesteps: 2835000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.99\n",
      "Num timesteps: 2836000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.01\n",
      "Num timesteps: 2837000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.41\n",
      "Num timesteps: 2838000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.18\n",
      "Num timesteps: 2839000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.48\n",
      "Num timesteps: 2840000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 334      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1229     |\n",
      "|    iterations         | 28400    |\n",
      "|    time_elapsed       | 2309     |\n",
      "|    total_timesteps    | 2840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.445   |\n",
      "|    explained_variance | -0.183   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28399    |\n",
      "|    policy_loss        | -2.02    |\n",
      "|    value_loss         | 411      |\n",
      "------------------------------------\n",
      "Num timesteps: 2841000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.74\n",
      "Num timesteps: 2842000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.68\n",
      "Num timesteps: 2843000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.95\n",
      "Num timesteps: 2844000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.14\n",
      "Num timesteps: 2845000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.50\n",
      "Num timesteps: 2846000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.41\n",
      "Num timesteps: 2847000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 148.00\n",
      "Num timesteps: 2848000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.51\n",
      "Num timesteps: 2849000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2850000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1230     |\n",
      "|    iterations         | 28500    |\n",
      "|    time_elapsed       | 2315     |\n",
      "|    total_timesteps    | 2850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | -6.09    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28499    |\n",
      "|    policy_loss        | 22.3     |\n",
      "|    value_loss         | 1.5e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 2851000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.55\n",
      "Num timesteps: 2852000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.74\n",
      "Num timesteps: 2853000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.46\n",
      "Num timesteps: 2854000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.09\n",
      "Num timesteps: 2855000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 140.26\n",
      "Num timesteps: 2856000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.91\n",
      "Num timesteps: 2857000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.54\n",
      "Num timesteps: 2858000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.06\n",
      "Num timesteps: 2859000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.84\n",
      "Num timesteps: 2860000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 303      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1231     |\n",
      "|    iterations         | 28600    |\n",
      "|    time_elapsed       | 2321     |\n",
      "|    total_timesteps    | 2860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.628   |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28599    |\n",
      "|    policy_loss        | -2.27    |\n",
      "|    value_loss         | 57.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 2861000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 131.54\n",
      "Num timesteps: 2862000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.24\n",
      "Num timesteps: 2863000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.23\n",
      "Num timesteps: 2864000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.23\n",
      "Num timesteps: 2865000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.17\n",
      "Num timesteps: 2866000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.46\n",
      "Num timesteps: 2867000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.93\n",
      "Num timesteps: 2868000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.43\n",
      "Num timesteps: 2869000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.70\n",
      "Num timesteps: 2870000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 281      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1233     |\n",
      "|    iterations         | 28700    |\n",
      "|    time_elapsed       | 2327     |\n",
      "|    total_timesteps    | 2870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.991   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28699    |\n",
      "|    policy_loss        | 2.06     |\n",
      "|    value_loss         | 22.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 2871000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 134.54\n",
      "Num timesteps: 2872000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 132.40\n",
      "Num timesteps: 2873000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.72\n",
      "Num timesteps: 2874000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 127.57\n",
      "Num timesteps: 2875000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.30\n",
      "Num timesteps: 2876000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.17\n",
      "Num timesteps: 2877000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 124.18\n",
      "Num timesteps: 2878000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 123.47\n",
      "Num timesteps: 2879000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 128.59\n",
      "Num timesteps: 2880000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1234     |\n",
      "|    iterations         | 28800    |\n",
      "|    time_elapsed       | 2333     |\n",
      "|    total_timesteps    | 2880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | -37.4    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28799    |\n",
      "|    policy_loss        | 4.19     |\n",
      "|    value_loss         | 711      |\n",
      "------------------------------------\n",
      "Num timesteps: 2881000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.95\n",
      "Num timesteps: 2882000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 136.27\n",
      "Num timesteps: 2883000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 133.50\n",
      "Num timesteps: 2884000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.72\n",
      "Num timesteps: 2885000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 138.62\n",
      "Num timesteps: 2886000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 137.86\n",
      "Num timesteps: 2887000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 135.56\n",
      "Num timesteps: 2888000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 139.64\n",
      "Num timesteps: 2889000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 142.21\n",
      "Num timesteps: 2890000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1234     |\n",
      "|    iterations         | 28900    |\n",
      "|    time_elapsed       | 2340     |\n",
      "|    total_timesteps    | 2890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | -10.5    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28899    |\n",
      "|    policy_loss        | 19.2     |\n",
      "|    value_loss         | 918      |\n",
      "------------------------------------\n",
      "Num timesteps: 2891000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.70\n",
      "Num timesteps: 2892000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 143.55\n",
      "Num timesteps: 2893000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 144.10\n",
      "Num timesteps: 2894000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 145.93\n",
      "Num timesteps: 2895000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 149.50\n",
      "Num timesteps: 2896000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 151.87\n",
      "Num timesteps: 2897000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.22\n",
      "Num timesteps: 2898000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 150.96\n",
      "Num timesteps: 2899000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 146.95\n",
      "Num timesteps: 2900000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 147.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 279      |\n",
      "|    ep_rew_mean        | 147      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1236     |\n",
      "|    iterations         | 29000    |\n",
      "|    time_elapsed       | 2346     |\n",
      "|    total_timesteps    | 2900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.925   |\n",
      "|    explained_variance | 0.323    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 28999    |\n",
      "|    policy_loss        | -2.7     |\n",
      "|    value_loss         | 16.1     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2901000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.17\n",
      "Num timesteps: 2902000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.59\n",
      "Num timesteps: 2903000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 154.11\n",
      "Num timesteps: 2904000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.42\n",
      "Num timesteps: 2905000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 159.64\n",
      "Num timesteps: 2906000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.45\n",
      "Num timesteps: 2907000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 153.86\n",
      "Num timesteps: 2908000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.45\n",
      "Num timesteps: 2909000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 152.65\n",
      "Num timesteps: 2910000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 264      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1237     |\n",
      "|    iterations         | 29100    |\n",
      "|    time_elapsed       | 2351     |\n",
      "|    total_timesteps    | 2910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | -1.14    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29099    |\n",
      "|    policy_loss        | 39.4     |\n",
      "|    value_loss         | 2.21e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2911000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.68\n",
      "Num timesteps: 2912000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 163.00\n",
      "Num timesteps: 2913000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.76\n",
      "Num timesteps: 2914000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 158.67\n",
      "Num timesteps: 2915000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 159.81\n",
      "Num timesteps: 2916000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 162.80\n",
      "Num timesteps: 2917000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.34\n",
      "Num timesteps: 2918000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 164.19\n",
      "Num timesteps: 2919000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 163.75\n",
      "Num timesteps: 2920000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 163.78\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 260      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1238     |\n",
      "|    iterations         | 29200    |\n",
      "|    time_elapsed       | 2357     |\n",
      "|    total_timesteps    | 2920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.932   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29199    |\n",
      "|    policy_loss        | 1.54     |\n",
      "|    value_loss         | 22.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 2921000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.58\n",
      "Num timesteps: 2922000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 162.03\n",
      "Num timesteps: 2923000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 2924000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 157.17\n",
      "Num timesteps: 2925000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 2926000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 164.16\n",
      "Num timesteps: 2927000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 162.44\n",
      "Num timesteps: 2928000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 162.19\n",
      "Num timesteps: 2929000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 167.06\n",
      "Num timesteps: 2930000\n",
      "Best mean reward: 167.40 - Last mean reward per episode: 167.54\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 260      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1240     |\n",
      "|    iterations         | 29300    |\n",
      "|    time_elapsed       | 2362     |\n",
      "|    total_timesteps    | 2930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.298   |\n",
      "|    explained_variance | -2.08    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29299    |\n",
      "|    policy_loss        | 1.24     |\n",
      "|    value_loss         | 1.1e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 2931000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 160.03\n",
      "Num timesteps: 2932000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 159.02\n",
      "Num timesteps: 2933000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 161.82\n",
      "Num timesteps: 2934000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 161.53\n",
      "Num timesteps: 2935000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 164.10\n",
      "Num timesteps: 2936000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 2937000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 159.81\n",
      "Num timesteps: 2938000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 152.74\n",
      "Num timesteps: 2939000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 150.23\n",
      "Num timesteps: 2940000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 150.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 261      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1241     |\n",
      "|    iterations         | 29400    |\n",
      "|    time_elapsed       | 2368     |\n",
      "|    total_timesteps    | 2940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | -4.74    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29399    |\n",
      "|    policy_loss        | 3.05     |\n",
      "|    value_loss         | 1.33e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 2941000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 152.72\n",
      "Num timesteps: 2942000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 149.25\n",
      "Num timesteps: 2943000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 142.64\n",
      "Num timesteps: 2944000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 141.61\n",
      "Num timesteps: 2945000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 145.98\n",
      "Num timesteps: 2946000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 138.00\n",
      "Num timesteps: 2947000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 137.11\n",
      "Num timesteps: 2948000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.84\n",
      "Num timesteps: 2949000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 128.95\n",
      "Num timesteps: 2950000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 135.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 263      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1242     |\n",
      "|    iterations         | 29500    |\n",
      "|    time_elapsed       | 2373     |\n",
      "|    total_timesteps    | 2950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.228   |\n",
      "|    explained_variance | -0.179   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29499    |\n",
      "|    policy_loss        | -1.89    |\n",
      "|    value_loss         | 546      |\n",
      "------------------------------------\n",
      "Num timesteps: 2951000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 137.70\n",
      "Num timesteps: 2952000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.61\n",
      "Num timesteps: 2953000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 135.67\n",
      "Num timesteps: 2954000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.62\n",
      "Num timesteps: 2955000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 131.74\n",
      "Num timesteps: 2956000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 125.26\n",
      "Num timesteps: 2957000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 127.83\n",
      "Num timesteps: 2958000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 127.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 2959000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 127.61\n",
      "Num timesteps: 2960000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 127.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 262      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1244     |\n",
      "|    iterations         | 29600    |\n",
      "|    time_elapsed       | 2379     |\n",
      "|    total_timesteps    | 2960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0.84     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29599    |\n",
      "|    policy_loss        | 0.631    |\n",
      "|    value_loss         | 55.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 2961000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 129.36\n",
      "Num timesteps: 2962000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.81\n",
      "Num timesteps: 2963000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 135.90\n",
      "Num timesteps: 2964000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 132.05\n",
      "Num timesteps: 2965000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 134.03\n",
      "Num timesteps: 2966000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 140.51\n",
      "Num timesteps: 2967000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 140.70\n",
      "Num timesteps: 2968000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 138.17\n",
      "Num timesteps: 2969000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.30\n",
      "Num timesteps: 2970000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 138.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 275      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1245     |\n",
      "|    iterations         | 29700    |\n",
      "|    time_elapsed       | 2385     |\n",
      "|    total_timesteps    | 2970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29699    |\n",
      "|    policy_loss        | -1.06    |\n",
      "|    value_loss         | 7.25     |\n",
      "------------------------------------\n",
      "Num timesteps: 2971000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 134.33\n",
      "Num timesteps: 2972000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 134.68\n",
      "Num timesteps: 2973000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 137.65\n",
      "Num timesteps: 2974000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 135.11\n",
      "Num timesteps: 2975000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 135.57\n",
      "Num timesteps: 2976000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 136.84\n",
      "Num timesteps: 2977000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 138.44\n",
      "Num timesteps: 2978000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 136.73\n",
      "Num timesteps: 2979000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 133.97\n",
      "Num timesteps: 2980000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 138.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1246     |\n",
      "|    iterations         | 29800    |\n",
      "|    time_elapsed       | 2391     |\n",
      "|    total_timesteps    | 2980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | -1.77    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29799    |\n",
      "|    policy_loss        | -0.952   |\n",
      "|    value_loss         | 841      |\n",
      "------------------------------------\n",
      "Num timesteps: 2981000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 2982000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 143.02\n",
      "Num timesteps: 2983000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 147.48\n",
      "Num timesteps: 2984000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 146.83\n",
      "Num timesteps: 2985000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 153.01\n",
      "Num timesteps: 2986000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 156.18\n",
      "Num timesteps: 2987000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 153.51\n",
      "Num timesteps: 2988000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 153.25\n",
      "Num timesteps: 2989000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 155.61\n",
      "Num timesteps: 2990000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 145.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1247     |\n",
      "|    iterations         | 29900    |\n",
      "|    time_elapsed       | 2397     |\n",
      "|    total_timesteps    | 2990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.801   |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29899    |\n",
      "|    policy_loss        | -1.31    |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 2991000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 145.66\n",
      "Num timesteps: 2992000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 147.54\n",
      "Num timesteps: 2993000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 144.89\n",
      "Num timesteps: 2994000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 148.19\n",
      "Num timesteps: 2995000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 148.15\n",
      "Num timesteps: 2996000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 151.16\n",
      "Num timesteps: 2997000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 153.40\n",
      "Num timesteps: 2998000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 156.17\n",
      "Num timesteps: 2999000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 161.29\n",
      "Num timesteps: 3000000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 162.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 305      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1248     |\n",
      "|    iterations         | 30000    |\n",
      "|    time_elapsed       | 2403     |\n",
      "|    total_timesteps    | 3000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.524    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 29999    |\n",
      "|    policy_loss        | -8.08    |\n",
      "|    value_loss         | 71.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3001000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 165.67\n",
      "Num timesteps: 3002000\n",
      "Best mean reward: 167.54 - Last mean reward per episode: 168.58\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3003000\n",
      "Best mean reward: 168.58 - Last mean reward per episode: 165.19\n",
      "Num timesteps: 3004000\n",
      "Best mean reward: 168.58 - Last mean reward per episode: 162.64\n",
      "Num timesteps: 3005000\n",
      "Best mean reward: 168.58 - Last mean reward per episode: 170.36\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3006000\n",
      "Best mean reward: 170.36 - Last mean reward per episode: 168.65\n",
      "Num timesteps: 3007000\n",
      "Best mean reward: 170.36 - Last mean reward per episode: 171.84\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3008000\n",
      "Best mean reward: 171.84 - Last mean reward per episode: 172.90\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3009000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 169.75\n",
      "Num timesteps: 3010000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 301      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1248     |\n",
      "|    iterations         | 30100    |\n",
      "|    time_elapsed       | 2410     |\n",
      "|    total_timesteps    | 3010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.614   |\n",
      "|    explained_variance | 0.52     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30099    |\n",
      "|    policy_loss        | -1.1     |\n",
      "|    value_loss         | 659      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3011000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.16\n",
      "Num timesteps: 3012000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 162.75\n",
      "Num timesteps: 3013000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.00\n",
      "Num timesteps: 3014000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.41\n",
      "Num timesteps: 3015000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.39\n",
      "Num timesteps: 3016000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.63\n",
      "Num timesteps: 3017000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.75\n",
      "Num timesteps: 3018000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.18\n",
      "Num timesteps: 3019000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.38\n",
      "Num timesteps: 3020000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.91\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 303      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1249     |\n",
      "|    iterations         | 30200    |\n",
      "|    time_elapsed       | 2416     |\n",
      "|    total_timesteps    | 3020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.71    |\n",
      "|    explained_variance | 0.558    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30199    |\n",
      "|    policy_loss        | -63.2    |\n",
      "|    value_loss         | 9.19e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3021000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.55\n",
      "Num timesteps: 3022000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.36\n",
      "Num timesteps: 3023000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.87\n",
      "Num timesteps: 3024000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.49\n",
      "Num timesteps: 3025000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.66\n",
      "Num timesteps: 3026000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.76\n",
      "Num timesteps: 3027000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.74\n",
      "Num timesteps: 3028000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.96\n",
      "Num timesteps: 3029000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.95\n",
      "Num timesteps: 3030000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 301      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1250     |\n",
      "|    iterations         | 30300    |\n",
      "|    time_elapsed       | 2422     |\n",
      "|    total_timesteps    | 3030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.716   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30299    |\n",
      "|    policy_loss        | -0.376   |\n",
      "|    value_loss         | 5.74     |\n",
      "------------------------------------\n",
      "Num timesteps: 3031000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.02\n",
      "Num timesteps: 3032000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.97\n",
      "Num timesteps: 3033000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.55\n",
      "Num timesteps: 3034000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.18\n",
      "Num timesteps: 3035000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.19\n",
      "Num timesteps: 3036000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.94\n",
      "Num timesteps: 3037000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.81\n",
      "Num timesteps: 3038000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.75\n",
      "Num timesteps: 3039000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.53\n",
      "Num timesteps: 3040000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 124.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 319      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1251     |\n",
      "|    iterations         | 30400    |\n",
      "|    time_elapsed       | 2429     |\n",
      "|    total_timesteps    | 3040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30399    |\n",
      "|    policy_loss        | 3.41     |\n",
      "|    value_loss         | 30.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 3041000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.29\n",
      "Num timesteps: 3042000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.32\n",
      "Num timesteps: 3043000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.23\n",
      "Num timesteps: 3044000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.67\n",
      "Num timesteps: 3045000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.27\n",
      "Num timesteps: 3046000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.93\n",
      "Num timesteps: 3047000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.79\n",
      "Num timesteps: 3048000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.19\n",
      "Num timesteps: 3049000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.46\n",
      "Num timesteps: 3050000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 339      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1251     |\n",
      "|    iterations         | 30500    |\n",
      "|    time_elapsed       | 2436     |\n",
      "|    total_timesteps    | 3050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0.438    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30499    |\n",
      "|    policy_loss        | -41.2    |\n",
      "|    value_loss         | 6.42e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3051000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.48\n",
      "Num timesteps: 3052000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.42\n",
      "Num timesteps: 3053000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.57\n",
      "Num timesteps: 3054000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.93\n",
      "Num timesteps: 3055000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.51\n",
      "Num timesteps: 3056000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.49\n",
      "Num timesteps: 3057000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.69\n",
      "Num timesteps: 3058000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.43\n",
      "Num timesteps: 3059000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.11\n",
      "Num timesteps: 3060000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.47\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 333      |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1252     |\n",
      "|    iterations         | 30600    |\n",
      "|    time_elapsed       | 2443     |\n",
      "|    total_timesteps    | 3060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.373   |\n",
      "|    explained_variance | 0.766    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30599    |\n",
      "|    policy_loss        | -2.35    |\n",
      "|    value_loss         | 93.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 3061000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.46\n",
      "Num timesteps: 3062000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.01\n",
      "Num timesteps: 3063000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.14\n",
      "Num timesteps: 3064000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.60\n",
      "Num timesteps: 3065000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.70\n",
      "Num timesteps: 3066000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.40\n",
      "Num timesteps: 3067000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.29\n",
      "Num timesteps: 3068000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.33\n",
      "Num timesteps: 3069000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3070000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1253     |\n",
      "|    iterations         | 30700    |\n",
      "|    time_elapsed       | 2449     |\n",
      "|    total_timesteps    | 3070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.959   |\n",
      "|    explained_variance | 0.0607   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30699    |\n",
      "|    policy_loss        | -5.64    |\n",
      "|    value_loss         | 43.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 3071000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.01\n",
      "Num timesteps: 3072000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.24\n",
      "Num timesteps: 3073000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.44\n",
      "Num timesteps: 3074000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.27\n",
      "Num timesteps: 3075000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.06\n",
      "Num timesteps: 3076000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.02\n",
      "Num timesteps: 3077000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.15\n",
      "Num timesteps: 3078000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.48\n",
      "Num timesteps: 3079000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.51\n",
      "Num timesteps: 3080000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1253     |\n",
      "|    iterations         | 30800    |\n",
      "|    time_elapsed       | 2457     |\n",
      "|    total_timesteps    | 3080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.522   |\n",
      "|    explained_variance | -5.97    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30799    |\n",
      "|    policy_loss        | 21.4     |\n",
      "|    value_loss         | 1.02e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3081000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 3082000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.51\n",
      "Num timesteps: 3083000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.83\n",
      "Num timesteps: 3084000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.82\n",
      "Num timesteps: 3085000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.38\n",
      "Num timesteps: 3086000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.93\n",
      "Num timesteps: 3087000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.19\n",
      "Num timesteps: 3088000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.75\n",
      "Num timesteps: 3089000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.35\n",
      "Num timesteps: 3090000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 332      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1254     |\n",
      "|    iterations         | 30900    |\n",
      "|    time_elapsed       | 2463     |\n",
      "|    total_timesteps    | 3090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.904   |\n",
      "|    explained_variance | 0.404    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30899    |\n",
      "|    policy_loss        | -1.25    |\n",
      "|    value_loss         | 16.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 3091000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.74\n",
      "Num timesteps: 3092000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.38\n",
      "Num timesteps: 3093000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.08\n",
      "Num timesteps: 3094000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.57\n",
      "Num timesteps: 3095000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.64\n",
      "Num timesteps: 3096000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.85\n",
      "Num timesteps: 3097000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.73\n",
      "Num timesteps: 3098000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 162.15\n",
      "Num timesteps: 3099000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.34\n",
      "Num timesteps: 3100000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 324      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1254     |\n",
      "|    iterations         | 31000    |\n",
      "|    time_elapsed       | 2470     |\n",
      "|    total_timesteps    | 3100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.49    |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 30999    |\n",
      "|    policy_loss        | -0.805   |\n",
      "|    value_loss         | 38.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3101000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 166.19\n",
      "Num timesteps: 3102000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 167.01\n",
      "Num timesteps: 3103000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 170.28\n",
      "Num timesteps: 3104000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 168.94\n",
      "Num timesteps: 3105000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 167.91\n",
      "Num timesteps: 3106000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 166.87\n",
      "Num timesteps: 3107000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.66\n",
      "Num timesteps: 3108000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.17\n",
      "Num timesteps: 3109000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.46\n",
      "Num timesteps: 3110000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 317      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1255     |\n",
      "|    iterations         | 31100    |\n",
      "|    time_elapsed       | 2476     |\n",
      "|    total_timesteps    | 3110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0.00113  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31099    |\n",
      "|    policy_loss        | -5.36    |\n",
      "|    value_loss         | 192      |\n",
      "------------------------------------\n",
      "Num timesteps: 3111000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.24\n",
      "Num timesteps: 3112000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.21\n",
      "Num timesteps: 3113000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 162.89\n",
      "Num timesteps: 3114000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.41\n",
      "Num timesteps: 3115000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.23\n",
      "Num timesteps: 3116000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.97\n",
      "Num timesteps: 3117000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 171.40\n",
      "Num timesteps: 3118000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 168.56\n",
      "Num timesteps: 3119000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 165.17\n",
      "Num timesteps: 3120000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 166.15\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1256     |\n",
      "|    iterations         | 31200    |\n",
      "|    time_elapsed       | 2482     |\n",
      "|    total_timesteps    | 3120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.881   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31199    |\n",
      "|    policy_loss        | -3.07    |\n",
      "|    value_loss         | 14       |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3121000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 162.92\n",
      "Num timesteps: 3122000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.18\n",
      "Num timesteps: 3123000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.00\n",
      "Num timesteps: 3124000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.42\n",
      "Num timesteps: 3125000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.80\n",
      "Num timesteps: 3126000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.07\n",
      "Num timesteps: 3127000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.61\n",
      "Num timesteps: 3128000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.27\n",
      "Num timesteps: 3129000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.05\n",
      "Num timesteps: 3130000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1257     |\n",
      "|    iterations         | 31300    |\n",
      "|    time_elapsed       | 2489     |\n",
      "|    total_timesteps    | 3130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.604   |\n",
      "|    explained_variance | 0.838    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31299    |\n",
      "|    policy_loss        | 0.649    |\n",
      "|    value_loss         | 35.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 3131000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.81\n",
      "Num timesteps: 3132000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.64\n",
      "Num timesteps: 3133000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.71\n",
      "Num timesteps: 3134000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.24\n",
      "Num timesteps: 3135000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.51\n",
      "Num timesteps: 3136000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.02\n",
      "Num timesteps: 3137000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.57\n",
      "Num timesteps: 3138000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.03\n",
      "Num timesteps: 3139000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.65\n",
      "Num timesteps: 3140000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 294      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1258     |\n",
      "|    iterations         | 31400    |\n",
      "|    time_elapsed       | 2495     |\n",
      "|    total_timesteps    | 3140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.712   |\n",
      "|    explained_variance | 0.93     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31399    |\n",
      "|    policy_loss        | -2.38    |\n",
      "|    value_loss         | 27.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 3141000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.17\n",
      "Num timesteps: 3142000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.28\n",
      "Num timesteps: 3143000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.27\n",
      "Num timesteps: 3144000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.80\n",
      "Num timesteps: 3145000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.30\n",
      "Num timesteps: 3146000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 121.41\n",
      "Num timesteps: 3147000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 119.63\n",
      "Num timesteps: 3148000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 122.54\n",
      "Num timesteps: 3149000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 125.57\n",
      "Num timesteps: 3150000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 124.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 308      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1258     |\n",
      "|    iterations         | 31500    |\n",
      "|    time_elapsed       | 2502     |\n",
      "|    total_timesteps    | 3150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.797   |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31499    |\n",
      "|    policy_loss        | -2.39    |\n",
      "|    value_loss         | 32.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 3151000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 120.16\n",
      "Num timesteps: 3152000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.24\n",
      "Num timesteps: 3153000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.60\n",
      "Num timesteps: 3154000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 122.45\n",
      "Num timesteps: 3155000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 119.82\n",
      "Num timesteps: 3156000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.15\n",
      "Num timesteps: 3157000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 119.99\n",
      "Num timesteps: 3158000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.36\n",
      "Num timesteps: 3159000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.28\n",
      "Num timesteps: 3160000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 120.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1259     |\n",
      "|    iterations         | 31600    |\n",
      "|    time_elapsed       | 2508     |\n",
      "|    total_timesteps    | 3160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.76    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31599    |\n",
      "|    policy_loss        | -0.422   |\n",
      "|    value_loss         | 39.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 3161000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.00\n",
      "Num timesteps: 3162000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 119.68\n",
      "Num timesteps: 3163000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 120.22\n",
      "Num timesteps: 3164000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 122.53\n",
      "Num timesteps: 3165000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 122.85\n",
      "Num timesteps: 3166000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.09\n",
      "Num timesteps: 3167000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 125.99\n",
      "Num timesteps: 3168000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.45\n",
      "Num timesteps: 3169000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.13\n",
      "Num timesteps: 3170000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 317      |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1260     |\n",
      "|    iterations         | 31700    |\n",
      "|    time_elapsed       | 2514     |\n",
      "|    total_timesteps    | 3170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.699   |\n",
      "|    explained_variance | 0.271    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31699    |\n",
      "|    policy_loss        | -0.507   |\n",
      "|    value_loss         | 233      |\n",
      "------------------------------------\n",
      "Num timesteps: 3171000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.21\n",
      "Num timesteps: 3172000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.74\n",
      "Num timesteps: 3173000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.86\n",
      "Num timesteps: 3174000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.40\n",
      "Num timesteps: 3175000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.41\n",
      "Num timesteps: 3176000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.74\n",
      "Num timesteps: 3177000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.34\n",
      "Num timesteps: 3178000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.07\n",
      "Num timesteps: 3179000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3180000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 302      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1261     |\n",
      "|    iterations         | 31800    |\n",
      "|    time_elapsed       | 2520     |\n",
      "|    total_timesteps    | 3180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.878   |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31799    |\n",
      "|    policy_loss        | -4.71    |\n",
      "|    value_loss         | 28.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3181000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.26\n",
      "Num timesteps: 3182000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.35\n",
      "Num timesteps: 3183000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.75\n",
      "Num timesteps: 3184000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.97\n",
      "Num timesteps: 3185000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.49\n",
      "Num timesteps: 3186000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.41\n",
      "Num timesteps: 3187000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.50\n",
      "Num timesteps: 3188000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.27\n",
      "Num timesteps: 3189000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.85\n",
      "Num timesteps: 3190000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1261     |\n",
      "|    iterations         | 31900    |\n",
      "|    time_elapsed       | 2527     |\n",
      "|    total_timesteps    | 3190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.735   |\n",
      "|    explained_variance | -1.7     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31899    |\n",
      "|    policy_loss        | 29.5     |\n",
      "|    value_loss         | 1.81e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3191000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.15\n",
      "Num timesteps: 3192000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.50\n",
      "Num timesteps: 3193000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.07\n",
      "Num timesteps: 3194000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.75\n",
      "Num timesteps: 3195000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.77\n",
      "Num timesteps: 3196000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.22\n",
      "Num timesteps: 3197000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.29\n",
      "Num timesteps: 3198000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.86\n",
      "Num timesteps: 3199000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.34\n",
      "Num timesteps: 3200000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 325      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1262     |\n",
      "|    iterations         | 32000    |\n",
      "|    time_elapsed       | 2535     |\n",
      "|    total_timesteps    | 3200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.685   |\n",
      "|    explained_variance | -3.21    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 31999    |\n",
      "|    policy_loss        | 22.4     |\n",
      "|    value_loss         | 1e+03    |\n",
      "------------------------------------\n",
      "Num timesteps: 3201000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.69\n",
      "Num timesteps: 3202000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.66\n",
      "Num timesteps: 3203000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.93\n",
      "Num timesteps: 3204000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.66\n",
      "Num timesteps: 3205000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.19\n",
      "Num timesteps: 3206000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.64\n",
      "Num timesteps: 3207000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.59\n",
      "Num timesteps: 3208000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.04\n",
      "Num timesteps: 3209000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 117.64\n",
      "Num timesteps: 3210000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 118.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 340      |\n",
      "|    ep_rew_mean        | 119      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1262     |\n",
      "|    iterations         | 32100    |\n",
      "|    time_elapsed       | 2542     |\n",
      "|    total_timesteps    | 3210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.943   |\n",
      "|    explained_variance | 0.915    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32099    |\n",
      "|    policy_loss        | -2.67    |\n",
      "|    value_loss         | 25.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3211000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 115.48\n",
      "Num timesteps: 3212000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.71\n",
      "Num timesteps: 3213000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 121.12\n",
      "Num timesteps: 3214000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.84\n",
      "Num timesteps: 3215000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 123.48\n",
      "Num timesteps: 3216000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.31\n",
      "Num timesteps: 3217000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.30\n",
      "Num timesteps: 3218000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.03\n",
      "Num timesteps: 3219000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.04\n",
      "Num timesteps: 3220000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 127.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1263     |\n",
      "|    iterations         | 32200    |\n",
      "|    time_elapsed       | 2549     |\n",
      "|    total_timesteps    | 3220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | 0.0324   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32199    |\n",
      "|    policy_loss        | -37.7    |\n",
      "|    value_loss         | 5.76e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3221000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.99\n",
      "Num timesteps: 3222000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 121.76\n",
      "Num timesteps: 3223000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 117.34\n",
      "Num timesteps: 3224000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 114.77\n",
      "Num timesteps: 3225000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 115.83\n",
      "Num timesteps: 3226000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 121.21\n",
      "Num timesteps: 3227000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 122.60\n",
      "Num timesteps: 3228000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 131.14\n",
      "Num timesteps: 3229000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.54\n",
      "Num timesteps: 3230000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.63\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 300      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1264     |\n",
      "|    iterations         | 32300    |\n",
      "|    time_elapsed       | 2554     |\n",
      "|    total_timesteps    | 3230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.883   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32299    |\n",
      "|    policy_loss        | -1.1     |\n",
      "|    value_loss         | 17.5     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3231000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.66\n",
      "Num timesteps: 3232000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.42\n",
      "Num timesteps: 3233000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.30\n",
      "Num timesteps: 3234000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.04\n",
      "Num timesteps: 3235000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.95\n",
      "Num timesteps: 3236000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.89\n",
      "Num timesteps: 3237000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.60\n",
      "Num timesteps: 3238000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 135.87\n",
      "Num timesteps: 3239000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.44\n",
      "Num timesteps: 3240000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 262      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1265     |\n",
      "|    iterations         | 32400    |\n",
      "|    time_elapsed       | 2559     |\n",
      "|    total_timesteps    | 3240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1       |\n",
      "|    explained_variance | -0.0767  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32399    |\n",
      "|    policy_loss        | -3.77    |\n",
      "|    value_loss         | 17.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3241000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.70\n",
      "Num timesteps: 3242000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 133.76\n",
      "Num timesteps: 3243000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 125.95\n",
      "Num timesteps: 3244000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.98\n",
      "Num timesteps: 3245000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 128.51\n",
      "Num timesteps: 3246000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 126.53\n",
      "Num timesteps: 3247000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.46\n",
      "Num timesteps: 3248000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.90\n",
      "Num timesteps: 3249000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.95\n",
      "Num timesteps: 3250000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 243      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1266     |\n",
      "|    iterations         | 32500    |\n",
      "|    time_elapsed       | 2565     |\n",
      "|    total_timesteps    | 3250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.201   |\n",
      "|    explained_variance | -0.25    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32499    |\n",
      "|    policy_loss        | -0.277   |\n",
      "|    value_loss         | 317      |\n",
      "------------------------------------\n",
      "Num timesteps: 3251000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.54\n",
      "Num timesteps: 3252000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.87\n",
      "Num timesteps: 3253000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.91\n",
      "Num timesteps: 3254000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.66\n",
      "Num timesteps: 3255000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.70\n",
      "Num timesteps: 3256000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.38\n",
      "Num timesteps: 3257000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.17\n",
      "Num timesteps: 3258000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.94\n",
      "Num timesteps: 3259000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.92\n",
      "Num timesteps: 3260000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 246      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1268     |\n",
      "|    iterations         | 32600    |\n",
      "|    time_elapsed       | 2570     |\n",
      "|    total_timesteps    | 3260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32599    |\n",
      "|    policy_loss        | -2.26    |\n",
      "|    value_loss         | 8.62     |\n",
      "------------------------------------\n",
      "Num timesteps: 3261000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.30\n",
      "Num timesteps: 3262000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.54\n",
      "Num timesteps: 3263000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.23\n",
      "Num timesteps: 3264000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.44\n",
      "Num timesteps: 3265000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.72\n",
      "Num timesteps: 3266000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.22\n",
      "Num timesteps: 3267000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.94\n",
      "Num timesteps: 3268000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.18\n",
      "Num timesteps: 3269000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.68\n",
      "Num timesteps: 3270000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 255      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1269     |\n",
      "|    iterations         | 32700    |\n",
      "|    time_elapsed       | 2576     |\n",
      "|    total_timesteps    | 3270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.639   |\n",
      "|    explained_variance | 0.557    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32699    |\n",
      "|    policy_loss        | -33.3    |\n",
      "|    value_loss         | 4.46e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3271000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.37\n",
      "Num timesteps: 3272000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.29\n",
      "Num timesteps: 3273000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.13\n",
      "Num timesteps: 3274000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.83\n",
      "Num timesteps: 3275000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.77\n",
      "Num timesteps: 3276000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.95\n",
      "Num timesteps: 3277000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.14\n",
      "Num timesteps: 3278000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.75\n",
      "Num timesteps: 3279000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.60\n",
      "Num timesteps: 3280000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 263      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1270     |\n",
      "|    iterations         | 32800    |\n",
      "|    time_elapsed       | 2581     |\n",
      "|    total_timesteps    | 3280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0557  |\n",
      "|    explained_variance | -0.0107  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32799    |\n",
      "|    policy_loss        | 1.55     |\n",
      "|    value_loss         | 380      |\n",
      "------------------------------------\n",
      "Num timesteps: 3281000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.75\n",
      "Num timesteps: 3282000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.56\n",
      "Num timesteps: 3283000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.89\n",
      "Num timesteps: 3284000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.73\n",
      "Num timesteps: 3285000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.48\n",
      "Num timesteps: 3286000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.36\n",
      "Num timesteps: 3287000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.97\n",
      "Num timesteps: 3288000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.76\n",
      "Num timesteps: 3289000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3290000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 256      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1271     |\n",
      "|    iterations         | 32900    |\n",
      "|    time_elapsed       | 2587     |\n",
      "|    total_timesteps    | 3290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.901   |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32899    |\n",
      "|    policy_loss        | -2.13    |\n",
      "|    value_loss         | 9.8      |\n",
      "------------------------------------\n",
      "Num timesteps: 3291000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.97\n",
      "Num timesteps: 3292000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.46\n",
      "Num timesteps: 3293000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.59\n",
      "Num timesteps: 3294000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.24\n",
      "Num timesteps: 3295000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.16\n",
      "Num timesteps: 3296000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.87\n",
      "Num timesteps: 3297000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.98\n",
      "Num timesteps: 3298000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.24\n",
      "Num timesteps: 3299000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.78\n",
      "Num timesteps: 3300000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 266      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1272     |\n",
      "|    iterations         | 33000    |\n",
      "|    time_elapsed       | 2593     |\n",
      "|    total_timesteps    | 3300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.38    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 32999    |\n",
      "|    policy_loss        | 0.757    |\n",
      "|    value_loss         | 51       |\n",
      "------------------------------------\n",
      "Num timesteps: 3301000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.14\n",
      "Num timesteps: 3302000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.20\n",
      "Num timesteps: 3303000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.65\n",
      "Num timesteps: 3304000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 134.49\n",
      "Num timesteps: 3305000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.64\n",
      "Num timesteps: 3306000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.43\n",
      "Num timesteps: 3307000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.62\n",
      "Num timesteps: 3308000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.74\n",
      "Num timesteps: 3309000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.75\n",
      "Num timesteps: 3310000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 261      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1273     |\n",
      "|    iterations         | 33100    |\n",
      "|    time_elapsed       | 2598     |\n",
      "|    total_timesteps    | 3310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.629   |\n",
      "|    explained_variance | -11.9    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33099    |\n",
      "|    policy_loss        | 20.9     |\n",
      "|    value_loss         | 1.08e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3311000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 132.30\n",
      "Num timesteps: 3312000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.19\n",
      "Num timesteps: 3313000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.92\n",
      "Num timesteps: 3314000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.36\n",
      "Num timesteps: 3315000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.73\n",
      "Num timesteps: 3316000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.50\n",
      "Num timesteps: 3317000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.35\n",
      "Num timesteps: 3318000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.62\n",
      "Num timesteps: 3319000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.01\n",
      "Num timesteps: 3320000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 243      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1275     |\n",
      "|    iterations         | 33200    |\n",
      "|    time_elapsed       | 2603     |\n",
      "|    total_timesteps    | 3320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.872   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33199    |\n",
      "|    policy_loss        | 0.245    |\n",
      "|    value_loss         | 9.5      |\n",
      "------------------------------------\n",
      "Num timesteps: 3321000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.07\n",
      "Num timesteps: 3322000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 129.56\n",
      "Num timesteps: 3323000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 130.91\n",
      "Num timesteps: 3324000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.22\n",
      "Num timesteps: 3325000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.67\n",
      "Num timesteps: 3326000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.30\n",
      "Num timesteps: 3327000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.20\n",
      "Num timesteps: 3328000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.68\n",
      "Num timesteps: 3329000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.93\n",
      "Num timesteps: 3330000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 246      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1276     |\n",
      "|    iterations         | 33300    |\n",
      "|    time_elapsed       | 2609     |\n",
      "|    total_timesteps    | 3330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.192   |\n",
      "|    explained_variance | -0.604   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33299    |\n",
      "|    policy_loss        | 2.56     |\n",
      "|    value_loss         | 792      |\n",
      "------------------------------------\n",
      "Num timesteps: 3331000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.23\n",
      "Num timesteps: 3332000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 142.16\n",
      "Num timesteps: 3333000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.78\n",
      "Num timesteps: 3334000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.22\n",
      "Num timesteps: 3335000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.64\n",
      "Num timesteps: 3336000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.11\n",
      "Num timesteps: 3337000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.18\n",
      "Num timesteps: 3338000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.64\n",
      "Num timesteps: 3339000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.32\n",
      "Num timesteps: 3340000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 242      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1277     |\n",
      "|    iterations         | 33400    |\n",
      "|    time_elapsed       | 2614     |\n",
      "|    total_timesteps    | 3340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.642   |\n",
      "|    explained_variance | 0.6      |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33399    |\n",
      "|    policy_loss        | -45.1    |\n",
      "|    value_loss         | 8.27e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3341000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.61\n",
      "Num timesteps: 3342000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.93\n",
      "Num timesteps: 3343000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.77\n",
      "Num timesteps: 3344000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.81\n",
      "Num timesteps: 3345000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.45\n",
      "Num timesteps: 3346000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.54\n",
      "Num timesteps: 3347000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 159.87\n",
      "Num timesteps: 3348000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.15\n",
      "Num timesteps: 3349000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 162.49\n",
      "Num timesteps: 3350000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 159.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 244      |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1278     |\n",
      "|    iterations         | 33500    |\n",
      "|    time_elapsed       | 2619     |\n",
      "|    total_timesteps    | 3350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.729   |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33499    |\n",
      "|    policy_loss        | -6.48    |\n",
      "|    value_loss         | 140      |\n",
      "------------------------------------\n",
      "Num timesteps: 3351000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.67\n",
      "Num timesteps: 3352000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.60\n",
      "Num timesteps: 3353000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.81\n",
      "Num timesteps: 3354000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.72\n",
      "Num timesteps: 3355000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.59\n",
      "Num timesteps: 3356000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.47\n",
      "Num timesteps: 3357000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.68\n",
      "Num timesteps: 3358000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.11\n",
      "Num timesteps: 3359000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.79\n",
      "Num timesteps: 3360000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 270      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1279     |\n",
      "|    iterations         | 33600    |\n",
      "|    time_elapsed       | 2626     |\n",
      "|    total_timesteps    | 3360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | -1.36    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33599    |\n",
      "|    policy_loss        | 29.9     |\n",
      "|    value_loss         | 1.17e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3361000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.31\n",
      "Num timesteps: 3362000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.25\n",
      "Num timesteps: 3363000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.48\n",
      "Num timesteps: 3364000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.42\n",
      "Num timesteps: 3365000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.20\n",
      "Num timesteps: 3366000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.52\n",
      "Num timesteps: 3367000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.06\n",
      "Num timesteps: 3368000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.35\n",
      "Num timesteps: 3369000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.05\n",
      "Num timesteps: 3370000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 274      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1280     |\n",
      "|    iterations         | 33700    |\n",
      "|    time_elapsed       | 2632     |\n",
      "|    total_timesteps    | 3370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.751   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33699    |\n",
      "|    policy_loss        | -0.566   |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3371000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.27\n",
      "Num timesteps: 3372000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.73\n",
      "Num timesteps: 3373000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.58\n",
      "Num timesteps: 3374000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.41\n",
      "Num timesteps: 3375000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.14\n",
      "Num timesteps: 3376000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.43\n",
      "Num timesteps: 3377000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.60\n",
      "Num timesteps: 3378000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.74\n",
      "Num timesteps: 3379000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.36\n",
      "Num timesteps: 3380000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 274      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1281     |\n",
      "|    iterations         | 33800    |\n",
      "|    time_elapsed       | 2637     |\n",
      "|    total_timesteps    | 3380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.642   |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33799    |\n",
      "|    policy_loss        | 2.18     |\n",
      "|    value_loss         | 35.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3381000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.71\n",
      "Num timesteps: 3382000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.40\n",
      "Num timesteps: 3383000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.44\n",
      "Num timesteps: 3384000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.91\n",
      "Num timesteps: 3385000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 147.92\n",
      "Num timesteps: 3386000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.35\n",
      "Num timesteps: 3387000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.84\n",
      "Num timesteps: 3388000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.00\n",
      "Num timesteps: 3389000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.69\n",
      "Num timesteps: 3390000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 273      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1282     |\n",
      "|    iterations         | 33900    |\n",
      "|    time_elapsed       | 2643     |\n",
      "|    total_timesteps    | 3390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.815   |\n",
      "|    explained_variance | 0.188    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33899    |\n",
      "|    policy_loss        | 8.6      |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "Num timesteps: 3391000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.89\n",
      "Num timesteps: 3392000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.71\n",
      "Num timesteps: 3393000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 144.46\n",
      "Num timesteps: 3394000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.66\n",
      "Num timesteps: 3395000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.17\n",
      "Num timesteps: 3396000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.57\n",
      "Num timesteps: 3397000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.62\n",
      "Num timesteps: 3398000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.79\n",
      "Num timesteps: 3399000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3400000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 272      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1283     |\n",
      "|    iterations         | 34000    |\n",
      "|    time_elapsed       | 2649     |\n",
      "|    total_timesteps    | 3400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.595   |\n",
      "|    explained_variance | 0.572    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 33999    |\n",
      "|    policy_loss        | 0.463    |\n",
      "|    value_loss         | 1.05e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3401000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 135.10\n",
      "Num timesteps: 3402000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 137.76\n",
      "Num timesteps: 3403000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.48\n",
      "Num timesteps: 3404000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 135.96\n",
      "Num timesteps: 3405000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 141.70\n",
      "Num timesteps: 3406000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 138.83\n",
      "Num timesteps: 3407000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.49\n",
      "Num timesteps: 3408000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 136.25\n",
      "Num timesteps: 3409000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 139.67\n",
      "Num timesteps: 3410000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 140.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 279      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 34100    |\n",
      "|    time_elapsed       | 2655     |\n",
      "|    total_timesteps    | 3410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.669   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34099    |\n",
      "|    policy_loss        | -0.252   |\n",
      "|    value_loss         | 14.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3411000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 145.07\n",
      "Num timesteps: 3412000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 143.74\n",
      "Num timesteps: 3413000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.88\n",
      "Num timesteps: 3414000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 148.08\n",
      "Num timesteps: 3415000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.10\n",
      "Num timesteps: 3416000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.43\n",
      "Num timesteps: 3417000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.67\n",
      "Num timesteps: 3418000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.31\n",
      "Num timesteps: 3419000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.49\n",
      "Num timesteps: 3420000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 278      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1285     |\n",
      "|    iterations         | 34200    |\n",
      "|    time_elapsed       | 2661     |\n",
      "|    total_timesteps    | 3420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.695   |\n",
      "|    explained_variance | 0.607    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34199    |\n",
      "|    policy_loss        | -28.5    |\n",
      "|    value_loss         | 3.78e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3421000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.70\n",
      "Num timesteps: 3422000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.71\n",
      "Num timesteps: 3423000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.55\n",
      "Num timesteps: 3424000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.08\n",
      "Num timesteps: 3425000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.84\n",
      "Num timesteps: 3426000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 152.47\n",
      "Num timesteps: 3427000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.48\n",
      "Num timesteps: 3428000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.88\n",
      "Num timesteps: 3429000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.31\n",
      "Num timesteps: 3430000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 273      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1286     |\n",
      "|    iterations         | 34300    |\n",
      "|    time_elapsed       | 2666     |\n",
      "|    total_timesteps    | 3430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | -0.182   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34299    |\n",
      "|    policy_loss        | 0.913    |\n",
      "|    value_loss         | 160      |\n",
      "------------------------------------\n",
      "Num timesteps: 3431000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.11\n",
      "Num timesteps: 3432000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.35\n",
      "Num timesteps: 3433000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.88\n",
      "Num timesteps: 3434000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 166.86\n",
      "Num timesteps: 3435000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.51\n",
      "Num timesteps: 3436000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 164.31\n",
      "Num timesteps: 3437000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.64\n",
      "Num timesteps: 3438000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 157.26\n",
      "Num timesteps: 3439000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.39\n",
      "Num timesteps: 3440000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 146.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 279      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 34400    |\n",
      "|    time_elapsed       | 2672     |\n",
      "|    total_timesteps    | 3440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.543   |\n",
      "|    explained_variance | 0.479    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34399    |\n",
      "|    policy_loss        | -26.1    |\n",
      "|    value_loss         | 4.66e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3441000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.74\n",
      "Num timesteps: 3442000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.00\n",
      "Num timesteps: 3443000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.04\n",
      "Num timesteps: 3444000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.33\n",
      "Num timesteps: 3445000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 149.56\n",
      "Num timesteps: 3446000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.89\n",
      "Num timesteps: 3447000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 151.24\n",
      "Num timesteps: 3448000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 154.00\n",
      "Num timesteps: 3449000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 161.45\n",
      "Num timesteps: 3450000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 34500    |\n",
      "|    time_elapsed       | 2678     |\n",
      "|    total_timesteps    | 3450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.369   |\n",
      "|    explained_variance | -5.83    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34499    |\n",
      "|    policy_loss        | 7.78     |\n",
      "|    value_loss         | 2.93e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3451000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.60\n",
      "Num timesteps: 3452000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.26\n",
      "Num timesteps: 3453000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.83\n",
      "Num timesteps: 3454000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 159.31\n",
      "Num timesteps: 3455000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 158.33\n",
      "Num timesteps: 3456000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.02\n",
      "Num timesteps: 3457000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 155.92\n",
      "Num timesteps: 3458000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.21\n",
      "Num timesteps: 3459000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 153.97\n",
      "Num timesteps: 3460000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 268      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 34600    |\n",
      "|    time_elapsed       | 2684     |\n",
      "|    total_timesteps    | 3460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | -2.9     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34599    |\n",
      "|    policy_loss        | 24.2     |\n",
      "|    value_loss         | 1.97e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3461000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 150.25\n",
      "Num timesteps: 3462000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 156.09\n",
      "Num timesteps: 3463000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 160.09\n",
      "Num timesteps: 3464000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.60\n",
      "Num timesteps: 3465000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 163.92\n",
      "Num timesteps: 3466000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 170.35\n",
      "Num timesteps: 3467000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 170.68\n",
      "Num timesteps: 3468000\n",
      "Best mean reward: 172.90 - Last mean reward per episode: 175.21\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3469000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 172.62\n",
      "Num timesteps: 3470000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 170.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 249      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1290     |\n",
      "|    iterations         | 34700    |\n",
      "|    time_elapsed       | 2689     |\n",
      "|    total_timesteps    | 3470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.813   |\n",
      "|    explained_variance | 0.122    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34699    |\n",
      "|    policy_loss        | -2.53    |\n",
      "|    value_loss         | 104      |\n",
      "------------------------------------\n",
      "Num timesteps: 3471000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 172.91\n",
      "Num timesteps: 3472000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 172.29\n",
      "Num timesteps: 3473000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 167.73\n",
      "Num timesteps: 3474000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 166.04\n",
      "Num timesteps: 3475000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 166.68\n",
      "Num timesteps: 3476000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 165.15\n",
      "Num timesteps: 3477000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 170.08\n",
      "Num timesteps: 3478000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 170.18\n",
      "Num timesteps: 3479000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 170.29\n",
      "Num timesteps: 3480000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 171.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 242      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1291     |\n",
      "|    iterations         | 34800    |\n",
      "|    time_elapsed       | 2694     |\n",
      "|    total_timesteps    | 3480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.89    |\n",
      "|    explained_variance | 0.483    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34799    |\n",
      "|    policy_loss        | -14.1    |\n",
      "|    value_loss         | 258      |\n",
      "------------------------------------\n",
      "Num timesteps: 3481000\n",
      "Best mean reward: 175.21 - Last mean reward per episode: 177.00\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3482000\n",
      "Best mean reward: 177.00 - Last mean reward per episode: 178.64\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3483000\n",
      "Best mean reward: 178.64 - Last mean reward per episode: 173.59\n",
      "Num timesteps: 3484000\n",
      "Best mean reward: 178.64 - Last mean reward per episode: 174.22\n",
      "Num timesteps: 3485000\n",
      "Best mean reward: 178.64 - Last mean reward per episode: 176.57\n",
      "Num timesteps: 3486000\n",
      "Best mean reward: 178.64 - Last mean reward per episode: 180.98\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3487000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 172.48\n",
      "Num timesteps: 3488000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.65\n",
      "Num timesteps: 3489000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.25\n",
      "Num timesteps: 3490000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 265      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1292     |\n",
      "|    iterations         | 34900    |\n",
      "|    time_elapsed       | 2700     |\n",
      "|    total_timesteps    | 3490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.753   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34899    |\n",
      "|    policy_loss        | -2.42    |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 3491000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.65\n",
      "Num timesteps: 3492000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 176.80\n",
      "Num timesteps: 3493000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.49\n",
      "Num timesteps: 3494000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.03\n",
      "Num timesteps: 3495000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.62\n",
      "Num timesteps: 3496000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.81\n",
      "Num timesteps: 3497000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.01\n",
      "Num timesteps: 3498000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.97\n",
      "Num timesteps: 3499000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.85\n",
      "Num timesteps: 3500000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 262      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1293     |\n",
      "|    iterations         | 35000    |\n",
      "|    time_elapsed       | 2706     |\n",
      "|    total_timesteps    | 3500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | 0.955    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 34999    |\n",
      "|    policy_loss        | 0.967    |\n",
      "|    value_loss         | 16       |\n",
      "------------------------------------\n",
      "Num timesteps: 3501000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.24\n",
      "Num timesteps: 3502000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.87\n",
      "Num timesteps: 3503000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.20\n",
      "Num timesteps: 3504000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.64\n",
      "Num timesteps: 3505000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.54\n",
      "Num timesteps: 3506000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3507000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.19\n",
      "Num timesteps: 3508000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 159.70\n",
      "Num timesteps: 3509000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.90\n",
      "Num timesteps: 3510000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 277      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1293     |\n",
      "|    iterations         | 35100    |\n",
      "|    time_elapsed       | 2712     |\n",
      "|    total_timesteps    | 3510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.635   |\n",
      "|    explained_variance | -0.00109 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35099    |\n",
      "|    policy_loss        | -5.21    |\n",
      "|    value_loss         | 73.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3511000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 152.11\n",
      "Num timesteps: 3512000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.34\n",
      "Num timesteps: 3513000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.14\n",
      "Num timesteps: 3514000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.94\n",
      "Num timesteps: 3515000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.33\n",
      "Num timesteps: 3516000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 146.73\n",
      "Num timesteps: 3517000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.59\n",
      "Num timesteps: 3518000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.64\n",
      "Num timesteps: 3519000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.27\n",
      "Num timesteps: 3520000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 140.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1294     |\n",
      "|    iterations         | 35200    |\n",
      "|    time_elapsed       | 2719     |\n",
      "|    total_timesteps    | 3520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.557   |\n",
      "|    explained_variance | -6.15    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35199    |\n",
      "|    policy_loss        | 24.3     |\n",
      "|    value_loss         | 1.07e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3521000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 140.30\n",
      "Num timesteps: 3522000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 3523000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.54\n",
      "Num timesteps: 3524000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.02\n",
      "Num timesteps: 3525000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.06\n",
      "Num timesteps: 3526000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.87\n",
      "Num timesteps: 3527000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.00\n",
      "Num timesteps: 3528000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 150.43\n",
      "Num timesteps: 3529000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.40\n",
      "Num timesteps: 3530000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 295      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 35300    |\n",
      "|    time_elapsed       | 2725     |\n",
      "|    total_timesteps    | 3530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.775   |\n",
      "|    explained_variance | 0.974    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35299    |\n",
      "|    policy_loss        | -2.68    |\n",
      "|    value_loss         | 18.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 3531000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 153.35\n",
      "Num timesteps: 3532000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.83\n",
      "Num timesteps: 3533000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.67\n",
      "Num timesteps: 3534000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.53\n",
      "Num timesteps: 3535000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 152.32\n",
      "Num timesteps: 3536000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 152.02\n",
      "Num timesteps: 3537000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.24\n",
      "Num timesteps: 3538000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 154.65\n",
      "Num timesteps: 3539000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.14\n",
      "Num timesteps: 3540000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 284      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 35400    |\n",
      "|    time_elapsed       | 2730     |\n",
      "|    total_timesteps    | 3540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.761   |\n",
      "|    explained_variance | 0.192    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35399    |\n",
      "|    policy_loss        | 4.81     |\n",
      "|    value_loss         | 118      |\n",
      "------------------------------------\n",
      "Num timesteps: 3541000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 153.18\n",
      "Num timesteps: 3542000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 152.63\n",
      "Num timesteps: 3543000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 154.98\n",
      "Num timesteps: 3544000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.84\n",
      "Num timesteps: 3545000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.39\n",
      "Num timesteps: 3546000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.28\n",
      "Num timesteps: 3547000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.35\n",
      "Num timesteps: 3548000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.73\n",
      "Num timesteps: 3549000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.56\n",
      "Num timesteps: 3550000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 279      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 35500    |\n",
      "|    time_elapsed       | 2736     |\n",
      "|    total_timesteps    | 3550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.431   |\n",
      "|    explained_variance | -3.83    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35499    |\n",
      "|    policy_loss        | 0.0901   |\n",
      "|    value_loss         | 1.35e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3551000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.62\n",
      "Num timesteps: 3552000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.61\n",
      "Num timesteps: 3553000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.26\n",
      "Num timesteps: 3554000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.69\n",
      "Num timesteps: 3555000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 141.92\n",
      "Num timesteps: 3556000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 139.07\n",
      "Num timesteps: 3557000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.29\n",
      "Num timesteps: 3558000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.15\n",
      "Num timesteps: 3559000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.36\n",
      "Num timesteps: 3560000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 141.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 35600    |\n",
      "|    time_elapsed       | 2742     |\n",
      "|    total_timesteps    | 3560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35599    |\n",
      "|    policy_loss        | 2.8      |\n",
      "|    value_loss         | 36.5     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3561000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 140.61\n",
      "Num timesteps: 3562000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.36\n",
      "Num timesteps: 3563000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.43\n",
      "Num timesteps: 3564000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.14\n",
      "Num timesteps: 3565000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.95\n",
      "Num timesteps: 3566000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.15\n",
      "Num timesteps: 3567000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.84\n",
      "Num timesteps: 3568000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.01\n",
      "Num timesteps: 3569000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.95\n",
      "Num timesteps: 3570000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 35700    |\n",
      "|    time_elapsed       | 2748     |\n",
      "|    total_timesteps    | 3570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.747   |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35699    |\n",
      "|    policy_loss        | 1.03     |\n",
      "|    value_loss         | 9.66     |\n",
      "------------------------------------\n",
      "Num timesteps: 3571000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.72\n",
      "Num timesteps: 3572000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.53\n",
      "Num timesteps: 3573000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.33\n",
      "Num timesteps: 3574000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 148.97\n",
      "Num timesteps: 3575000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.80\n",
      "Num timesteps: 3576000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.61\n",
      "Num timesteps: 3577000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.99\n",
      "Num timesteps: 3578000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 150.54\n",
      "Num timesteps: 3579000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 154.91\n",
      "Num timesteps: 3580000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 153.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 291      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 35800    |\n",
      "|    time_elapsed       | 2755     |\n",
      "|    total_timesteps    | 3580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.719   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35799    |\n",
      "|    policy_loss        | -0.814   |\n",
      "|    value_loss         | 4        |\n",
      "------------------------------------\n",
      "Num timesteps: 3581000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.11\n",
      "Num timesteps: 3582000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.94\n",
      "Num timesteps: 3583000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.52\n",
      "Num timesteps: 3584000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.68\n",
      "Num timesteps: 3585000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 159.56\n",
      "Num timesteps: 3586000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.79\n",
      "Num timesteps: 3587000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 154.74\n",
      "Num timesteps: 3588000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.49\n",
      "Num timesteps: 3589000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.80\n",
      "Num timesteps: 3590000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 35900    |\n",
      "|    time_elapsed       | 2761     |\n",
      "|    total_timesteps    | 3590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.732   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35899    |\n",
      "|    policy_loss        | 2.59     |\n",
      "|    value_loss         | 18.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 3591000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.92\n",
      "Num timesteps: 3592000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.17\n",
      "Num timesteps: 3593000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.65\n",
      "Num timesteps: 3594000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.65\n",
      "Num timesteps: 3595000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 159.65\n",
      "Num timesteps: 3596000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.10\n",
      "Num timesteps: 3597000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.03\n",
      "Num timesteps: 3598000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.21\n",
      "Num timesteps: 3599000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.29\n",
      "Num timesteps: 3600000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 308      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36000    |\n",
      "|    time_elapsed       | 2769     |\n",
      "|    total_timesteps    | 3600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.374   |\n",
      "|    explained_variance | -6.11    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 35999    |\n",
      "|    policy_loss        | -7.24    |\n",
      "|    value_loss         | 547      |\n",
      "------------------------------------\n",
      "Num timesteps: 3601000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.22\n",
      "Num timesteps: 3602000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.51\n",
      "Num timesteps: 3603000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.76\n",
      "Num timesteps: 3604000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.90\n",
      "Num timesteps: 3605000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.17\n",
      "Num timesteps: 3606000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 153.26\n",
      "Num timesteps: 3607000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 150.58\n",
      "Num timesteps: 3608000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.84\n",
      "Num timesteps: 3609000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.99\n",
      "Num timesteps: 3610000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 149.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 36100    |\n",
      "|    time_elapsed       | 2776     |\n",
      "|    total_timesteps    | 3610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.619   |\n",
      "|    explained_variance | 0.248    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36099    |\n",
      "|    policy_loss        | -0.262   |\n",
      "|    value_loss         | 60.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 3611000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 146.64\n",
      "Num timesteps: 3612000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 147.95\n",
      "Num timesteps: 3613000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.91\n",
      "Num timesteps: 3614000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.37\n",
      "Num timesteps: 3615000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.74\n",
      "Num timesteps: 3616000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.73\n",
      "Num timesteps: 3617000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.53\n",
      "Num timesteps: 3618000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 144.65\n",
      "Num timesteps: 3619000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3620000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 141.70\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 342      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 36200    |\n",
      "|    time_elapsed       | 2783     |\n",
      "|    total_timesteps    | 3620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | -0.00551 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36199    |\n",
      "|    policy_loss        | -4.03    |\n",
      "|    value_loss         | 47.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 3621000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.74\n",
      "Num timesteps: 3622000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.85\n",
      "Num timesteps: 3623000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 143.74\n",
      "Num timesteps: 3624000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.01\n",
      "Num timesteps: 3625000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 141.65\n",
      "Num timesteps: 3626000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 137.89\n",
      "Num timesteps: 3627000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 137.66\n",
      "Num timesteps: 3628000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 137.10\n",
      "Num timesteps: 3629000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 138.24\n",
      "Num timesteps: 3630000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 138.70\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 358      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 36300    |\n",
      "|    time_elapsed       | 2792     |\n",
      "|    total_timesteps    | 3630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.793   |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36299    |\n",
      "|    policy_loss        | 3.35     |\n",
      "|    value_loss         | 22.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 3631000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 138.71\n",
      "Num timesteps: 3632000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.46\n",
      "Num timesteps: 3633000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 142.81\n",
      "Num timesteps: 3634000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.35\n",
      "Num timesteps: 3635000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.53\n",
      "Num timesteps: 3636000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 146.46\n",
      "Num timesteps: 3637000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 141.55\n",
      "Num timesteps: 3638000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 145.63\n",
      "Num timesteps: 3639000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 151.67\n",
      "Num timesteps: 3640000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 150.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36400    |\n",
      "|    time_elapsed       | 2800     |\n",
      "|    total_timesteps    | 3640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36399    |\n",
      "|    policy_loss        | -1.23    |\n",
      "|    value_loss         | 10.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3641000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.84\n",
      "Num timesteps: 3642000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.52\n",
      "Num timesteps: 3643000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.12\n",
      "Num timesteps: 3644000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.52\n",
      "Num timesteps: 3645000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.94\n",
      "Num timesteps: 3646000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.01\n",
      "Num timesteps: 3647000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.07\n",
      "Num timesteps: 3648000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.97\n",
      "Num timesteps: 3649000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.35\n",
      "Num timesteps: 3650000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 380      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36500    |\n",
      "|    time_elapsed       | 2808     |\n",
      "|    total_timesteps    | 3650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.849   |\n",
      "|    explained_variance | 0.879    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36499    |\n",
      "|    policy_loss        | -2.67    |\n",
      "|    value_loss         | 14.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3651000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.26\n",
      "Num timesteps: 3652000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.80\n",
      "Num timesteps: 3653000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.00\n",
      "Num timesteps: 3654000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.97\n",
      "Num timesteps: 3655000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.64\n",
      "Num timesteps: 3656000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.20\n",
      "Num timesteps: 3657000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.11\n",
      "Num timesteps: 3658000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.08\n",
      "Num timesteps: 3659000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.03\n",
      "Num timesteps: 3660000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 169.81\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 381      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36600    |\n",
      "|    time_elapsed       | 2816     |\n",
      "|    total_timesteps    | 3660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | -2.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36599    |\n",
      "|    policy_loss        | 1.46     |\n",
      "|    value_loss         | 1.37e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3661000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.04\n",
      "Num timesteps: 3662000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.06\n",
      "Num timesteps: 3663000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.69\n",
      "Num timesteps: 3664000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.01\n",
      "Num timesteps: 3665000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 172.44\n",
      "Num timesteps: 3666000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.96\n",
      "Num timesteps: 3667000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.43\n",
      "Num timesteps: 3668000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.16\n",
      "Num timesteps: 3669000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.16\n",
      "Num timesteps: 3670000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 383      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36700    |\n",
      "|    time_elapsed       | 2824     |\n",
      "|    total_timesteps    | 3670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0619  |\n",
      "|    explained_variance | -1.92    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36699    |\n",
      "|    policy_loss        | 0.866    |\n",
      "|    value_loss         | 625      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3671000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.43\n",
      "Num timesteps: 3672000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.99\n",
      "Num timesteps: 3673000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.66\n",
      "Num timesteps: 3674000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.59\n",
      "Num timesteps: 3675000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.20\n",
      "Num timesteps: 3676000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.44\n",
      "Num timesteps: 3677000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.46\n",
      "Num timesteps: 3678000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.61\n",
      "Num timesteps: 3679000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.97\n",
      "Num timesteps: 3680000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 396      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 36800    |\n",
      "|    time_elapsed       | 2832     |\n",
      "|    total_timesteps    | 3680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36799    |\n",
      "|    policy_loss        | 3        |\n",
      "|    value_loss         | 57.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3681000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.95\n",
      "Num timesteps: 3682000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.09\n",
      "Num timesteps: 3683000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.14\n",
      "Num timesteps: 3684000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.93\n",
      "Num timesteps: 3685000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.10\n",
      "Num timesteps: 3686000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.48\n",
      "Num timesteps: 3687000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.38\n",
      "Num timesteps: 3688000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.44\n",
      "Num timesteps: 3689000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.03\n",
      "Num timesteps: 3690000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.72\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 403      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 36900    |\n",
      "|    time_elapsed       | 2841     |\n",
      "|    total_timesteps    | 3690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | -0.145   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36899    |\n",
      "|    policy_loss        | 0.937    |\n",
      "|    value_loss         | 9.05     |\n",
      "------------------------------------\n",
      "Num timesteps: 3691000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.35\n",
      "Num timesteps: 3692000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.84\n",
      "Num timesteps: 3693000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.44\n",
      "Num timesteps: 3694000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.29\n",
      "Num timesteps: 3695000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.13\n",
      "Num timesteps: 3696000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 161.53\n",
      "Num timesteps: 3697000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.21\n",
      "Num timesteps: 3698000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.20\n",
      "Num timesteps: 3699000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.00\n",
      "Num timesteps: 3700000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 366      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 37000    |\n",
      "|    time_elapsed       | 2847     |\n",
      "|    total_timesteps    | 3700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.728   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 36999    |\n",
      "|    policy_loss        | 0.577    |\n",
      "|    value_loss         | 5.92     |\n",
      "------------------------------------\n",
      "Num timesteps: 3701000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.53\n",
      "Num timesteps: 3702000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 159.06\n",
      "Num timesteps: 3703000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.44\n",
      "Num timesteps: 3704000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.23\n",
      "Num timesteps: 3705000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.52\n",
      "Num timesteps: 3706000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 156.98\n",
      "Num timesteps: 3707000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.25\n",
      "Num timesteps: 3708000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.26\n",
      "Num timesteps: 3709000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.22\n",
      "Num timesteps: 3710000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 155.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 346      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 37100    |\n",
      "|    time_elapsed       | 2856     |\n",
      "|    total_timesteps    | 3710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.699   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37099    |\n",
      "|    policy_loss        | 4.57     |\n",
      "|    value_loss         | 60.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3711000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 157.95\n",
      "Num timesteps: 3712000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.71\n",
      "Num timesteps: 3713000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.15\n",
      "Num timesteps: 3714000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 158.82\n",
      "Num timesteps: 3715000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.51\n",
      "Num timesteps: 3716000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.53\n",
      "Num timesteps: 3717000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.28\n",
      "Num timesteps: 3718000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.76\n",
      "Num timesteps: 3719000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.97\n",
      "Num timesteps: 3720000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 169.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 340      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 37200    |\n",
      "|    time_elapsed       | 2865     |\n",
      "|    total_timesteps    | 3720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.814   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37199    |\n",
      "|    policy_loss        | -1.21    |\n",
      "|    value_loss         | 4.61     |\n",
      "------------------------------------\n",
      "Num timesteps: 3721000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.92\n",
      "Num timesteps: 3722000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.50\n",
      "Num timesteps: 3723000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.64\n",
      "Num timesteps: 3724000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.70\n",
      "Num timesteps: 3725000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.81\n",
      "Num timesteps: 3726000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.13\n",
      "Num timesteps: 3727000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.02\n",
      "Num timesteps: 3728000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.50\n",
      "Num timesteps: 3729000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3730000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 172.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 37300    |\n",
      "|    time_elapsed       | 2871     |\n",
      "|    total_timesteps    | 3730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.73    |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37299    |\n",
      "|    policy_loss        | -6.9     |\n",
      "|    value_loss         | 75.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 3731000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 169.75\n",
      "Num timesteps: 3732000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.93\n",
      "Num timesteps: 3733000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.10\n",
      "Num timesteps: 3734000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.70\n",
      "Num timesteps: 3735000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.40\n",
      "Num timesteps: 3736000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 173.87\n",
      "Num timesteps: 3737000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 169.13\n",
      "Num timesteps: 3738000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.33\n",
      "Num timesteps: 3739000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.54\n",
      "Num timesteps: 3740000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 167.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 335      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 37400    |\n",
      "|    time_elapsed       | 2878     |\n",
      "|    total_timesteps    | 3740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.472   |\n",
      "|    explained_variance | -6.63    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37399    |\n",
      "|    policy_loss        | 9.57     |\n",
      "|    value_loss         | 2.83e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3741000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.34\n",
      "Num timesteps: 3742000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.82\n",
      "Num timesteps: 3743000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.05\n",
      "Num timesteps: 3744000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.59\n",
      "Num timesteps: 3745000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.44\n",
      "Num timesteps: 3746000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 163.01\n",
      "Num timesteps: 3747000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.49\n",
      "Num timesteps: 3748000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.41\n",
      "Num timesteps: 3749000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 164.97\n",
      "Num timesteps: 3750000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 37500    |\n",
      "|    time_elapsed       | 2884     |\n",
      "|    total_timesteps    | 3750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.722   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37499    |\n",
      "|    policy_loss        | 2.78     |\n",
      "|    value_loss         | 33.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3751000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.76\n",
      "Num timesteps: 3752000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.56\n",
      "Num timesteps: 3753000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 159.42\n",
      "Num timesteps: 3754000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 160.29\n",
      "Num timesteps: 3755000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 162.54\n",
      "Num timesteps: 3756000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 165.48\n",
      "Num timesteps: 3757000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 168.65\n",
      "Num timesteps: 3758000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 166.39\n",
      "Num timesteps: 3759000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 169.86\n",
      "Num timesteps: 3760000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 37600    |\n",
      "|    time_elapsed       | 2891     |\n",
      "|    total_timesteps    | 3760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.643   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37599    |\n",
      "|    policy_loss        | -8.71    |\n",
      "|    value_loss         | 167      |\n",
      "------------------------------------\n",
      "Num timesteps: 3761000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 171.60\n",
      "Num timesteps: 3762000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 170.90\n",
      "Num timesteps: 3763000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 172.19\n",
      "Num timesteps: 3764000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.97\n",
      "Num timesteps: 3765000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 177.74\n",
      "Num timesteps: 3766000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 179.43\n",
      "Num timesteps: 3767000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 175.60\n",
      "Num timesteps: 3768000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.80\n",
      "Num timesteps: 3769000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 174.12\n",
      "Num timesteps: 3770000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 176.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 319      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 37700    |\n",
      "|    time_elapsed       | 2898     |\n",
      "|    total_timesteps    | 3770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.772   |\n",
      "|    explained_variance | 0.537    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37699    |\n",
      "|    policy_loss        | 8.65     |\n",
      "|    value_loss         | 110      |\n",
      "------------------------------------\n",
      "Num timesteps: 3771000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 176.46\n",
      "Num timesteps: 3772000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 175.77\n",
      "Num timesteps: 3773000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 180.13\n",
      "Num timesteps: 3774000\n",
      "Best mean reward: 180.98 - Last mean reward per episode: 183.03\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3775000\n",
      "Best mean reward: 183.03 - Last mean reward per episode: 187.41\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3776000\n",
      "Best mean reward: 187.41 - Last mean reward per episode: 185.61\n",
      "Num timesteps: 3777000\n",
      "Best mean reward: 187.41 - Last mean reward per episode: 185.76\n",
      "Num timesteps: 3778000\n",
      "Best mean reward: 187.41 - Last mean reward per episode: 189.68\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 3779000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 183.51\n",
      "Num timesteps: 3780000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 184.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 316      |\n",
      "|    ep_rew_mean        | 185      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 37800    |\n",
      "|    time_elapsed       | 2905     |\n",
      "|    total_timesteps    | 3780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.347   |\n",
      "|    explained_variance | -0.684   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37799    |\n",
      "|    policy_loss        | 13       |\n",
      "|    value_loss         | 3.6e+03  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3781000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 184.75\n",
      "Num timesteps: 3782000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 183.24\n",
      "Num timesteps: 3783000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 182.25\n",
      "Num timesteps: 3784000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 176.03\n",
      "Num timesteps: 3785000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 176.85\n",
      "Num timesteps: 3786000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 177.25\n",
      "Num timesteps: 3787000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 178.99\n",
      "Num timesteps: 3788000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 174.61\n",
      "Num timesteps: 3789000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 174.32\n",
      "Num timesteps: 3790000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 171.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 325      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 37900    |\n",
      "|    time_elapsed       | 2911     |\n",
      "|    total_timesteps    | 3790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.781   |\n",
      "|    explained_variance | 0.0341   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37899    |\n",
      "|    policy_loss        | -5.6     |\n",
      "|    value_loss         | 58       |\n",
      "------------------------------------\n",
      "Num timesteps: 3791000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 171.04\n",
      "Num timesteps: 3792000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 169.88\n",
      "Num timesteps: 3793000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 169.98\n",
      "Num timesteps: 3794000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 169.83\n",
      "Num timesteps: 3795000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 169.89\n",
      "Num timesteps: 3796000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.01\n",
      "Num timesteps: 3797000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.01\n",
      "Num timesteps: 3798000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.38\n",
      "Num timesteps: 3799000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.90\n",
      "Num timesteps: 3800000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 38000    |\n",
      "|    time_elapsed       | 2920     |\n",
      "|    total_timesteps    | 3800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.336   |\n",
      "|    explained_variance | -0.0357  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 37999    |\n",
      "|    policy_loss        | -0.465   |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "Num timesteps: 3801000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.32\n",
      "Num timesteps: 3802000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.54\n",
      "Num timesteps: 3803000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.82\n",
      "Num timesteps: 3804000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.26\n",
      "Num timesteps: 3805000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 167.65\n",
      "Num timesteps: 3806000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.83\n",
      "Num timesteps: 3807000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.71\n",
      "Num timesteps: 3808000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.21\n",
      "Num timesteps: 3809000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.05\n",
      "Num timesteps: 3810000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.63\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 342      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 38100    |\n",
      "|    time_elapsed       | 2927     |\n",
      "|    total_timesteps    | 3810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.749   |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38099    |\n",
      "|    policy_loss        | 3.39     |\n",
      "|    value_loss         | 23.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 3811000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.66\n",
      "Num timesteps: 3812000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 156.79\n",
      "Num timesteps: 3813000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 158.23\n",
      "Num timesteps: 3814000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.77\n",
      "Num timesteps: 3815000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.62\n",
      "Num timesteps: 3816000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.72\n",
      "Num timesteps: 3817000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.85\n",
      "Num timesteps: 3818000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 167.57\n",
      "Num timesteps: 3819000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.19\n",
      "Num timesteps: 3820000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 172.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 38200    |\n",
      "|    time_elapsed       | 2935     |\n",
      "|    total_timesteps    | 3820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.384   |\n",
      "|    explained_variance | -2       |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38199    |\n",
      "|    policy_loss        | 1.2      |\n",
      "|    value_loss         | 716      |\n",
      "------------------------------------\n",
      "Num timesteps: 3821000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 171.77\n",
      "Num timesteps: 3822000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.67\n",
      "Num timesteps: 3823000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.73\n",
      "Num timesteps: 3824000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.58\n",
      "Num timesteps: 3825000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.82\n",
      "Num timesteps: 3826000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.10\n",
      "Num timesteps: 3827000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 167.54\n",
      "Num timesteps: 3828000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 3829000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.22\n",
      "Num timesteps: 3830000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 38300    |\n",
      "|    time_elapsed       | 2944     |\n",
      "|    total_timesteps    | 3830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.926   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38299    |\n",
      "|    policy_loss        | -2.17    |\n",
      "|    value_loss         | 6.8      |\n",
      "------------------------------------\n",
      "Num timesteps: 3831000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 167.95\n",
      "Num timesteps: 3832000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 167.30\n",
      "Num timesteps: 3833000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.58\n",
      "Num timesteps: 3834000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.30\n",
      "Num timesteps: 3835000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.99\n",
      "Num timesteps: 3836000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.45\n",
      "Num timesteps: 3837000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.21\n",
      "Num timesteps: 3838000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 175.85\n",
      "Num timesteps: 3839000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 173.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3840000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 174.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 364      |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 38400    |\n",
      "|    time_elapsed       | 2951     |\n",
      "|    total_timesteps    | 3840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.546   |\n",
      "|    explained_variance | 0.561    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38399    |\n",
      "|    policy_loss        | -24.1    |\n",
      "|    value_loss         | 5.55e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3841000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 172.01\n",
      "Num timesteps: 3842000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 171.13\n",
      "Num timesteps: 3843000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 176.55\n",
      "Num timesteps: 3844000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 175.73\n",
      "Num timesteps: 3845000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 174.04\n",
      "Num timesteps: 3846000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 175.78\n",
      "Num timesteps: 3847000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 170.82\n",
      "Num timesteps: 3848000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.59\n",
      "Num timesteps: 3849000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.29\n",
      "Num timesteps: 3850000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 378      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 38500    |\n",
      "|    time_elapsed       | 2959     |\n",
      "|    total_timesteps    | 3850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | -0.00285 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38499    |\n",
      "|    policy_loss        | -8.22    |\n",
      "|    value_loss         | 172      |\n",
      "------------------------------------\n",
      "Num timesteps: 3851000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 171.29\n",
      "Num timesteps: 3852000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 168.37\n",
      "Num timesteps: 3853000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.37\n",
      "Num timesteps: 3854000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.12\n",
      "Num timesteps: 3855000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.80\n",
      "Num timesteps: 3856000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.44\n",
      "Num timesteps: 3857000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 158.13\n",
      "Num timesteps: 3858000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 156.36\n",
      "Num timesteps: 3859000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 153.43\n",
      "Num timesteps: 3860000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 401      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 38600    |\n",
      "|    time_elapsed       | 2971     |\n",
      "|    total_timesteps    | 3860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.582   |\n",
      "|    explained_variance | -19.2    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38599    |\n",
      "|    policy_loss        | 14.2     |\n",
      "|    value_loss         | 492      |\n",
      "------------------------------------\n",
      "Num timesteps: 3861000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 152.90\n",
      "Num timesteps: 3862000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.34\n",
      "Num timesteps: 3863000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.34\n",
      "Num timesteps: 3864000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 157.75\n",
      "Num timesteps: 3865000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 154.65\n",
      "Num timesteps: 3866000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 153.75\n",
      "Num timesteps: 3867000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.97\n",
      "Num timesteps: 3868000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.19\n",
      "Num timesteps: 3869000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.27\n",
      "Num timesteps: 3870000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 145.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 415      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 38700    |\n",
      "|    time_elapsed       | 2980     |\n",
      "|    total_timesteps    | 3870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.865   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38699    |\n",
      "|    policy_loss        | -3.31    |\n",
      "|    value_loss         | 18.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 3871000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.27\n",
      "Num timesteps: 3872000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.55\n",
      "Num timesteps: 3873000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.41\n",
      "Num timesteps: 3874000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.45\n",
      "Num timesteps: 3875000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.58\n",
      "Num timesteps: 3876000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.94\n",
      "Num timesteps: 3877000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.21\n",
      "Num timesteps: 3878000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 143.52\n",
      "Num timesteps: 3879000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.67\n",
      "Num timesteps: 3880000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 139.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 431      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 38800    |\n",
      "|    time_elapsed       | 2989     |\n",
      "|    total_timesteps    | 3880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.843   |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38799    |\n",
      "|    policy_loss        | 6.05     |\n",
      "|    value_loss         | 70.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3881000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 136.63\n",
      "Num timesteps: 3882000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 135.90\n",
      "Num timesteps: 3883000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.41\n",
      "Num timesteps: 3884000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 135.35\n",
      "Num timesteps: 3885000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 137.85\n",
      "Num timesteps: 3886000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.78\n",
      "Num timesteps: 3887000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.89\n",
      "Num timesteps: 3888000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 137.84\n",
      "Num timesteps: 3889000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.56\n",
      "Num timesteps: 3890000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 435      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 38900    |\n",
      "|    time_elapsed       | 2998     |\n",
      "|    total_timesteps    | 3890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.165   |\n",
      "|    explained_variance | 0.00175  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38899    |\n",
      "|    policy_loss        | -3.26    |\n",
      "|    value_loss         | 129      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3891000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.26\n",
      "Num timesteps: 3892000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.75\n",
      "Num timesteps: 3893000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.74\n",
      "Num timesteps: 3894000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.02\n",
      "Num timesteps: 3895000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 150.36\n",
      "Num timesteps: 3896000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.72\n",
      "Num timesteps: 3897000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.44\n",
      "Num timesteps: 3898000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.89\n",
      "Num timesteps: 3899000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.99\n",
      "Num timesteps: 3900000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 145.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 39000    |\n",
      "|    time_elapsed       | 3008     |\n",
      "|    total_timesteps    | 3900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | -0.00345 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 38999    |\n",
      "|    policy_loss        | -9.56    |\n",
      "|    value_loss         | 307      |\n",
      "------------------------------------\n",
      "Num timesteps: 3901000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.60\n",
      "Num timesteps: 3902000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 145.87\n",
      "Num timesteps: 3903000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 148.47\n",
      "Num timesteps: 3904000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 148.86\n",
      "Num timesteps: 3905000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.79\n",
      "Num timesteps: 3906000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 148.74\n",
      "Num timesteps: 3907000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 143.69\n",
      "Num timesteps: 3908000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.02\n",
      "Num timesteps: 3909000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.84\n",
      "Num timesteps: 3910000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.13\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 415      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 39100    |\n",
      "|    time_elapsed       | 3015     |\n",
      "|    total_timesteps    | 3910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.825   |\n",
      "|    explained_variance | -0.347   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39099    |\n",
      "|    policy_loss        | 4.94     |\n",
      "|    value_loss         | 76.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 3911000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.60\n",
      "Num timesteps: 3912000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.55\n",
      "Num timesteps: 3913000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.66\n",
      "Num timesteps: 3914000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.17\n",
      "Num timesteps: 3915000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.23\n",
      "Num timesteps: 3916000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.14\n",
      "Num timesteps: 3917000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 165.46\n",
      "Num timesteps: 3918000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.08\n",
      "Num timesteps: 3919000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.44\n",
      "Num timesteps: 3920000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 386      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 39200    |\n",
      "|    time_elapsed       | 3023     |\n",
      "|    total_timesteps    | 3920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.793   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39199    |\n",
      "|    policy_loss        | 4.71     |\n",
      "|    value_loss         | 46.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 3921000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.48\n",
      "Num timesteps: 3922000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.91\n",
      "Num timesteps: 3923000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.86\n",
      "Num timesteps: 3924000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 161.71\n",
      "Num timesteps: 3925000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 159.42\n",
      "Num timesteps: 3926000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 158.62\n",
      "Num timesteps: 3927000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 154.04\n",
      "Num timesteps: 3928000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 156.09\n",
      "Num timesteps: 3929000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 157.41\n",
      "Num timesteps: 3930000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 156.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 414      |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 39300    |\n",
      "|    time_elapsed       | 3033     |\n",
      "|    total_timesteps    | 3930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.769   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39299    |\n",
      "|    policy_loss        | -6.01    |\n",
      "|    value_loss         | 78.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 3931000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 152.19\n",
      "Num timesteps: 3932000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 150.38\n",
      "Num timesteps: 3933000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.60\n",
      "Num timesteps: 3934000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 145.28\n",
      "Num timesteps: 3935000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.16\n",
      "Num timesteps: 3936000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 148.57\n",
      "Num timesteps: 3937000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 154.22\n",
      "Num timesteps: 3938000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.28\n",
      "Num timesteps: 3939000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 157.84\n",
      "Num timesteps: 3940000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 363      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 39400    |\n",
      "|    time_elapsed       | 3039     |\n",
      "|    total_timesteps    | 3940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.7     |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39399    |\n",
      "|    policy_loss        | -40.7    |\n",
      "|    value_loss         | 5.39e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3941000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 155.96\n",
      "Num timesteps: 3942000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 154.38\n",
      "Num timesteps: 3943000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 153.64\n",
      "Num timesteps: 3944000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.84\n",
      "Num timesteps: 3945000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 154.82\n",
      "Num timesteps: 3946000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 151.81\n",
      "Num timesteps: 3947000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.98\n",
      "Num timesteps: 3948000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.46\n",
      "Num timesteps: 3949000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3950000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 143.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 39500    |\n",
      "|    time_elapsed       | 3048     |\n",
      "|    total_timesteps    | 3950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.913   |\n",
      "|    explained_variance | -0.00354 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39499    |\n",
      "|    policy_loss        | -1.49    |\n",
      "|    value_loss         | 19.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 3951000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.66\n",
      "Num timesteps: 3952000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 137.99\n",
      "Num timesteps: 3953000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 136.57\n",
      "Num timesteps: 3954000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.02\n",
      "Num timesteps: 3955000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.04\n",
      "Num timesteps: 3956000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 139.14\n",
      "Num timesteps: 3957000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 130.29\n",
      "Num timesteps: 3958000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 135.64\n",
      "Num timesteps: 3959000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 134.13\n",
      "Num timesteps: 3960000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 135.72\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 335      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 39600    |\n",
      "|    time_elapsed       | 3056     |\n",
      "|    total_timesteps    | 3960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.597   |\n",
      "|    explained_variance | -3.95    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39599    |\n",
      "|    policy_loss        | 12.2     |\n",
      "|    value_loss         | 384      |\n",
      "------------------------------------\n",
      "Num timesteps: 3961000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 136.19\n",
      "Num timesteps: 3962000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 139.82\n",
      "Num timesteps: 3963000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.79\n",
      "Num timesteps: 3964000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.45\n",
      "Num timesteps: 3965000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 139.70\n",
      "Num timesteps: 3966000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.43\n",
      "Num timesteps: 3967000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.01\n",
      "Num timesteps: 3968000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 146.71\n",
      "Num timesteps: 3969000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 142.93\n",
      "Num timesteps: 3970000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 39700    |\n",
      "|    time_elapsed       | 3063     |\n",
      "|    total_timesteps    | 3970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | 0.142    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39699    |\n",
      "|    policy_loss        | -0.755   |\n",
      "|    value_loss         | 7.27     |\n",
      "------------------------------------\n",
      "Num timesteps: 3971000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 137.10\n",
      "Num timesteps: 3972000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 135.69\n",
      "Num timesteps: 3973000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 137.35\n",
      "Num timesteps: 3974000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.98\n",
      "Num timesteps: 3975000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 138.91\n",
      "Num timesteps: 3976000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 140.80\n",
      "Num timesteps: 3977000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 138.94\n",
      "Num timesteps: 3978000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.49\n",
      "Num timesteps: 3979000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 141.62\n",
      "Num timesteps: 3980000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 138.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 39800    |\n",
      "|    time_elapsed       | 3071     |\n",
      "|    total_timesteps    | 3980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | 0.0712   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39799    |\n",
      "|    policy_loss        | 3.27     |\n",
      "|    value_loss         | 22.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 3981000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 144.13\n",
      "Num timesteps: 3982000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 150.10\n",
      "Num timesteps: 3983000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 147.13\n",
      "Num timesteps: 3984000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 149.93\n",
      "Num timesteps: 3985000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 159.96\n",
      "Num timesteps: 3986000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.25\n",
      "Num timesteps: 3987000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 163.48\n",
      "Num timesteps: 3988000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.80\n",
      "Num timesteps: 3989000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 160.44\n",
      "Num timesteps: 3990000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 162.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 39900    |\n",
      "|    time_elapsed       | 3077     |\n",
      "|    total_timesteps    | 3990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.3     |\n",
      "|    explained_variance | -2.37    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39899    |\n",
      "|    policy_loss        | 16.1     |\n",
      "|    value_loss         | 2.78e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 3991000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 164.17\n",
      "Num timesteps: 3992000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 166.25\n",
      "Num timesteps: 3993000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 173.97\n",
      "Num timesteps: 3994000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 176.77\n",
      "Num timesteps: 3995000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 179.16\n",
      "Num timesteps: 3996000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 175.95\n",
      "Num timesteps: 3997000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 185.02\n",
      "Num timesteps: 3998000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 188.85\n",
      "Num timesteps: 3999000\n",
      "Best mean reward: 189.68 - Last mean reward per episode: 192.05\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4000000\n",
      "Best mean reward: 192.05 - Last mean reward per episode: 189.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 260      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 40000    |\n",
      "|    time_elapsed       | 3082     |\n",
      "|    total_timesteps    | 4000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | -7.65    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 39999    |\n",
      "|    policy_loss        | 11.7     |\n",
      "|    value_loss         | 494      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4001000\n",
      "Best mean reward: 192.05 - Last mean reward per episode: 190.09\n",
      "Num timesteps: 4002000\n",
      "Best mean reward: 192.05 - Last mean reward per episode: 190.69\n",
      "Num timesteps: 4003000\n",
      "Best mean reward: 192.05 - Last mean reward per episode: 193.72\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4004000\n",
      "Best mean reward: 193.72 - Last mean reward per episode: 190.26\n",
      "Num timesteps: 4005000\n",
      "Best mean reward: 193.72 - Last mean reward per episode: 194.38\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4006000\n",
      "Best mean reward: 194.38 - Last mean reward per episode: 199.31\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4007000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 194.65\n",
      "Num timesteps: 4008000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 194.76\n",
      "Num timesteps: 4009000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 196.15\n",
      "Num timesteps: 4010000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 194.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 278      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 40100    |\n",
      "|    time_elapsed       | 3090     |\n",
      "|    total_timesteps    | 4010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.985   |\n",
      "|    explained_variance | 0.0761   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40099    |\n",
      "|    policy_loss        | 0.292    |\n",
      "|    value_loss         | 6.85     |\n",
      "------------------------------------\n",
      "Num timesteps: 4011000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 186.83\n",
      "Num timesteps: 4012000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 189.13\n",
      "Num timesteps: 4013000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 185.60\n",
      "Num timesteps: 4014000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.61\n",
      "Num timesteps: 4015000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.34\n",
      "Num timesteps: 4016000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 172.49\n",
      "Num timesteps: 4017000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 176.39\n",
      "Num timesteps: 4018000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 176.82\n",
      "Num timesteps: 4019000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 176.50\n",
      "Num timesteps: 4020000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 176.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 289      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 40200    |\n",
      "|    time_elapsed       | 3098     |\n",
      "|    total_timesteps    | 4020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.921   |\n",
      "|    explained_variance | 0.15     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40199    |\n",
      "|    policy_loss        | -3.21    |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "Num timesteps: 4021000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.93\n",
      "Num timesteps: 4022000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.55\n",
      "Num timesteps: 4023000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.56\n",
      "Num timesteps: 4024000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.40\n",
      "Num timesteps: 4025000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.32\n",
      "Num timesteps: 4026000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 157.93\n",
      "Num timesteps: 4027000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 157.70\n",
      "Num timesteps: 4028000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 152.78\n",
      "Num timesteps: 4029000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 151.56\n",
      "Num timesteps: 4030000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 152.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 298      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 40300    |\n",
      "|    time_elapsed       | 3104     |\n",
      "|    total_timesteps    | 4030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.639   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40299    |\n",
      "|    policy_loss        | -2.38    |\n",
      "|    value_loss         | 27       |\n",
      "------------------------------------\n",
      "Num timesteps: 4031000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.29\n",
      "Num timesteps: 4032000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.66\n",
      "Num timesteps: 4033000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.61\n",
      "Num timesteps: 4034000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.10\n",
      "Num timesteps: 4035000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 140.82\n",
      "Num timesteps: 4036000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 143.91\n",
      "Num timesteps: 4037000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.95\n",
      "Num timesteps: 4038000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 145.94\n",
      "Num timesteps: 4039000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.37\n",
      "Num timesteps: 4040000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 305      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 40400    |\n",
      "|    time_elapsed       | 3111     |\n",
      "|    total_timesteps    | 4040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0.52     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40399    |\n",
      "|    policy_loss        | -38.4    |\n",
      "|    value_loss         | 5e+03    |\n",
      "------------------------------------\n",
      "Num timesteps: 4041000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.62\n",
      "Num timesteps: 4042000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 155.76\n",
      "Num timesteps: 4043000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 154.52\n",
      "Num timesteps: 4044000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.90\n",
      "Num timesteps: 4045000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.97\n",
      "Num timesteps: 4046000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 151.18\n",
      "Num timesteps: 4047000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.73\n",
      "Num timesteps: 4048000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 148.01\n",
      "Num timesteps: 4049000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 139.07\n",
      "Num timesteps: 4050000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 139.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 40500    |\n",
      "|    time_elapsed       | 3118     |\n",
      "|    total_timesteps    | 4050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | -2.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40499    |\n",
      "|    policy_loss        | 15       |\n",
      "|    value_loss         | 1.1e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 4051000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 141.49\n",
      "Num timesteps: 4052000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.22\n",
      "Num timesteps: 4053000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.08\n",
      "Num timesteps: 4054000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 152.51\n",
      "Num timesteps: 4055000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.66\n",
      "Num timesteps: 4056000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.17\n",
      "Num timesteps: 4057000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 139.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4058000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 141.47\n",
      "Num timesteps: 4059000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 140.40\n",
      "Num timesteps: 4060000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 278      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 40600    |\n",
      "|    time_elapsed       | 3124     |\n",
      "|    total_timesteps    | 4060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.129   |\n",
      "|    explained_variance | -1.21    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40599    |\n",
      "|    policy_loss        | -0.449   |\n",
      "|    value_loss         | 9.94     |\n",
      "------------------------------------\n",
      "Num timesteps: 4061000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 141.40\n",
      "Num timesteps: 4062000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.44\n",
      "Num timesteps: 4063000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 145.16\n",
      "Num timesteps: 4064000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.21\n",
      "Num timesteps: 4065000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 145.61\n",
      "Num timesteps: 4066000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.80\n",
      "Num timesteps: 4067000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 137.79\n",
      "Num timesteps: 4068000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 141.13\n",
      "Num timesteps: 4069000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.55\n",
      "Num timesteps: 4070000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 251      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 40700    |\n",
      "|    time_elapsed       | 3131     |\n",
      "|    total_timesteps    | 4070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | -2.31    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40699    |\n",
      "|    policy_loss        | 3.47     |\n",
      "|    value_loss         | 2.29e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4071000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 151.47\n",
      "Num timesteps: 4072000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 159.97\n",
      "Num timesteps: 4073000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 156.23\n",
      "Num timesteps: 4074000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.66\n",
      "Num timesteps: 4075000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.32\n",
      "Num timesteps: 4076000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.08\n",
      "Num timesteps: 4077000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.61\n",
      "Num timesteps: 4078000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 153.69\n",
      "Num timesteps: 4079000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 154.20\n",
      "Num timesteps: 4080000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 153.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 253      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 40800    |\n",
      "|    time_elapsed       | 3136     |\n",
      "|    total_timesteps    | 4080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | -0.289   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40799    |\n",
      "|    policy_loss        | -2.36    |\n",
      "|    value_loss         | 10.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 4081000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.77\n",
      "Num timesteps: 4082000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.04\n",
      "Num timesteps: 4083000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.38\n",
      "Num timesteps: 4084000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 169.29\n",
      "Num timesteps: 4085000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 168.45\n",
      "Num timesteps: 4086000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.74\n",
      "Num timesteps: 4087000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.81\n",
      "Num timesteps: 4088000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.72\n",
      "Num timesteps: 4089000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.13\n",
      "Num timesteps: 4090000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 249      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 40900    |\n",
      "|    time_elapsed       | 3142     |\n",
      "|    total_timesteps    | 4090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.758   |\n",
      "|    explained_variance | -1.33    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40899    |\n",
      "|    policy_loss        | 9.12     |\n",
      "|    value_loss         | 114      |\n",
      "------------------------------------\n",
      "Num timesteps: 4091000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.82\n",
      "Num timesteps: 4092000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.98\n",
      "Num timesteps: 4093000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 159.86\n",
      "Num timesteps: 4094000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.55\n",
      "Num timesteps: 4095000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.72\n",
      "Num timesteps: 4096000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.96\n",
      "Num timesteps: 4097000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.88\n",
      "Num timesteps: 4098000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.58\n",
      "Num timesteps: 4099000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.26\n",
      "Num timesteps: 4100000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 243      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1302     |\n",
      "|    iterations         | 41000    |\n",
      "|    time_elapsed       | 3148     |\n",
      "|    total_timesteps    | 4100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | -5.46    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 40999    |\n",
      "|    policy_loss        | 9.95     |\n",
      "|    value_loss         | 395      |\n",
      "------------------------------------\n",
      "Num timesteps: 4101000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.18\n",
      "Num timesteps: 4102000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.04\n",
      "Num timesteps: 4103000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.93\n",
      "Num timesteps: 4104000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 167.29\n",
      "Num timesteps: 4105000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 168.11\n",
      "Num timesteps: 4106000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.42\n",
      "Num timesteps: 4107000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.27\n",
      "Num timesteps: 4108000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.53\n",
      "Num timesteps: 4109000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.00\n",
      "Num timesteps: 4110000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 228      |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 41100    |\n",
      "|    time_elapsed       | 3153     |\n",
      "|    total_timesteps    | 4110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | -0.514   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41099    |\n",
      "|    policy_loss        | 2.82     |\n",
      "|    value_loss         | 786      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4111000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 180.62\n",
      "Num timesteps: 4112000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.81\n",
      "Num timesteps: 4113000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.47\n",
      "Num timesteps: 4114000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.81\n",
      "Num timesteps: 4115000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 184.57\n",
      "Num timesteps: 4116000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 186.00\n",
      "Num timesteps: 4117000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 189.32\n",
      "Num timesteps: 4118000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 187.67\n",
      "Num timesteps: 4119000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 184.82\n",
      "Num timesteps: 4120000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 234      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 41200    |\n",
      "|    time_elapsed       | 3158     |\n",
      "|    total_timesteps    | 4120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41199    |\n",
      "|    policy_loss        | -1.47    |\n",
      "|    value_loss         | 48.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 4121000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.03\n",
      "Num timesteps: 4122000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 180.45\n",
      "Num timesteps: 4123000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 180.21\n",
      "Num timesteps: 4124000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.95\n",
      "Num timesteps: 4125000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.50\n",
      "Num timesteps: 4126000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 181.91\n",
      "Num timesteps: 4127000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 183.70\n",
      "Num timesteps: 4128000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.39\n",
      "Num timesteps: 4129000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.48\n",
      "Num timesteps: 4130000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 169.13\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 249      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 41300    |\n",
      "|    time_elapsed       | 3165     |\n",
      "|    total_timesteps    | 4130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | -1.07    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41299    |\n",
      "|    policy_loss        | 4.23     |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "Num timesteps: 4131000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.09\n",
      "Num timesteps: 4132000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.19\n",
      "Num timesteps: 4133000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.48\n",
      "Num timesteps: 4134000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.53\n",
      "Num timesteps: 4135000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.71\n",
      "Num timesteps: 4136000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.41\n",
      "Num timesteps: 4137000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.27\n",
      "Num timesteps: 4138000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.74\n",
      "Num timesteps: 4139000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.48\n",
      "Num timesteps: 4140000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 259      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 41400    |\n",
      "|    time_elapsed       | 3171     |\n",
      "|    total_timesteps    | 4140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.617   |\n",
      "|    explained_variance | 0.0117   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41399    |\n",
      "|    policy_loss        | -2.72    |\n",
      "|    value_loss         | 19.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 4141000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.71\n",
      "Num timesteps: 4142000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.25\n",
      "Num timesteps: 4143000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.91\n",
      "Num timesteps: 4144000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.83\n",
      "Num timesteps: 4145000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.07\n",
      "Num timesteps: 4146000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.54\n",
      "Num timesteps: 4147000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.02\n",
      "Num timesteps: 4148000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.50\n",
      "Num timesteps: 4149000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 166.38\n",
      "Num timesteps: 4150000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 41500    |\n",
      "|    time_elapsed       | 3179     |\n",
      "|    total_timesteps    | 4150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.728   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41499    |\n",
      "|    policy_loss        | 0.311    |\n",
      "|    value_loss         | 4.85     |\n",
      "------------------------------------\n",
      "Num timesteps: 4151000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.14\n",
      "Num timesteps: 4152000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.22\n",
      "Num timesteps: 4153000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.67\n",
      "Num timesteps: 4154000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 167.51\n",
      "Num timesteps: 4155000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 168.27\n",
      "Num timesteps: 4156000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.88\n",
      "Num timesteps: 4157000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.56\n",
      "Num timesteps: 4158000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.99\n",
      "Num timesteps: 4159000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 177.16\n",
      "Num timesteps: 4160000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 176.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 296      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 41600    |\n",
      "|    time_elapsed       | 3185     |\n",
      "|    total_timesteps    | 4160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.734   |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41599    |\n",
      "|    policy_loss        | 18.2     |\n",
      "|    value_loss         | 753      |\n",
      "------------------------------------\n",
      "Num timesteps: 4161000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.84\n",
      "Num timesteps: 4162000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.22\n",
      "Num timesteps: 4163000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.06\n",
      "Num timesteps: 4164000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.18\n",
      "Num timesteps: 4165000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.54\n",
      "Num timesteps: 4166000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.32\n",
      "Num timesteps: 4167000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.84\n",
      "Num timesteps: 4168000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.31\n",
      "Num timesteps: 4169000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 172.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4170000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 175.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 41700    |\n",
      "|    time_elapsed       | 3191     |\n",
      "|    total_timesteps    | 4170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41699    |\n",
      "|    policy_loss        | 2.22     |\n",
      "|    value_loss         | 120      |\n",
      "------------------------------------\n",
      "Num timesteps: 4171000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.92\n",
      "Num timesteps: 4172000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.19\n",
      "Num timesteps: 4173000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 172.96\n",
      "Num timesteps: 4174000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 172.56\n",
      "Num timesteps: 4175000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 173.54\n",
      "Num timesteps: 4176000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.24\n",
      "Num timesteps: 4177000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 170.20\n",
      "Num timesteps: 4178000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.12\n",
      "Num timesteps: 4179000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.02\n",
      "Num timesteps: 4180000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 171.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 41800    |\n",
      "|    time_elapsed       | 3199     |\n",
      "|    total_timesteps    | 4180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | 0.0206   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41799    |\n",
      "|    policy_loss        | -2.81    |\n",
      "|    value_loss         | 21       |\n",
      "------------------------------------\n",
      "Num timesteps: 4181000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 170.07\n",
      "Num timesteps: 4182000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 172.81\n",
      "Num timesteps: 4183000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.32\n",
      "Num timesteps: 4184000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 157.49\n",
      "Num timesteps: 4185000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 154.82\n",
      "Num timesteps: 4186000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.80\n",
      "Num timesteps: 4187000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 153.45\n",
      "Num timesteps: 4188000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.88\n",
      "Num timesteps: 4189000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 141.04\n",
      "Num timesteps: 4190000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.19\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 263      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 41900    |\n",
      "|    time_elapsed       | 3204     |\n",
      "|    total_timesteps    | 4190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | -15.5    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41899    |\n",
      "|    policy_loss        | 12.8     |\n",
      "|    value_loss         | 2.62e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4191000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.85\n",
      "Num timesteps: 4192000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.26\n",
      "Num timesteps: 4193000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 148.27\n",
      "Num timesteps: 4194000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.46\n",
      "Num timesteps: 4195000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 145.84\n",
      "Num timesteps: 4196000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 146.20\n",
      "Num timesteps: 4197000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 144.72\n",
      "Num timesteps: 4198000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.62\n",
      "Num timesteps: 4199000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 147.58\n",
      "Num timesteps: 4200000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 148.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 206      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 42000    |\n",
      "|    time_elapsed       | 3209     |\n",
      "|    total_timesteps    | 4200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | -0.552   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 41999    |\n",
      "|    policy_loss        | 16.5     |\n",
      "|    value_loss         | 1.61e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4201000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 150.17\n",
      "Num timesteps: 4202000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.59\n",
      "Num timesteps: 4203000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 149.30\n",
      "Num timesteps: 4204000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 153.96\n",
      "Num timesteps: 4205000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 155.86\n",
      "Num timesteps: 4206000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 158.68\n",
      "Num timesteps: 4207000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.88\n",
      "Num timesteps: 4208000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 155.46\n",
      "Num timesteps: 4209000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 162.18\n",
      "Num timesteps: 4210000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 161.98\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 42100    |\n",
      "|    time_elapsed       | 3214     |\n",
      "|    total_timesteps    | 4210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42099    |\n",
      "|    policy_loss        | 5.71     |\n",
      "|    value_loss         | 150      |\n",
      "------------------------------------\n",
      "Num timesteps: 4211000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.64\n",
      "Num timesteps: 4212000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 168.47\n",
      "Num timesteps: 4213000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.14\n",
      "Num timesteps: 4214000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 163.16\n",
      "Num timesteps: 4215000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 164.36\n",
      "Num timesteps: 4216000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 165.67\n",
      "Num timesteps: 4217000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 168.65\n",
      "Num timesteps: 4218000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 170.80\n",
      "Num timesteps: 4219000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 174.24\n",
      "Num timesteps: 4220000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 179.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 245      |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 42200    |\n",
      "|    time_elapsed       | 3220     |\n",
      "|    total_timesteps    | 4220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.482   |\n",
      "|    explained_variance | -1.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42199    |\n",
      "|    policy_loss        | 21.4     |\n",
      "|    value_loss         | 2.13e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4221000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.15\n",
      "Num timesteps: 4222000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.11\n",
      "Num timesteps: 4223000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 178.43\n",
      "Num timesteps: 4224000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 180.44\n",
      "Num timesteps: 4225000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 182.83\n",
      "Num timesteps: 4226000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 185.27\n",
      "Num timesteps: 4227000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 190.09\n",
      "Num timesteps: 4228000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 191.44\n",
      "Num timesteps: 4229000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 190.77\n",
      "Num timesteps: 4230000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 192.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 265      |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 42300    |\n",
      "|    time_elapsed       | 3226     |\n",
      "|    total_timesteps    | 4230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.203   |\n",
      "|    explained_variance | -10.7    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42299    |\n",
      "|    policy_loss        | 1.96     |\n",
      "|    value_loss         | 161      |\n",
      "------------------------------------\n",
      "Num timesteps: 4231000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 192.47\n",
      "Num timesteps: 4232000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 193.62\n",
      "Num timesteps: 4233000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 193.68\n",
      "Num timesteps: 4234000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 191.94\n",
      "Num timesteps: 4235000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 192.24\n",
      "Num timesteps: 4236000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 191.95\n",
      "Num timesteps: 4237000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 196.51\n",
      "Num timesteps: 4238000\n",
      "Best mean reward: 199.31 - Last mean reward per episode: 200.03\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4239000\n",
      "Best mean reward: 200.03 - Last mean reward per episode: 202.67\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4240000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 197.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 293      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 42400    |\n",
      "|    time_elapsed       | 3232     |\n",
      "|    total_timesteps    | 4240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42399    |\n",
      "|    policy_loss        | -1.47    |\n",
      "|    value_loss         | 363      |\n",
      "------------------------------------\n",
      "Num timesteps: 4241000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 192.61\n",
      "Num timesteps: 4242000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 191.38\n",
      "Num timesteps: 4243000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 193.80\n",
      "Num timesteps: 4244000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 197.06\n",
      "Num timesteps: 4245000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 193.72\n",
      "Num timesteps: 4246000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 194.15\n",
      "Num timesteps: 4247000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 191.27\n",
      "Num timesteps: 4248000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 193.79\n",
      "Num timesteps: 4249000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 188.76\n",
      "Num timesteps: 4250000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 183.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 42500    |\n",
      "|    time_elapsed       | 3238     |\n",
      "|    total_timesteps    | 4250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.401   |\n",
      "|    explained_variance | 0.669    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42499    |\n",
      "|    policy_loss        | -30.8    |\n",
      "|    value_loss         | 4.99e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4251000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 181.46\n",
      "Num timesteps: 4252000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 182.30\n",
      "Num timesteps: 4253000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 180.97\n",
      "Num timesteps: 4254000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 177.10\n",
      "Num timesteps: 4255000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 176.30\n",
      "Num timesteps: 4256000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.45\n",
      "Num timesteps: 4257000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 175.82\n",
      "Num timesteps: 4258000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.43\n",
      "Num timesteps: 4259000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.96\n",
      "Num timesteps: 4260000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 278      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 42600    |\n",
      "|    time_elapsed       | 3244     |\n",
      "|    total_timesteps    | 4260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.596    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42599    |\n",
      "|    policy_loss        | 2.4      |\n",
      "|    value_loss         | 14.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 4261000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.98\n",
      "Num timesteps: 4262000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 176.34\n",
      "Num timesteps: 4263000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.14\n",
      "Num timesteps: 4264000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.86\n",
      "Num timesteps: 4265000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.47\n",
      "Num timesteps: 4266000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 170.65\n",
      "Num timesteps: 4267000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.55\n",
      "Num timesteps: 4268000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.33\n",
      "Num timesteps: 4269000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.64\n",
      "Num timesteps: 4270000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 280      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 42700    |\n",
      "|    time_elapsed       | 3250     |\n",
      "|    total_timesteps    | 4270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.358   |\n",
      "|    explained_variance | -10.8    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42699    |\n",
      "|    policy_loss        | 6.99     |\n",
      "|    value_loss         | 3.18e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4271000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.20\n",
      "Num timesteps: 4272000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 170.29\n",
      "Num timesteps: 4273000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.02\n",
      "Num timesteps: 4274000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.66\n",
      "Num timesteps: 4275000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.60\n",
      "Num timesteps: 4276000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.91\n",
      "Num timesteps: 4277000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4278000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.65\n",
      "Num timesteps: 4279000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.75\n",
      "Num timesteps: 4280000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 42800    |\n",
      "|    time_elapsed       | 3256     |\n",
      "|    total_timesteps    | 4280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.203   |\n",
      "|    explained_variance | -3.33    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42799    |\n",
      "|    policy_loss        | 6.02     |\n",
      "|    value_loss         | 3.61e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4281000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.61\n",
      "Num timesteps: 4282000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.95\n",
      "Num timesteps: 4283000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.01\n",
      "Num timesteps: 4284000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.11\n",
      "Num timesteps: 4285000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 169.36\n",
      "Num timesteps: 4286000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 170.63\n",
      "Num timesteps: 4287000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 169.40\n",
      "Num timesteps: 4288000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.76\n",
      "Num timesteps: 4289000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.76\n",
      "Num timesteps: 4290000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 301      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 42900    |\n",
      "|    time_elapsed       | 3264     |\n",
      "|    total_timesteps    | 4290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.457   |\n",
      "|    explained_variance | 0.184    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42899    |\n",
      "|    policy_loss        | -1.71    |\n",
      "|    value_loss         | 68.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 4291000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.78\n",
      "Num timesteps: 4292000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.56\n",
      "Num timesteps: 4293000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.35\n",
      "Num timesteps: 4294000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.44\n",
      "Num timesteps: 4295000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.93\n",
      "Num timesteps: 4296000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.23\n",
      "Num timesteps: 4297000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.78\n",
      "Num timesteps: 4298000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.26\n",
      "Num timesteps: 4299000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.84\n",
      "Num timesteps: 4300000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.40\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 43000    |\n",
      "|    time_elapsed       | 3271     |\n",
      "|    total_timesteps    | 4300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.502    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 42999    |\n",
      "|    policy_loss        | -10.7    |\n",
      "|    value_loss         | 123      |\n",
      "------------------------------------\n",
      "Num timesteps: 4301000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.77\n",
      "Num timesteps: 4302000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.45\n",
      "Num timesteps: 4303000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.01\n",
      "Num timesteps: 4304000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.18\n",
      "Num timesteps: 4305000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.00\n",
      "Num timesteps: 4306000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.75\n",
      "Num timesteps: 4307000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.61\n",
      "Num timesteps: 4308000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 4309000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 170.54\n",
      "Num timesteps: 4310000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 325      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 43100    |\n",
      "|    time_elapsed       | 3279     |\n",
      "|    total_timesteps    | 4310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.938   |\n",
      "|    explained_variance | 0.153    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43099    |\n",
      "|    policy_loss        | 3.84     |\n",
      "|    value_loss         | 48.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 4311000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.13\n",
      "Num timesteps: 4312000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 169.38\n",
      "Num timesteps: 4313000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.20\n",
      "Num timesteps: 4314000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.02\n",
      "Num timesteps: 4315000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.36\n",
      "Num timesteps: 4316000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.30\n",
      "Num timesteps: 4317000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.77\n",
      "Num timesteps: 4318000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.75\n",
      "Num timesteps: 4319000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.18\n",
      "Num timesteps: 4320000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 349      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 43200    |\n",
      "|    time_elapsed       | 3288     |\n",
      "|    total_timesteps    | 4320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | -0.358   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43199    |\n",
      "|    policy_loss        | -0.251   |\n",
      "|    value_loss         | 0.957    |\n",
      "------------------------------------\n",
      "Num timesteps: 4321000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.67\n",
      "Num timesteps: 4322000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.68\n",
      "Num timesteps: 4323000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.78\n",
      "Num timesteps: 4324000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.09\n",
      "Num timesteps: 4325000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.53\n",
      "Num timesteps: 4326000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.43\n",
      "Num timesteps: 4327000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.36\n",
      "Num timesteps: 4328000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.98\n",
      "Num timesteps: 4329000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.43\n",
      "Num timesteps: 4330000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 156.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 43300    |\n",
      "|    time_elapsed       | 3299     |\n",
      "|    total_timesteps    | 4330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.981   |\n",
      "|    explained_variance | 0.883    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43299    |\n",
      "|    policy_loss        | -3.78    |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4331000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.74\n",
      "Num timesteps: 4332000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.83\n",
      "Num timesteps: 4333000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.44\n",
      "Num timesteps: 4334000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.24\n",
      "Num timesteps: 4335000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.70\n",
      "Num timesteps: 4336000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.85\n",
      "Num timesteps: 4337000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.49\n",
      "Num timesteps: 4338000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.52\n",
      "Num timesteps: 4339000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.52\n",
      "Num timesteps: 4340000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 393      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 43400    |\n",
      "|    time_elapsed       | 3308     |\n",
      "|    total_timesteps    | 4340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.808   |\n",
      "|    explained_variance | -2.96    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43399    |\n",
      "|    policy_loss        | -4.02    |\n",
      "|    value_loss         | 98       |\n",
      "------------------------------------\n",
      "Num timesteps: 4341000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.14\n",
      "Num timesteps: 4342000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.74\n",
      "Num timesteps: 4343000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.37\n",
      "Num timesteps: 4344000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 150.41\n",
      "Num timesteps: 4345000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.11\n",
      "Num timesteps: 4346000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.54\n",
      "Num timesteps: 4347000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 154.18\n",
      "Num timesteps: 4348000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.15\n",
      "Num timesteps: 4349000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 150.86\n",
      "Num timesteps: 4350000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 403      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 43500    |\n",
      "|    time_elapsed       | 3315     |\n",
      "|    total_timesteps    | 4350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.331   |\n",
      "|    explained_variance | -1.74    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43499    |\n",
      "|    policy_loss        | 1.8      |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n",
      "Num timesteps: 4351000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.20\n",
      "Num timesteps: 4352000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.25\n",
      "Num timesteps: 4353000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.99\n",
      "Num timesteps: 4354000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.20\n",
      "Num timesteps: 4355000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.06\n",
      "Num timesteps: 4356000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 154.58\n",
      "Num timesteps: 4357000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.25\n",
      "Num timesteps: 4358000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 153.33\n",
      "Num timesteps: 4359000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.50\n",
      "Num timesteps: 4360000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 388      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 43600    |\n",
      "|    time_elapsed       | 3322     |\n",
      "|    total_timesteps    | 4360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.79    |\n",
      "|    explained_variance | 0.882    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43599    |\n",
      "|    policy_loss        | -0.301   |\n",
      "|    value_loss         | 14.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 4361000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.94\n",
      "Num timesteps: 4362000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.70\n",
      "Num timesteps: 4363000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.14\n",
      "Num timesteps: 4364000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.79\n",
      "Num timesteps: 4365000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.00\n",
      "Num timesteps: 4366000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.88\n",
      "Num timesteps: 4367000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.82\n",
      "Num timesteps: 4368000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.35\n",
      "Num timesteps: 4369000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.59\n",
      "Num timesteps: 4370000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 163.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 383      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 43700    |\n",
      "|    time_elapsed       | 3331     |\n",
      "|    total_timesteps    | 4370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.828   |\n",
      "|    explained_variance | 0.905    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43699    |\n",
      "|    policy_loss        | -3.79    |\n",
      "|    value_loss         | 29.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 4371000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.95\n",
      "Num timesteps: 4372000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.05\n",
      "Num timesteps: 4373000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.77\n",
      "Num timesteps: 4374000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.97\n",
      "Num timesteps: 4375000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.52\n",
      "Num timesteps: 4376000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 164.16\n",
      "Num timesteps: 4377000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.54\n",
      "Num timesteps: 4378000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.52\n",
      "Num timesteps: 4379000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.90\n",
      "Num timesteps: 4380000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 379      |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 43800    |\n",
      "|    time_elapsed       | 3339     |\n",
      "|    total_timesteps    | 4380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.811   |\n",
      "|    explained_variance | -1.18    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43799    |\n",
      "|    policy_loss        | 11.1     |\n",
      "|    value_loss         | 502      |\n",
      "------------------------------------\n",
      "Num timesteps: 4381000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 176.58\n",
      "Num timesteps: 4382000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.91\n",
      "Num timesteps: 4383000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 176.11\n",
      "Num timesteps: 4384000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.03\n",
      "Num timesteps: 4385000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 176.77\n",
      "Num timesteps: 4386000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 174.10\n",
      "Num timesteps: 4387000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 175.57\n",
      "Num timesteps: 4388000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.11\n",
      "Num timesteps: 4389000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 171.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4390000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 375      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 43900    |\n",
      "|    time_elapsed       | 3347     |\n",
      "|    total_timesteps    | 4390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.457   |\n",
      "|    explained_variance | -0.0339  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43899    |\n",
      "|    policy_loss        | -0.464   |\n",
      "|    value_loss         | 0.827    |\n",
      "------------------------------------\n",
      "Num timesteps: 4391000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.19\n",
      "Num timesteps: 4392000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.65\n",
      "Num timesteps: 4393000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 166.55\n",
      "Num timesteps: 4394000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 165.71\n",
      "Num timesteps: 4395000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 162.60\n",
      "Num timesteps: 4396000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.60\n",
      "Num timesteps: 4397000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.87\n",
      "Num timesteps: 4398000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.72\n",
      "Num timesteps: 4399000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 154.79\n",
      "Num timesteps: 4400000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44000    |\n",
      "|    time_elapsed       | 3355     |\n",
      "|    total_timesteps    | 4400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.96    |\n",
      "|    explained_variance | 0.581    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 43999    |\n",
      "|    policy_loss        | -2.87    |\n",
      "|    value_loss         | 12.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 4401000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 154.16\n",
      "Num timesteps: 4402000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.65\n",
      "Num timesteps: 4403000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 150.77\n",
      "Num timesteps: 4404000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 156.48\n",
      "Num timesteps: 4405000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.68\n",
      "Num timesteps: 4406000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.48\n",
      "Num timesteps: 4407000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 154.24\n",
      "Num timesteps: 4408000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.47\n",
      "Num timesteps: 4409000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 156.28\n",
      "Num timesteps: 4410000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 44100    |\n",
      "|    time_elapsed       | 3363     |\n",
      "|    total_timesteps    | 4410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.898   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44099    |\n",
      "|    policy_loss        | -3.44    |\n",
      "|    value_loss         | 17.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 4411000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.41\n",
      "Num timesteps: 4412000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.98\n",
      "Num timesteps: 4413000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 159.92\n",
      "Num timesteps: 4414000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.61\n",
      "Num timesteps: 4415000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 160.20\n",
      "Num timesteps: 4416000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.80\n",
      "Num timesteps: 4417000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 157.47\n",
      "Num timesteps: 4418000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.69\n",
      "Num timesteps: 4419000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.56\n",
      "Num timesteps: 4420000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 412      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 44200    |\n",
      "|    time_elapsed       | 3371     |\n",
      "|    total_timesteps    | 4420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | 0.0947   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44199    |\n",
      "|    policy_loss        | -0.0728  |\n",
      "|    value_loss         | 0.0933   |\n",
      "------------------------------------\n",
      "Num timesteps: 4421000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.53\n",
      "Num timesteps: 4422000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.92\n",
      "Num timesteps: 4423000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.83\n",
      "Num timesteps: 4424000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.01\n",
      "Num timesteps: 4425000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.90\n",
      "Num timesteps: 4426000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 151.23\n",
      "Num timesteps: 4427000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 152.30\n",
      "Num timesteps: 4428000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 156.89\n",
      "Num timesteps: 4429000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.91\n",
      "Num timesteps: 4430000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44300    |\n",
      "|    time_elapsed       | 3378     |\n",
      "|    total_timesteps    | 4430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.456   |\n",
      "|    explained_variance | 0.611    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44299    |\n",
      "|    policy_loss        | -0.647   |\n",
      "|    value_loss         | 1.8      |\n",
      "------------------------------------\n",
      "Num timesteps: 4431000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 155.77\n",
      "Num timesteps: 4432000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 158.73\n",
      "Num timesteps: 4433000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.37\n",
      "Num timesteps: 4434000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 161.38\n",
      "Num timesteps: 4435000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 168.16\n",
      "Num timesteps: 4436000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 167.56\n",
      "Num timesteps: 4437000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 172.45\n",
      "Num timesteps: 4438000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 175.21\n",
      "Num timesteps: 4439000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 173.88\n",
      "Num timesteps: 4440000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 179.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 403      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44400    |\n",
      "|    time_elapsed       | 3386     |\n",
      "|    total_timesteps    | 4440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.795   |\n",
      "|    explained_variance | -1.09    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44399    |\n",
      "|    policy_loss        | 16.1     |\n",
      "|    value_loss         | 415      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4441000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 179.44\n",
      "Num timesteps: 4442000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 178.12\n",
      "Num timesteps: 4443000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 181.37\n",
      "Num timesteps: 4444000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 177.19\n",
      "Num timesteps: 4445000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 181.05\n",
      "Num timesteps: 4446000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 179.99\n",
      "Num timesteps: 4447000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 177.85\n",
      "Num timesteps: 4448000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 180.30\n",
      "Num timesteps: 4449000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 181.98\n",
      "Num timesteps: 4450000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 178.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44500    |\n",
      "|    time_elapsed       | 3392     |\n",
      "|    total_timesteps    | 4450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.541   |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44499    |\n",
      "|    policy_loss        | 8.24     |\n",
      "|    value_loss         | 405      |\n",
      "------------------------------------\n",
      "Num timesteps: 4451000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 178.31\n",
      "Num timesteps: 4452000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 179.98\n",
      "Num timesteps: 4453000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 181.42\n",
      "Num timesteps: 4454000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 185.12\n",
      "Num timesteps: 4455000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 185.11\n",
      "Num timesteps: 4456000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 188.62\n",
      "Num timesteps: 4457000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 187.83\n",
      "Num timesteps: 4458000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 190.76\n",
      "Num timesteps: 4459000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 194.05\n",
      "Num timesteps: 4460000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 196.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 374      |\n",
      "|    ep_rew_mean        | 197      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44600    |\n",
      "|    time_elapsed       | 3400     |\n",
      "|    total_timesteps    | 4460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.648   |\n",
      "|    explained_variance | -0.0451  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44599    |\n",
      "|    policy_loss        | -1.86    |\n",
      "|    value_loss         | 9.47     |\n",
      "------------------------------------\n",
      "Num timesteps: 4461000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 200.45\n",
      "Num timesteps: 4462000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 198.21\n",
      "Num timesteps: 4463000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 201.88\n",
      "Num timesteps: 4464000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 200.56\n",
      "Num timesteps: 4465000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 200.50\n",
      "Num timesteps: 4466000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 200.60\n",
      "Num timesteps: 4467000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 199.57\n",
      "Num timesteps: 4468000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 200.33\n",
      "Num timesteps: 4469000\n",
      "Best mean reward: 202.67 - Last mean reward per episode: 203.31\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4470000\n",
      "Best mean reward: 203.31 - Last mean reward per episode: 205.03\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 382      |\n",
      "|    ep_rew_mean        | 205      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44700    |\n",
      "|    time_elapsed       | 3407     |\n",
      "|    total_timesteps    | 4470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | -0.128   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44699    |\n",
      "|    policy_loss        | 5.31     |\n",
      "|    value_loss         | 121      |\n",
      "------------------------------------\n",
      "Num timesteps: 4471000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 203.42\n",
      "Num timesteps: 4472000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 200.70\n",
      "Num timesteps: 4473000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 199.30\n",
      "Num timesteps: 4474000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.99\n",
      "Num timesteps: 4475000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.68\n",
      "Num timesteps: 4476000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.05\n",
      "Num timesteps: 4477000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.36\n",
      "Num timesteps: 4478000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.49\n",
      "Num timesteps: 4479000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.52\n",
      "Num timesteps: 4480000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 384      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44800    |\n",
      "|    time_elapsed       | 3416     |\n",
      "|    total_timesteps    | 4480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.871   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44799    |\n",
      "|    policy_loss        | 0.523    |\n",
      "|    value_loss         | 7.84     |\n",
      "------------------------------------\n",
      "Num timesteps: 4481000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.19\n",
      "Num timesteps: 4482000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.38\n",
      "Num timesteps: 4483000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.91\n",
      "Num timesteps: 4484000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.97\n",
      "Num timesteps: 4485000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.82\n",
      "Num timesteps: 4486000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.73\n",
      "Num timesteps: 4487000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.29\n",
      "Num timesteps: 4488000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.18\n",
      "Num timesteps: 4489000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.38\n",
      "Num timesteps: 4490000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 396      |\n",
      "|    ep_rew_mean        | 199      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 44900    |\n",
      "|    time_elapsed       | 3422     |\n",
      "|    total_timesteps    | 4490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | -0.00485 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44899    |\n",
      "|    policy_loss        | -1.96    |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 4491000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 199.51\n",
      "Num timesteps: 4492000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.67\n",
      "Num timesteps: 4493000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.76\n",
      "Num timesteps: 4494000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.09\n",
      "Num timesteps: 4495000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.24\n",
      "Num timesteps: 4496000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.00\n",
      "Num timesteps: 4497000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4498000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.09\n",
      "Num timesteps: 4499000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.37\n",
      "Num timesteps: 4500000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 382      |\n",
      "|    ep_rew_mean        | 196      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 45000    |\n",
      "|    time_elapsed       | 3430     |\n",
      "|    total_timesteps    | 4500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.906   |\n",
      "|    explained_variance | -0.183   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 44999    |\n",
      "|    policy_loss        | -10.5    |\n",
      "|    value_loss         | 149      |\n",
      "------------------------------------\n",
      "Num timesteps: 4501000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.15\n",
      "Num timesteps: 4502000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.64\n",
      "Num timesteps: 4503000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.37\n",
      "Num timesteps: 4504000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.85\n",
      "Num timesteps: 4505000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.57\n",
      "Num timesteps: 4506000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.63\n",
      "Num timesteps: 4507000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.30\n",
      "Num timesteps: 4508000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 191.82\n",
      "Num timesteps: 4509000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.79\n",
      "Num timesteps: 4510000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 411      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 45100    |\n",
      "|    time_elapsed       | 3438     |\n",
      "|    total_timesteps    | 4510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45099    |\n",
      "|    policy_loss        | 7.16     |\n",
      "|    value_loss         | 226      |\n",
      "------------------------------------\n",
      "Num timesteps: 4511000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 191.37\n",
      "Num timesteps: 4512000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 188.86\n",
      "Num timesteps: 4513000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.29\n",
      "Num timesteps: 4514000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.46\n",
      "Num timesteps: 4515000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.03\n",
      "Num timesteps: 4516000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.32\n",
      "Num timesteps: 4517000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.33\n",
      "Num timesteps: 4518000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.80\n",
      "Num timesteps: 4519000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.23\n",
      "Num timesteps: 4520000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 45200    |\n",
      "|    time_elapsed       | 3446     |\n",
      "|    total_timesteps    | 4520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.731   |\n",
      "|    explained_variance | -0.364   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45199    |\n",
      "|    policy_loss        | -0.283   |\n",
      "|    value_loss         | 676      |\n",
      "------------------------------------\n",
      "Num timesteps: 4521000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.44\n",
      "Num timesteps: 4522000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.74\n",
      "Num timesteps: 4523000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 190.22\n",
      "Num timesteps: 4524000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 188.84\n",
      "Num timesteps: 4525000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 191.56\n",
      "Num timesteps: 4526000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 187.82\n",
      "Num timesteps: 4527000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 189.06\n",
      "Num timesteps: 4528000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 191.94\n",
      "Num timesteps: 4529000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.17\n",
      "Num timesteps: 4530000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 392      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 45300    |\n",
      "|    time_elapsed       | 3453     |\n",
      "|    total_timesteps    | 4530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45299    |\n",
      "|    policy_loss        | -0.467   |\n",
      "|    value_loss         | 2.9      |\n",
      "------------------------------------\n",
      "Num timesteps: 4531000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.43\n",
      "Num timesteps: 4532000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.33\n",
      "Num timesteps: 4533000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.57\n",
      "Num timesteps: 4534000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.30\n",
      "Num timesteps: 4535000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 194.25\n",
      "Num timesteps: 4536000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.13\n",
      "Num timesteps: 4537000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.28\n",
      "Num timesteps: 4538000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.25\n",
      "Num timesteps: 4539000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 199.75\n",
      "Num timesteps: 4540000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 200.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 359      |\n",
      "|    ep_rew_mean        | 200      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 45400    |\n",
      "|    time_elapsed       | 3459     |\n",
      "|    total_timesteps    | 4540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.947   |\n",
      "|    explained_variance | 0.22     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45399    |\n",
      "|    policy_loss        | -9.13    |\n",
      "|    value_loss         | 157      |\n",
      "------------------------------------\n",
      "Num timesteps: 4541000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.34\n",
      "Num timesteps: 4542000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.44\n",
      "Num timesteps: 4543000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 193.53\n",
      "Num timesteps: 4544000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.32\n",
      "Num timesteps: 4545000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.84\n",
      "Num timesteps: 4546000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 197.69\n",
      "Num timesteps: 4547000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.28\n",
      "Num timesteps: 4548000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 196.36\n",
      "Num timesteps: 4549000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 192.54\n",
      "Num timesteps: 4550000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 195.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 45500    |\n",
      "|    time_elapsed       | 3467     |\n",
      "|    total_timesteps    | 4550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.625   |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45499    |\n",
      "|    policy_loss        | 0.000914 |\n",
      "|    value_loss         | 122      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4551000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 198.88\n",
      "Num timesteps: 4552000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 201.39\n",
      "Num timesteps: 4553000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 201.96\n",
      "Num timesteps: 4554000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 201.74\n",
      "Num timesteps: 4555000\n",
      "Best mean reward: 205.03 - Last mean reward per episode: 206.51\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4556000\n",
      "Best mean reward: 206.51 - Last mean reward per episode: 206.20\n",
      "Num timesteps: 4557000\n",
      "Best mean reward: 206.51 - Last mean reward per episode: 210.73\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4558000\n",
      "Best mean reward: 210.73 - Last mean reward per episode: 210.43\n",
      "Num timesteps: 4559000\n",
      "Best mean reward: 210.73 - Last mean reward per episode: 213.10\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4560000\n",
      "Best mean reward: 213.10 - Last mean reward per episode: 211.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 343      |\n",
      "|    ep_rew_mean        | 212      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 45600    |\n",
      "|    time_elapsed       | 3474     |\n",
      "|    total_timesteps    | 4560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45599    |\n",
      "|    policy_loss        | 1.6      |\n",
      "|    value_loss         | 16.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 4561000\n",
      "Best mean reward: 213.10 - Last mean reward per episode: 212.73\n",
      "Num timesteps: 4562000\n",
      "Best mean reward: 213.10 - Last mean reward per episode: 213.17\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4563000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 208.06\n",
      "Num timesteps: 4564000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 207.20\n",
      "Num timesteps: 4565000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 206.94\n",
      "Num timesteps: 4566000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.20\n",
      "Num timesteps: 4567000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.31\n",
      "Num timesteps: 4568000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.63\n",
      "Num timesteps: 4569000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 202.72\n",
      "Num timesteps: 4570000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 204.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 204      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 45700    |\n",
      "|    time_elapsed       | 3482     |\n",
      "|    total_timesteps    | 4570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.855   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45699    |\n",
      "|    policy_loss        | -1.24    |\n",
      "|    value_loss         | 5.86     |\n",
      "------------------------------------\n",
      "Num timesteps: 4571000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 204.35\n",
      "Num timesteps: 4572000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 205.89\n",
      "Num timesteps: 4573000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.79\n",
      "Num timesteps: 4574000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 205.86\n",
      "Num timesteps: 4575000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.03\n",
      "Num timesteps: 4576000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 205.37\n",
      "Num timesteps: 4577000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 203.81\n",
      "Num timesteps: 4578000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 206.03\n",
      "Num timesteps: 4579000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 206.14\n",
      "Num timesteps: 4580000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 202.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 358      |\n",
      "|    ep_rew_mean        | 203      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 45800    |\n",
      "|    time_elapsed       | 3489     |\n",
      "|    total_timesteps    | 4580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.758   |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45799    |\n",
      "|    policy_loss        | 0.292    |\n",
      "|    value_loss         | 7.66     |\n",
      "------------------------------------\n",
      "Num timesteps: 4581000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 200.57\n",
      "Num timesteps: 4582000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 199.71\n",
      "Num timesteps: 4583000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 200.05\n",
      "Num timesteps: 4584000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 200.34\n",
      "Num timesteps: 4585000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 199.60\n",
      "Num timesteps: 4586000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.53\n",
      "Num timesteps: 4587000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 196.27\n",
      "Num timesteps: 4588000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 196.07\n",
      "Num timesteps: 4589000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.40\n",
      "Num timesteps: 4590000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 45900    |\n",
      "|    time_elapsed       | 3495     |\n",
      "|    total_timesteps    | 4590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | -1.98    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45899    |\n",
      "|    policy_loss        | -1.12    |\n",
      "|    value_loss         | 10.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 4591000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.55\n",
      "Num timesteps: 4592000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.06\n",
      "Num timesteps: 4593000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.05\n",
      "Num timesteps: 4594000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.83\n",
      "Num timesteps: 4595000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.61\n",
      "Num timesteps: 4596000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.55\n",
      "Num timesteps: 4597000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.14\n",
      "Num timesteps: 4598000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.71\n",
      "Num timesteps: 4599000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.46\n",
      "Num timesteps: 4600000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 46000    |\n",
      "|    time_elapsed       | 3503     |\n",
      "|    total_timesteps    | 4600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0.437    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 45999    |\n",
      "|    policy_loss        | 0.209    |\n",
      "|    value_loss         | 16.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 4601000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.05\n",
      "Num timesteps: 4602000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.90\n",
      "Num timesteps: 4603000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.71\n",
      "Num timesteps: 4604000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.49\n",
      "Num timesteps: 4605000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.94\n",
      "Num timesteps: 4606000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4607000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.16\n",
      "Num timesteps: 4608000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.07\n",
      "Num timesteps: 4609000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.84\n",
      "Num timesteps: 4610000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 351      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 46100    |\n",
      "|    time_elapsed       | 3510     |\n",
      "|    total_timesteps    | 4610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.967   |\n",
      "|    explained_variance | 0.318    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46099    |\n",
      "|    policy_loss        | -5.36    |\n",
      "|    value_loss         | 47.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 4611000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.78\n",
      "Num timesteps: 4612000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.51\n",
      "Num timesteps: 4613000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.90\n",
      "Num timesteps: 4614000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.62\n",
      "Num timesteps: 4615000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.94\n",
      "Num timesteps: 4616000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.66\n",
      "Num timesteps: 4617000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.42\n",
      "Num timesteps: 4618000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.93\n",
      "Num timesteps: 4619000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 197.41\n",
      "Num timesteps: 4620000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 198.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 349      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 46200    |\n",
      "|    time_elapsed       | 3517     |\n",
      "|    total_timesteps    | 4620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46199    |\n",
      "|    policy_loss        | -3.33    |\n",
      "|    value_loss         | 24.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 4621000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 200.29\n",
      "Num timesteps: 4622000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 200.51\n",
      "Num timesteps: 4623000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.46\n",
      "Num timesteps: 4624000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.41\n",
      "Num timesteps: 4625000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.82\n",
      "Num timesteps: 4626000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.49\n",
      "Num timesteps: 4627000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.50\n",
      "Num timesteps: 4628000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.14\n",
      "Num timesteps: 4629000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.57\n",
      "Num timesteps: 4630000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 197.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 46300    |\n",
      "|    time_elapsed       | 3524     |\n",
      "|    total_timesteps    | 4630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.952   |\n",
      "|    explained_variance | 0.513    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46299    |\n",
      "|    policy_loss        | -6.17    |\n",
      "|    value_loss         | 60       |\n",
      "------------------------------------\n",
      "Num timesteps: 4631000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 197.71\n",
      "Num timesteps: 4632000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 198.25\n",
      "Num timesteps: 4633000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.59\n",
      "Num timesteps: 4634000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 197.87\n",
      "Num timesteps: 4635000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.80\n",
      "Num timesteps: 4636000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.73\n",
      "Num timesteps: 4637000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.53\n",
      "Num timesteps: 4638000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.57\n",
      "Num timesteps: 4639000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.53\n",
      "Num timesteps: 4640000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.91\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 345       |\n",
      "|    ep_rew_mean        | 188       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1313      |\n",
      "|    iterations         | 46400     |\n",
      "|    time_elapsed       | 3531      |\n",
      "|    total_timesteps    | 4640000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.671    |\n",
      "|    explained_variance | -0.000728 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 46399     |\n",
      "|    policy_loss        | -3.55     |\n",
      "|    value_loss         | 33.2      |\n",
      "-------------------------------------\n",
      "Num timesteps: 4641000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.79\n",
      "Num timesteps: 4642000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.62\n",
      "Num timesteps: 4643000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.74\n",
      "Num timesteps: 4644000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.20\n",
      "Num timesteps: 4645000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.19\n",
      "Num timesteps: 4646000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.36\n",
      "Num timesteps: 4647000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.51\n",
      "Num timesteps: 4648000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.84\n",
      "Num timesteps: 4649000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.80\n",
      "Num timesteps: 4650000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.74\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 46500    |\n",
      "|    time_elapsed       | 3539     |\n",
      "|    total_timesteps    | 4650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.163   |\n",
      "|    explained_variance | -12.1    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46499    |\n",
      "|    policy_loss        | -1       |\n",
      "|    value_loss         | 320      |\n",
      "------------------------------------\n",
      "Num timesteps: 4651000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.49\n",
      "Num timesteps: 4652000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.68\n",
      "Num timesteps: 4653000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.89\n",
      "Num timesteps: 4654000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.93\n",
      "Num timesteps: 4655000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.34\n",
      "Num timesteps: 4656000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.50\n",
      "Num timesteps: 4657000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.97\n",
      "Num timesteps: 4658000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.58\n",
      "Num timesteps: 4659000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.18\n",
      "Num timesteps: 4660000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.78\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 344      |\n",
      "|    ep_rew_mean        | 186      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 46600    |\n",
      "|    time_elapsed       | 3545     |\n",
      "|    total_timesteps    | 4660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.465   |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46599    |\n",
      "|    policy_loss        | -0.246   |\n",
      "|    value_loss         | 213      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4661000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.74\n",
      "Num timesteps: 4662000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.44\n",
      "Num timesteps: 4663000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.59\n",
      "Num timesteps: 4664000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.53\n",
      "Num timesteps: 4665000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.34\n",
      "Num timesteps: 4666000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.14\n",
      "Num timesteps: 4667000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.93\n",
      "Num timesteps: 4668000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.95\n",
      "Num timesteps: 4669000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.39\n",
      "Num timesteps: 4670000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 46700    |\n",
      "|    time_elapsed       | 3552     |\n",
      "|    total_timesteps    | 4670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.193   |\n",
      "|    explained_variance | -3.59    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46699    |\n",
      "|    policy_loss        | 3.18     |\n",
      "|    value_loss         | 2.95e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4671000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.73\n",
      "Num timesteps: 4672000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.59\n",
      "Num timesteps: 4673000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.95\n",
      "Num timesteps: 4674000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.95\n",
      "Num timesteps: 4675000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.18\n",
      "Num timesteps: 4676000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.89\n",
      "Num timesteps: 4677000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.52\n",
      "Num timesteps: 4678000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.23\n",
      "Num timesteps: 4679000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.08\n",
      "Num timesteps: 4680000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.28\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 46800    |\n",
      "|    time_elapsed       | 3558     |\n",
      "|    total_timesteps    | 4680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | -0.323   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46799    |\n",
      "|    policy_loss        | 4.07     |\n",
      "|    value_loss         | 1.13e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4681000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.01\n",
      "Num timesteps: 4682000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.55\n",
      "Num timesteps: 4683000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.23\n",
      "Num timesteps: 4684000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.46\n",
      "Num timesteps: 4685000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.80\n",
      "Num timesteps: 4686000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 179.32\n",
      "Num timesteps: 4687000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 179.68\n",
      "Num timesteps: 4688000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.95\n",
      "Num timesteps: 4689000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.96\n",
      "Num timesteps: 4690000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 298      |\n",
      "|    ep_rew_mean        | 185      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 46900    |\n",
      "|    time_elapsed       | 3565     |\n",
      "|    total_timesteps    | 4690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.58    |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46899    |\n",
      "|    policy_loss        | 2.13     |\n",
      "|    value_loss         | 71       |\n",
      "------------------------------------\n",
      "Num timesteps: 4691000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.99\n",
      "Num timesteps: 4692000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.79\n",
      "Num timesteps: 4693000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.90\n",
      "Num timesteps: 4694000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.75\n",
      "Num timesteps: 4695000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.19\n",
      "Num timesteps: 4696000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.56\n",
      "Num timesteps: 4697000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.77\n",
      "Num timesteps: 4698000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.29\n",
      "Num timesteps: 4699000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.18\n",
      "Num timesteps: 4700000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 295      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 47000    |\n",
      "|    time_elapsed       | 3572     |\n",
      "|    total_timesteps    | 4700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | -2.29    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 46999    |\n",
      "|    policy_loss        | -0.961   |\n",
      "|    value_loss         | 1.95e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4701000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.62\n",
      "Num timesteps: 4702000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.58\n",
      "Num timesteps: 4703000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.51\n",
      "Num timesteps: 4704000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.64\n",
      "Num timesteps: 4705000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 164.62\n",
      "Num timesteps: 4706000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 166.73\n",
      "Num timesteps: 4707000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.30\n",
      "Num timesteps: 4708000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.54\n",
      "Num timesteps: 4709000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.31\n",
      "Num timesteps: 4710000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 158.47\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 47100    |\n",
      "|    time_elapsed       | 3577     |\n",
      "|    total_timesteps    | 4710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.623   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47099    |\n",
      "|    policy_loss        | 2.22     |\n",
      "|    value_loss         | 48.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 4711000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 159.15\n",
      "Num timesteps: 4712000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 159.06\n",
      "Num timesteps: 4713000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.67\n",
      "Num timesteps: 4714000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.96\n",
      "Num timesteps: 4715000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 168.05\n",
      "Num timesteps: 4716000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 166.68\n",
      "Num timesteps: 4717000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 164.31\n",
      "Num timesteps: 4718000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 162.92\n",
      "Num timesteps: 4719000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 162.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4720000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 47200    |\n",
      "|    time_elapsed       | 3584     |\n",
      "|    total_timesteps    | 4720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | -4.34    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47199    |\n",
      "|    policy_loss        | 11.5     |\n",
      "|    value_loss         | 3.67e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4721000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.21\n",
      "Num timesteps: 4722000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 162.14\n",
      "Num timesteps: 4723000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.73\n",
      "Num timesteps: 4724000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.32\n",
      "Num timesteps: 4725000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.82\n",
      "Num timesteps: 4726000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.97\n",
      "Num timesteps: 4727000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.05\n",
      "Num timesteps: 4728000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.53\n",
      "Num timesteps: 4729000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 172.90\n",
      "Num timesteps: 4730000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 47300    |\n",
      "|    time_elapsed       | 3591     |\n",
      "|    total_timesteps    | 4730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.577   |\n",
      "|    explained_variance | -0.586   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47299    |\n",
      "|    policy_loss        | -0.119   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Num timesteps: 4731000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.46\n",
      "Num timesteps: 4732000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 172.89\n",
      "Num timesteps: 4733000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.09\n",
      "Num timesteps: 4734000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.95\n",
      "Num timesteps: 4735000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.29\n",
      "Num timesteps: 4736000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 179.24\n",
      "Num timesteps: 4737000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.47\n",
      "Num timesteps: 4738000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.71\n",
      "Num timesteps: 4739000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 179.88\n",
      "Num timesteps: 4740000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 305      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 47400    |\n",
      "|    time_elapsed       | 3597     |\n",
      "|    total_timesteps    | 4740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.573   |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47399    |\n",
      "|    policy_loss        | 2.7      |\n",
      "|    value_loss         | 200      |\n",
      "------------------------------------\n",
      "Num timesteps: 4741000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.27\n",
      "Num timesteps: 4742000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.09\n",
      "Num timesteps: 4743000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.04\n",
      "Num timesteps: 4744000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.70\n",
      "Num timesteps: 4745000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.62\n",
      "Num timesteps: 4746000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.43\n",
      "Num timesteps: 4747000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.93\n",
      "Num timesteps: 4748000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 168.47\n",
      "Num timesteps: 4749000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.36\n",
      "Num timesteps: 4750000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 276      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 47500    |\n",
      "|    time_elapsed       | 3602     |\n",
      "|    total_timesteps    | 4750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.295   |\n",
      "|    explained_variance | -0.398   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47499    |\n",
      "|    policy_loss        | 5.99     |\n",
      "|    value_loss         | 1.11e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4751000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.83\n",
      "Num timesteps: 4752000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.22\n",
      "Num timesteps: 4753000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.55\n",
      "Num timesteps: 4754000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.57\n",
      "Num timesteps: 4755000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.25\n",
      "Num timesteps: 4756000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.18\n",
      "Num timesteps: 4757000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.54\n",
      "Num timesteps: 4758000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.25\n",
      "Num timesteps: 4759000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.32\n",
      "Num timesteps: 4760000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 268      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 47600    |\n",
      "|    time_elapsed       | 3609     |\n",
      "|    total_timesteps    | 4760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.941   |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47599    |\n",
      "|    policy_loss        | 4.56     |\n",
      "|    value_loss         | 30.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 4761000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.18\n",
      "Num timesteps: 4762000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.23\n",
      "Num timesteps: 4763000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 172.07\n",
      "Num timesteps: 4764000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.13\n",
      "Num timesteps: 4765000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 172.37\n",
      "Num timesteps: 4766000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.28\n",
      "Num timesteps: 4767000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.69\n",
      "Num timesteps: 4768000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 170.83\n",
      "Num timesteps: 4769000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.84\n",
      "Num timesteps: 4770000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 299      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 47700    |\n",
      "|    time_elapsed       | 3617     |\n",
      "|    total_timesteps    | 4770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.745   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47699    |\n",
      "|    policy_loss        | 1.48     |\n",
      "|    value_loss         | 6.03     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4771000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.76\n",
      "Num timesteps: 4772000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 170.91\n",
      "Num timesteps: 4773000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 173.11\n",
      "Num timesteps: 4774000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.80\n",
      "Num timesteps: 4775000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.89\n",
      "Num timesteps: 4776000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 175.28\n",
      "Num timesteps: 4777000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.91\n",
      "Num timesteps: 4778000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.67\n",
      "Num timesteps: 4779000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.95\n",
      "Num timesteps: 4780000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 47800    |\n",
      "|    time_elapsed       | 3624     |\n",
      "|    total_timesteps    | 4780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | -0.689   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47799    |\n",
      "|    policy_loss        | -3.5     |\n",
      "|    value_loss         | 100      |\n",
      "------------------------------------\n",
      "Num timesteps: 4781000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.19\n",
      "Num timesteps: 4782000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.42\n",
      "Num timesteps: 4783000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.31\n",
      "Num timesteps: 4784000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.05\n",
      "Num timesteps: 4785000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.83\n",
      "Num timesteps: 4786000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.95\n",
      "Num timesteps: 4787000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.18\n",
      "Num timesteps: 4788000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.02\n",
      "Num timesteps: 4789000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.80\n",
      "Num timesteps: 4790000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 372      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 47900    |\n",
      "|    time_elapsed       | 3633     |\n",
      "|    total_timesteps    | 4790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.685   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47899    |\n",
      "|    policy_loss        | -2.79    |\n",
      "|    value_loss         | 19.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 4791000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 176.32\n",
      "Num timesteps: 4792000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.43\n",
      "Num timesteps: 4793000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.92\n",
      "Num timesteps: 4794000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.13\n",
      "Num timesteps: 4795000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.06\n",
      "Num timesteps: 4796000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.45\n",
      "Num timesteps: 4797000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.51\n",
      "Num timesteps: 4798000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 155.75\n",
      "Num timesteps: 4799000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 158.77\n",
      "Num timesteps: 4800000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 158.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 48000    |\n",
      "|    time_elapsed       | 3640     |\n",
      "|    total_timesteps    | 4800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | 0.394    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 47999    |\n",
      "|    policy_loss        | -0.00794 |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "Num timesteps: 4801000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 159.98\n",
      "Num timesteps: 4802000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.07\n",
      "Num timesteps: 4803000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.82\n",
      "Num timesteps: 4804000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.03\n",
      "Num timesteps: 4805000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.01\n",
      "Num timesteps: 4806000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.34\n",
      "Num timesteps: 4807000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 168.48\n",
      "Num timesteps: 4808000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 166.68\n",
      "Num timesteps: 4809000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 165.92\n",
      "Num timesteps: 4810000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 168.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 368      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 48100    |\n",
      "|    time_elapsed       | 3647     |\n",
      "|    total_timesteps    | 4810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.57    |\n",
      "|    explained_variance | -0.0526  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48099    |\n",
      "|    policy_loss        | 19.9     |\n",
      "|    value_loss         | 1.82e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4811000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.95\n",
      "Num timesteps: 4812000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.39\n",
      "Num timesteps: 4813000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.05\n",
      "Num timesteps: 4814000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.61\n",
      "Num timesteps: 4815000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.10\n",
      "Num timesteps: 4816000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 169.18\n",
      "Num timesteps: 4817000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 166.62\n",
      "Num timesteps: 4818000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 164.87\n",
      "Num timesteps: 4819000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 164.29\n",
      "Num timesteps: 4820000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 162.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 342      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 48200    |\n",
      "|    time_elapsed       | 3654     |\n",
      "|    total_timesteps    | 4820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.804   |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48199    |\n",
      "|    policy_loss        | -5.62    |\n",
      "|    value_loss         | 56.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 4821000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 163.15\n",
      "Num timesteps: 4822000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 167.65\n",
      "Num timesteps: 4823000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 171.55\n",
      "Num timesteps: 4824000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 172.49\n",
      "Num timesteps: 4825000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 174.45\n",
      "Num timesteps: 4826000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 177.43\n",
      "Num timesteps: 4827000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.19\n",
      "Num timesteps: 4828000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.68\n",
      "Num timesteps: 4829000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4830000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.08\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 48300    |\n",
      "|    time_elapsed       | 3660     |\n",
      "|    total_timesteps    | 4830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.596   |\n",
      "|    explained_variance | -1.79    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48299    |\n",
      "|    policy_loss        | 10.7     |\n",
      "|    value_loss         | 388      |\n",
      "------------------------------------\n",
      "Num timesteps: 4831000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 195.59\n",
      "Num timesteps: 4832000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 194.88\n",
      "Num timesteps: 4833000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.63\n",
      "Num timesteps: 4834000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.79\n",
      "Num timesteps: 4835000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.84\n",
      "Num timesteps: 4836000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.58\n",
      "Num timesteps: 4837000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.05\n",
      "Num timesteps: 4838000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.69\n",
      "Num timesteps: 4839000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.85\n",
      "Num timesteps: 4840000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 186.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 299      |\n",
      "|    ep_rew_mean        | 186      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 48400    |\n",
      "|    time_elapsed       | 3667     |\n",
      "|    total_timesteps    | 4840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.753   |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48399    |\n",
      "|    policy_loss        | 3.85     |\n",
      "|    value_loss         | 35.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 4841000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 184.28\n",
      "Num timesteps: 4842000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.00\n",
      "Num timesteps: 4843000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 179.18\n",
      "Num timesteps: 4844000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.50\n",
      "Num timesteps: 4845000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 183.15\n",
      "Num timesteps: 4846000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.97\n",
      "Num timesteps: 4847000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.37\n",
      "Num timesteps: 4848000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.68\n",
      "Num timesteps: 4849000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 188.19\n",
      "Num timesteps: 4850000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 296      |\n",
      "|    ep_rew_mean        | 186      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 48500    |\n",
      "|    time_elapsed       | 3673     |\n",
      "|    total_timesteps    | 4850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | -0.704   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48499    |\n",
      "|    policy_loss        | 33.4     |\n",
      "|    value_loss         | 3.17e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 4851000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.29\n",
      "Num timesteps: 4852000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.30\n",
      "Num timesteps: 4853000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 182.41\n",
      "Num timesteps: 4854000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.73\n",
      "Num timesteps: 4855000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 178.76\n",
      "Num timesteps: 4856000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.62\n",
      "Num timesteps: 4857000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 180.01\n",
      "Num timesteps: 4858000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 181.63\n",
      "Num timesteps: 4859000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 185.05\n",
      "Num timesteps: 4860000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 187.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 187      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 48600    |\n",
      "|    time_elapsed       | 3680     |\n",
      "|    total_timesteps    | 4860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | 0.508    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48599    |\n",
      "|    policy_loss        | 3.69     |\n",
      "|    value_loss         | 445      |\n",
      "------------------------------------\n",
      "Num timesteps: 4861000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.18\n",
      "Num timesteps: 4862000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 192.33\n",
      "Num timesteps: 4863000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 191.22\n",
      "Num timesteps: 4864000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 190.13\n",
      "Num timesteps: 4865000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 189.04\n",
      "Num timesteps: 4866000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 193.11\n",
      "Num timesteps: 4867000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 199.30\n",
      "Num timesteps: 4868000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 205.50\n",
      "Num timesteps: 4869000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 208.37\n",
      "Num timesteps: 4870000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 209.47\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 315      |\n",
      "|    ep_rew_mean        | 209      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 48700    |\n",
      "|    time_elapsed       | 3687     |\n",
      "|    total_timesteps    | 4870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.516   |\n",
      "|    explained_variance | -1.06    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48699    |\n",
      "|    policy_loss        | 9.76     |\n",
      "|    value_loss         | 869      |\n",
      "------------------------------------\n",
      "Num timesteps: 4871000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 208.58\n",
      "Num timesteps: 4872000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 213.13\n",
      "Num timesteps: 4873000\n",
      "Best mean reward: 213.17 - Last mean reward per episode: 216.74\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4874000\n",
      "Best mean reward: 216.74 - Last mean reward per episode: 217.73\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4875000\n",
      "Best mean reward: 217.73 - Last mean reward per episode: 223.14\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4876000\n",
      "Best mean reward: 223.14 - Last mean reward per episode: 223.66\n",
      "Saving new best model to log_dir_A2C_8/best_model.zip\n",
      "Num timesteps: 4877000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 220.49\n",
      "Num timesteps: 4878000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 209.66\n",
      "Num timesteps: 4879000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 211.06\n",
      "Num timesteps: 4880000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 208.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 208      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 48800    |\n",
      "|    time_elapsed       | 3695     |\n",
      "|    total_timesteps    | 4880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.542   |\n",
      "|    explained_variance | -0.0759  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48799    |\n",
      "|    policy_loss        | -0.898   |\n",
      "|    value_loss         | 3.97     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4881000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 209.85\n",
      "Num timesteps: 4882000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 213.30\n",
      "Num timesteps: 4883000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 212.67\n",
      "Num timesteps: 4884000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 212.84\n",
      "Num timesteps: 4885000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 211.68\n",
      "Num timesteps: 4886000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 213.19\n",
      "Num timesteps: 4887000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 211.95\n",
      "Num timesteps: 4888000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 209.62\n",
      "Num timesteps: 4889000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 209.51\n",
      "Num timesteps: 4890000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 208.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 319      |\n",
      "|    ep_rew_mean        | 208      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 48900    |\n",
      "|    time_elapsed       | 3701     |\n",
      "|    total_timesteps    | 4890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.569   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48899    |\n",
      "|    policy_loss        | 0.698    |\n",
      "|    value_loss         | 15.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 4891000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 208.15\n",
      "Num timesteps: 4892000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 204.08\n",
      "Num timesteps: 4893000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 202.75\n",
      "Num timesteps: 4894000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.38\n",
      "Num timesteps: 4895000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.67\n",
      "Num timesteps: 4896000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.02\n",
      "Num timesteps: 4897000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.06\n",
      "Num timesteps: 4898000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.26\n",
      "Num timesteps: 4899000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.33\n",
      "Num timesteps: 4900000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.98\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 49000    |\n",
      "|    time_elapsed       | 3709     |\n",
      "|    total_timesteps    | 4900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.652   |\n",
      "|    explained_variance | 0.732    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 48999    |\n",
      "|    policy_loss        | -0.755   |\n",
      "|    value_loss         | 713      |\n",
      "------------------------------------\n",
      "Num timesteps: 4901000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.95\n",
      "Num timesteps: 4902000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.67\n",
      "Num timesteps: 4903000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.11\n",
      "Num timesteps: 4904000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.70\n",
      "Num timesteps: 4905000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.53\n",
      "Num timesteps: 4906000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.22\n",
      "Num timesteps: 4907000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.95\n",
      "Num timesteps: 4908000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.27\n",
      "Num timesteps: 4909000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.09\n",
      "Num timesteps: 4910000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 49100    |\n",
      "|    time_elapsed       | 3716     |\n",
      "|    total_timesteps    | 4910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.68    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49099    |\n",
      "|    policy_loss        | -2.12    |\n",
      "|    value_loss         | 14.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 4911000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.71\n",
      "Num timesteps: 4912000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.20\n",
      "Num timesteps: 4913000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.67\n",
      "Num timesteps: 4914000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.08\n",
      "Num timesteps: 4915000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.95\n",
      "Num timesteps: 4916000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.94\n",
      "Num timesteps: 4917000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.72\n",
      "Num timesteps: 4918000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.82\n",
      "Num timesteps: 4919000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.46\n",
      "Num timesteps: 4920000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 328      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 49200    |\n",
      "|    time_elapsed       | 3723     |\n",
      "|    total_timesteps    | 4920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.236   |\n",
      "|    explained_variance | -1.92    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49199    |\n",
      "|    policy_loss        | 1.38     |\n",
      "|    value_loss         | 2.4e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 4921000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.94\n",
      "Num timesteps: 4922000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.14\n",
      "Num timesteps: 4923000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.76\n",
      "Num timesteps: 4924000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.22\n",
      "Num timesteps: 4925000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.83\n",
      "Num timesteps: 4926000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.19\n",
      "Num timesteps: 4927000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.46\n",
      "Num timesteps: 4928000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.27\n",
      "Num timesteps: 4929000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.58\n",
      "Num timesteps: 4930000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 49300    |\n",
      "|    time_elapsed       | 3729     |\n",
      "|    total_timesteps    | 4930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0.484    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49299    |\n",
      "|    policy_loss        | 2.25     |\n",
      "|    value_loss         | 316      |\n",
      "------------------------------------\n",
      "Num timesteps: 4931000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.61\n",
      "Num timesteps: 4932000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.46\n",
      "Num timesteps: 4933000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.77\n",
      "Num timesteps: 4934000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.81\n",
      "Num timesteps: 4935000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.03\n",
      "Num timesteps: 4936000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.93\n",
      "Num timesteps: 4937000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.73\n",
      "Num timesteps: 4938000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.92\n",
      "Num timesteps: 4939000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4940000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 317      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49400    |\n",
      "|    time_elapsed       | 3736     |\n",
      "|    total_timesteps    | 4940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.567   |\n",
      "|    explained_variance | 0.00455  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49399    |\n",
      "|    policy_loss        | -1.64    |\n",
      "|    value_loss         | 13.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 4941000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.59\n",
      "Num timesteps: 4942000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.60\n",
      "Num timesteps: 4943000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.34\n",
      "Num timesteps: 4944000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.75\n",
      "Num timesteps: 4945000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.88\n",
      "Num timesteps: 4946000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.55\n",
      "Num timesteps: 4947000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.42\n",
      "Num timesteps: 4948000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.76\n",
      "Num timesteps: 4949000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.16\n",
      "Num timesteps: 4950000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49500    |\n",
      "|    time_elapsed       | 3744     |\n",
      "|    total_timesteps    | 4950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.802   |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49499    |\n",
      "|    policy_loss        | 5.16     |\n",
      "|    value_loss         | 133      |\n",
      "------------------------------------\n",
      "Num timesteps: 4951000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.75\n",
      "Num timesteps: 4952000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.86\n",
      "Num timesteps: 4953000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.01\n",
      "Num timesteps: 4954000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.09\n",
      "Num timesteps: 4955000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.14\n",
      "Num timesteps: 4956000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.42\n",
      "Num timesteps: 4957000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.23\n",
      "Num timesteps: 4958000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.61\n",
      "Num timesteps: 4959000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 4960000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49600    |\n",
      "|    time_elapsed       | 3750     |\n",
      "|    total_timesteps    | 4960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0.0395   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49599    |\n",
      "|    policy_loss        | -0.0911  |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "Num timesteps: 4961000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.61\n",
      "Num timesteps: 4962000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.54\n",
      "Num timesteps: 4963000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.08\n",
      "Num timesteps: 4964000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.55\n",
      "Num timesteps: 4965000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.44\n",
      "Num timesteps: 4966000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.85\n",
      "Num timesteps: 4967000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.89\n",
      "Num timesteps: 4968000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.68\n",
      "Num timesteps: 4969000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.42\n",
      "Num timesteps: 4970000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49700    |\n",
      "|    time_elapsed       | 3759     |\n",
      "|    total_timesteps    | 4970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.644   |\n",
      "|    explained_variance | 0.11     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49699    |\n",
      "|    policy_loss        | -0.00762 |\n",
      "|    value_loss         | 0.0692   |\n",
      "------------------------------------\n",
      "Num timesteps: 4971000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.50\n",
      "Num timesteps: 4972000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.61\n",
      "Num timesteps: 4973000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.28\n",
      "Num timesteps: 4974000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.37\n",
      "Num timesteps: 4975000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.09\n",
      "Num timesteps: 4976000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.00\n",
      "Num timesteps: 4977000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.51\n",
      "Num timesteps: 4978000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.15\n",
      "Num timesteps: 4979000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.76\n",
      "Num timesteps: 4980000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 316      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49800    |\n",
      "|    time_elapsed       | 3766     |\n",
      "|    total_timesteps    | 4980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | -0.132   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49799    |\n",
      "|    policy_loss        | -0.327   |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "Num timesteps: 4981000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.38\n",
      "Num timesteps: 4982000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.19\n",
      "Num timesteps: 4983000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.18\n",
      "Num timesteps: 4984000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.25\n",
      "Num timesteps: 4985000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.74\n",
      "Num timesteps: 4986000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.19\n",
      "Num timesteps: 4987000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.13\n",
      "Num timesteps: 4988000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.82\n",
      "Num timesteps: 4989000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.58\n",
      "Num timesteps: 4990000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 307      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 49900    |\n",
      "|    time_elapsed       | 3772     |\n",
      "|    total_timesteps    | 4990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.453   |\n",
      "|    explained_variance | 0.00618  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49899    |\n",
      "|    policy_loss        | 1.27     |\n",
      "|    value_loss         | 629      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 4991000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.24\n",
      "Num timesteps: 4992000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.37\n",
      "Num timesteps: 4993000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.48\n",
      "Num timesteps: 4994000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.95\n",
      "Num timesteps: 4995000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.93\n",
      "Num timesteps: 4996000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.83\n",
      "Num timesteps: 4997000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.42\n",
      "Num timesteps: 4998000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.46\n",
      "Num timesteps: 4999000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.31\n",
      "Num timesteps: 5000000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.98\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 280      |\n",
      "|    ep_rew_mean        | 191      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 50000    |\n",
      "|    time_elapsed       | 3779     |\n",
      "|    total_timesteps    | 5000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.529   |\n",
      "|    explained_variance | 0.066    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 49999    |\n",
      "|    policy_loss        | -5.67    |\n",
      "|    value_loss         | 125      |\n",
      "------------------------------------\n",
      "Num timesteps: 5001000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.19\n",
      "Num timesteps: 5002000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.82\n",
      "Num timesteps: 5003000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.68\n",
      "Num timesteps: 5004000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.67\n",
      "Num timesteps: 5005000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.56\n",
      "Num timesteps: 5006000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.61\n",
      "Num timesteps: 5007000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.42\n",
      "Num timesteps: 5008000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.75\n",
      "Num timesteps: 5009000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.74\n",
      "Num timesteps: 5010000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.81\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 269      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 50100    |\n",
      "|    time_elapsed       | 3784     |\n",
      "|    total_timesteps    | 5010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.173   |\n",
      "|    explained_variance | 0.0745   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50099    |\n",
      "|    policy_loss        | 3.7      |\n",
      "|    value_loss         | 581      |\n",
      "------------------------------------\n",
      "Num timesteps: 5011000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.96\n",
      "Num timesteps: 5012000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.54\n",
      "Num timesteps: 5013000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.05\n",
      "Num timesteps: 5014000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.35\n",
      "Num timesteps: 5015000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.46\n",
      "Num timesteps: 5016000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.17\n",
      "Num timesteps: 5017000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.29\n",
      "Num timesteps: 5018000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.46\n",
      "Num timesteps: 5019000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.49\n",
      "Num timesteps: 5020000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 50200    |\n",
      "|    time_elapsed       | 3790     |\n",
      "|    total_timesteps    | 5020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50199    |\n",
      "|    policy_loss        | -42      |\n",
      "|    value_loss         | 3.86e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5021000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.89\n",
      "Num timesteps: 5022000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.29\n",
      "Num timesteps: 5023000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.30\n",
      "Num timesteps: 5024000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.09\n",
      "Num timesteps: 5025000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.98\n",
      "Num timesteps: 5026000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.50\n",
      "Num timesteps: 5027000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.11\n",
      "Num timesteps: 5028000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.87\n",
      "Num timesteps: 5029000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.90\n",
      "Num timesteps: 5030000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.63\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 272      |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 50300    |\n",
      "|    time_elapsed       | 3797     |\n",
      "|    total_timesteps    | 5030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.462   |\n",
      "|    explained_variance | -1.13    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50299    |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    value_loss         | 299      |\n",
      "------------------------------------\n",
      "Num timesteps: 5031000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.44\n",
      "Num timesteps: 5032000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.95\n",
      "Num timesteps: 5033000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.47\n",
      "Num timesteps: 5034000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.98\n",
      "Num timesteps: 5035000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.38\n",
      "Num timesteps: 5036000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.65\n",
      "Num timesteps: 5037000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.60\n",
      "Num timesteps: 5038000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.54\n",
      "Num timesteps: 5039000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.66\n",
      "Num timesteps: 5040000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 262      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 50400    |\n",
      "|    time_elapsed       | 3804     |\n",
      "|    total_timesteps    | 5040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50399    |\n",
      "|    policy_loss        | -29.1    |\n",
      "|    value_loss         | 6.11e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5041000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.10\n",
      "Num timesteps: 5042000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.83\n",
      "Num timesteps: 5043000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.28\n",
      "Num timesteps: 5044000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.63\n",
      "Num timesteps: 5045000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.32\n",
      "Num timesteps: 5046000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.56\n",
      "Num timesteps: 5047000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.18\n",
      "Num timesteps: 5048000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.14\n",
      "Num timesteps: 5049000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5050000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.89\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 264      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 50500    |\n",
      "|    time_elapsed       | 3810     |\n",
      "|    total_timesteps    | 5050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.392   |\n",
      "|    explained_variance | 0.598    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50499    |\n",
      "|    policy_loss        | -1.2     |\n",
      "|    value_loss         | 2.46e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5051000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.04\n",
      "Num timesteps: 5052000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.73\n",
      "Num timesteps: 5053000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.13\n",
      "Num timesteps: 5054000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.27\n",
      "Num timesteps: 5055000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.30\n",
      "Num timesteps: 5056000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.62\n",
      "Num timesteps: 5057000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.94\n",
      "Num timesteps: 5058000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.54\n",
      "Num timesteps: 5059000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.58\n",
      "Num timesteps: 5060000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 269      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 50600    |\n",
      "|    time_elapsed       | 3817     |\n",
      "|    total_timesteps    | 5060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.668   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50599    |\n",
      "|    policy_loss        | -0.493   |\n",
      "|    value_loss         | 16.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 5061000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.65\n",
      "Num timesteps: 5062000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.87\n",
      "Num timesteps: 5063000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.70\n",
      "Num timesteps: 5064000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.00\n",
      "Num timesteps: 5065000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.61\n",
      "Num timesteps: 5066000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.97\n",
      "Num timesteps: 5067000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.90\n",
      "Num timesteps: 5068000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.33\n",
      "Num timesteps: 5069000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.97\n",
      "Num timesteps: 5070000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 256      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 50700    |\n",
      "|    time_elapsed       | 3822     |\n",
      "|    total_timesteps    | 5070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | 0.587    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50699    |\n",
      "|    policy_loss        | -10.7    |\n",
      "|    value_loss         | 3.9e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 5071000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.07\n",
      "Num timesteps: 5072000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.00\n",
      "Num timesteps: 5073000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.62\n",
      "Num timesteps: 5074000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.92\n",
      "Num timesteps: 5075000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.64\n",
      "Num timesteps: 5076000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.36\n",
      "Num timesteps: 5077000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.11\n",
      "Num timesteps: 5078000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.51\n",
      "Num timesteps: 5079000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.23\n",
      "Num timesteps: 5080000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 247      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 50800    |\n",
      "|    time_elapsed       | 3827     |\n",
      "|    total_timesteps    | 5080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.497   |\n",
      "|    explained_variance | 0.976    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50799    |\n",
      "|    policy_loss        | 12       |\n",
      "|    value_loss         | 437      |\n",
      "------------------------------------\n",
      "Num timesteps: 5081000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.56\n",
      "Num timesteps: 5082000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.88\n",
      "Num timesteps: 5083000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.24\n",
      "Num timesteps: 5084000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.96\n",
      "Num timesteps: 5085000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.02\n",
      "Num timesteps: 5086000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.11\n",
      "Num timesteps: 5087000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.04\n",
      "Num timesteps: 5088000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.57\n",
      "Num timesteps: 5089000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.32\n",
      "Num timesteps: 5090000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.25\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 207      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 50900    |\n",
      "|    time_elapsed       | 3832     |\n",
      "|    total_timesteps    | 5090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.645   |\n",
      "|    explained_variance | -0.195   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50899    |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "Num timesteps: 5091000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.83\n",
      "Num timesteps: 5092000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.49\n",
      "Num timesteps: 5093000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.03\n",
      "Num timesteps: 5094000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.68\n",
      "Num timesteps: 5095000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.49\n",
      "Num timesteps: 5096000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.24\n",
      "Num timesteps: 5097000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.50\n",
      "Num timesteps: 5098000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.69\n",
      "Num timesteps: 5099000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.29\n",
      "Num timesteps: 5100000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.74\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 194      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 51000    |\n",
      "|    time_elapsed       | 3837     |\n",
      "|    total_timesteps    | 5100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.366   |\n",
      "|    explained_variance | -3.53    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 50999    |\n",
      "|    policy_loss        | 5.34     |\n",
      "|    value_loss         | 2.24e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5101000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.17\n",
      "Num timesteps: 5102000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.36\n",
      "Num timesteps: 5103000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.42\n",
      "Num timesteps: 5104000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.86\n",
      "Num timesteps: 5105000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.11\n",
      "Num timesteps: 5106000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.74\n",
      "Num timesteps: 5107000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.55\n",
      "Num timesteps: 5108000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.61\n",
      "Num timesteps: 5109000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.79\n",
      "Num timesteps: 5110000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 51100    |\n",
      "|    time_elapsed       | 3843     |\n",
      "|    total_timesteps    | 5110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | -0.0198  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51099    |\n",
      "|    policy_loss        | -1.5     |\n",
      "|    value_loss         | 6.25     |\n",
      "------------------------------------\n",
      "Num timesteps: 5111000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.99\n",
      "Num timesteps: 5112000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.92\n",
      "Num timesteps: 5113000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.86\n",
      "Num timesteps: 5114000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.80\n",
      "Num timesteps: 5115000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.99\n",
      "Num timesteps: 5116000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.59\n",
      "Num timesteps: 5117000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.71\n",
      "Num timesteps: 5118000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.02\n",
      "Num timesteps: 5119000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.50\n",
      "Num timesteps: 5120000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 234      |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 51200    |\n",
      "|    time_elapsed       | 3849     |\n",
      "|    total_timesteps    | 5120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.39    |\n",
      "|    explained_variance | 0.00383  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51199    |\n",
      "|    policy_loss        | -4.34    |\n",
      "|    value_loss         | 64.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 5121000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.66\n",
      "Num timesteps: 5122000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.98\n",
      "Num timesteps: 5123000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.49\n",
      "Num timesteps: 5124000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.57\n",
      "Num timesteps: 5125000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.40\n",
      "Num timesteps: 5126000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.60\n",
      "Num timesteps: 5127000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.61\n",
      "Num timesteps: 5128000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.60\n",
      "Num timesteps: 5129000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.61\n",
      "Num timesteps: 5130000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 247      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 51300    |\n",
      "|    time_elapsed       | 3855     |\n",
      "|    total_timesteps    | 5130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.603   |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51299    |\n",
      "|    policy_loss        | -5.62    |\n",
      "|    value_loss         | 141      |\n",
      "------------------------------------\n",
      "Num timesteps: 5131000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.26\n",
      "Num timesteps: 5132000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.68\n",
      "Num timesteps: 5133000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.37\n",
      "Num timesteps: 5134000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.97\n",
      "Num timesteps: 5135000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.72\n",
      "Num timesteps: 5136000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.72\n",
      "Num timesteps: 5137000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.66\n",
      "Num timesteps: 5138000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.19\n",
      "Num timesteps: 5139000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.98\n",
      "Num timesteps: 5140000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 233      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 51400    |\n",
      "|    time_elapsed       | 3860     |\n",
      "|    total_timesteps    | 5140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | -0.242   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51399    |\n",
      "|    policy_loss        | -5.57    |\n",
      "|    value_loss         | 624      |\n",
      "------------------------------------\n",
      "Num timesteps: 5141000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.94\n",
      "Num timesteps: 5142000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.70\n",
      "Num timesteps: 5143000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.39\n",
      "Num timesteps: 5144000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.95\n",
      "Num timesteps: 5145000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.51\n",
      "Num timesteps: 5146000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.06\n",
      "Num timesteps: 5147000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.76\n",
      "Num timesteps: 5148000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.13\n",
      "Num timesteps: 5149000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.30\n",
      "Num timesteps: 5150000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 236      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 51500    |\n",
      "|    time_elapsed       | 3866     |\n",
      "|    total_timesteps    | 5150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.305   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51499    |\n",
      "|    policy_loss        | 7.28     |\n",
      "|    value_loss         | 647      |\n",
      "------------------------------------\n",
      "Num timesteps: 5151000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.62\n",
      "Num timesteps: 5152000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.72\n",
      "Num timesteps: 5153000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.15\n",
      "Num timesteps: 5154000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.45\n",
      "Num timesteps: 5155000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.19\n",
      "Num timesteps: 5156000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.83\n",
      "Num timesteps: 5157000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.91\n",
      "Num timesteps: 5158000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.88\n",
      "Num timesteps: 5159000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5160000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 227      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 51600    |\n",
      "|    time_elapsed       | 3871     |\n",
      "|    total_timesteps    | 5160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0937  |\n",
      "|    explained_variance | -0.625   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51599    |\n",
      "|    policy_loss        | -1.73    |\n",
      "|    value_loss         | 153      |\n",
      "------------------------------------\n",
      "Num timesteps: 5161000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.24\n",
      "Num timesteps: 5162000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.57\n",
      "Num timesteps: 5163000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.19\n",
      "Num timesteps: 5164000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.87\n",
      "Num timesteps: 5165000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.53\n",
      "Num timesteps: 5166000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.33\n",
      "Num timesteps: 5167000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.04\n",
      "Num timesteps: 5168000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.07\n",
      "Num timesteps: 5169000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.86\n",
      "Num timesteps: 5170000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 223      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 51700    |\n",
      "|    time_elapsed       | 3876     |\n",
      "|    total_timesteps    | 5170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.368   |\n",
      "|    explained_variance | -3.15    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51699    |\n",
      "|    policy_loss        | 1.61     |\n",
      "|    value_loss         | 2.38e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5171000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.56\n",
      "Num timesteps: 5172000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.39\n",
      "Num timesteps: 5173000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.48\n",
      "Num timesteps: 5174000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.00\n",
      "Num timesteps: 5175000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.80\n",
      "Num timesteps: 5176000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.58\n",
      "Num timesteps: 5177000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.35\n",
      "Num timesteps: 5178000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.80\n",
      "Num timesteps: 5179000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.87\n",
      "Num timesteps: 5180000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 257      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 51800    |\n",
      "|    time_elapsed       | 3882     |\n",
      "|    total_timesteps    | 5180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0.735    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51799    |\n",
      "|    policy_loss        | -13      |\n",
      "|    value_loss         | 2.87e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5181000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.05\n",
      "Num timesteps: 5182000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.51\n",
      "Num timesteps: 5183000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.16\n",
      "Num timesteps: 5184000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.53\n",
      "Num timesteps: 5185000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.14\n",
      "Num timesteps: 5186000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.33\n",
      "Num timesteps: 5187000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.67\n",
      "Num timesteps: 5188000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.65\n",
      "Num timesteps: 5189000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.35\n",
      "Num timesteps: 5190000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 286      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 51900    |\n",
      "|    time_elapsed       | 3890     |\n",
      "|    total_timesteps    | 5190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.648   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51899    |\n",
      "|    policy_loss        | 0.815    |\n",
      "|    value_loss         | 39.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 5191000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.74\n",
      "Num timesteps: 5192000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.59\n",
      "Num timesteps: 5193000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.34\n",
      "Num timesteps: 5194000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.71\n",
      "Num timesteps: 5195000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.62\n",
      "Num timesteps: 5196000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.81\n",
      "Num timesteps: 5197000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.27\n",
      "Num timesteps: 5198000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.49\n",
      "Num timesteps: 5199000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.76\n",
      "Num timesteps: 5200000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 298      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 52000    |\n",
      "|    time_elapsed       | 3896     |\n",
      "|    total_timesteps    | 5200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.496   |\n",
      "|    explained_variance | -1.35    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 51999    |\n",
      "|    policy_loss        | 0.0364   |\n",
      "|    value_loss         | 1.79e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5201000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.86\n",
      "Num timesteps: 5202000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.62\n",
      "Num timesteps: 5203000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.67\n",
      "Num timesteps: 5204000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.99\n",
      "Num timesteps: 5205000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.43\n",
      "Num timesteps: 5206000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.36\n",
      "Num timesteps: 5207000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.16\n",
      "Num timesteps: 5208000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.26\n",
      "Num timesteps: 5209000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.53\n",
      "Num timesteps: 5210000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 276      |\n",
      "|    ep_rew_mean        | 186      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 52100    |\n",
      "|    time_elapsed       | 3902     |\n",
      "|    total_timesteps    | 5210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.522   |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52099    |\n",
      "|    policy_loss        | 3.49     |\n",
      "|    value_loss         | 108      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5211000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.95\n",
      "Num timesteps: 5212000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.96\n",
      "Num timesteps: 5213000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.33\n",
      "Num timesteps: 5214000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.79\n",
      "Num timesteps: 5215000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.36\n",
      "Num timesteps: 5216000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.57\n",
      "Num timesteps: 5217000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.54\n",
      "Num timesteps: 5218000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.42\n",
      "Num timesteps: 5219000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.32\n",
      "Num timesteps: 5220000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 234      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 52200    |\n",
      "|    time_elapsed       | 3907     |\n",
      "|    total_timesteps    | 5220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.572   |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52199    |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    value_loss         | 497      |\n",
      "------------------------------------\n",
      "Num timesteps: 5221000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.47\n",
      "Num timesteps: 5222000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.54\n",
      "Num timesteps: 5223000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.34\n",
      "Num timesteps: 5224000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.77\n",
      "Num timesteps: 5225000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.08\n",
      "Num timesteps: 5226000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.91\n",
      "Num timesteps: 5227000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.33\n",
      "Num timesteps: 5228000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.24\n",
      "Num timesteps: 5229000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.58\n",
      "Num timesteps: 5230000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 210      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 52300    |\n",
      "|    time_elapsed       | 3912     |\n",
      "|    total_timesteps    | 5230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.366   |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52299    |\n",
      "|    policy_loss        | 9.21     |\n",
      "|    value_loss         | 740      |\n",
      "------------------------------------\n",
      "Num timesteps: 5231000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.78\n",
      "Num timesteps: 5232000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.57\n",
      "Num timesteps: 5233000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.15\n",
      "Num timesteps: 5234000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.10\n",
      "Num timesteps: 5235000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.85\n",
      "Num timesteps: 5236000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.89\n",
      "Num timesteps: 5237000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.57\n",
      "Num timesteps: 5238000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.24\n",
      "Num timesteps: 5239000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.44\n",
      "Num timesteps: 5240000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 207      |\n",
      "|    ep_rew_mean        | 125      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1337     |\n",
      "|    iterations         | 52400    |\n",
      "|    time_elapsed       | 3918     |\n",
      "|    total_timesteps    | 5240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.697   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52399    |\n",
      "|    policy_loss        | -10.8    |\n",
      "|    value_loss         | 234      |\n",
      "------------------------------------\n",
      "Num timesteps: 5241000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.80\n",
      "Num timesteps: 5242000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.28\n",
      "Num timesteps: 5243000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.77\n",
      "Num timesteps: 5244000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.88\n",
      "Num timesteps: 5245000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.11\n",
      "Num timesteps: 5246000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.42\n",
      "Num timesteps: 5247000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.66\n",
      "Num timesteps: 5248000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.71\n",
      "Num timesteps: 5249000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.86\n",
      "Num timesteps: 5250000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 202      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 52500    |\n",
      "|    time_elapsed       | 3923     |\n",
      "|    total_timesteps    | 5250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52499    |\n",
      "|    policy_loss        | -2.16    |\n",
      "|    value_loss         | 45.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 5251000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.80\n",
      "Num timesteps: 5252000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.44\n",
      "Num timesteps: 5253000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.73\n",
      "Num timesteps: 5254000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.23\n",
      "Num timesteps: 5255000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 5256000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.30\n",
      "Num timesteps: 5257000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.00\n",
      "Num timesteps: 5258000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.01\n",
      "Num timesteps: 5259000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.08\n",
      "Num timesteps: 5260000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 189      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1339     |\n",
      "|    iterations         | 52600    |\n",
      "|    time_elapsed       | 3928     |\n",
      "|    total_timesteps    | 5260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0572  |\n",
      "|    explained_variance | 0.0358   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52599    |\n",
      "|    policy_loss        | 3.03     |\n",
      "|    value_loss         | 436      |\n",
      "------------------------------------\n",
      "Num timesteps: 5261000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.83\n",
      "Num timesteps: 5262000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.98\n",
      "Num timesteps: 5263000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.45\n",
      "Num timesteps: 5264000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.34\n",
      "Num timesteps: 5265000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.15\n",
      "Num timesteps: 5266000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.10\n",
      "Num timesteps: 5267000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.67\n",
      "Num timesteps: 5268000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.40\n",
      "Num timesteps: 5269000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5270000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 194      |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1339     |\n",
      "|    iterations         | 52700    |\n",
      "|    time_elapsed       | 3933     |\n",
      "|    total_timesteps    | 5270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.453   |\n",
      "|    explained_variance | -1.25    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52699    |\n",
      "|    policy_loss        | -1.87    |\n",
      "|    value_loss         | 22.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 5271000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.61\n",
      "Num timesteps: 5272000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.86\n",
      "Num timesteps: 5273000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.99\n",
      "Num timesteps: 5274000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.29\n",
      "Num timesteps: 5275000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.54\n",
      "Num timesteps: 5276000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.43\n",
      "Num timesteps: 5277000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.40\n",
      "Num timesteps: 5278000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.22\n",
      "Num timesteps: 5279000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.66\n",
      "Num timesteps: 5280000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.57\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 229      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 52800    |\n",
      "|    time_elapsed       | 3939     |\n",
      "|    total_timesteps    | 5280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.608   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52799    |\n",
      "|    policy_loss        | 4.8      |\n",
      "|    value_loss         | 88.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 5281000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.34\n",
      "Num timesteps: 5282000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.68\n",
      "Num timesteps: 5283000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.56\n",
      "Num timesteps: 5284000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.44\n",
      "Num timesteps: 5285000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.93\n",
      "Num timesteps: 5286000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.55\n",
      "Num timesteps: 5287000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.95\n",
      "Num timesteps: 5288000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.33\n",
      "Num timesteps: 5289000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.00\n",
      "Num timesteps: 5290000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 251      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 52900    |\n",
      "|    time_elapsed       | 3946     |\n",
      "|    total_timesteps    | 5290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.612   |\n",
      "|    explained_variance | 0.817    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52899    |\n",
      "|    policy_loss        | -4.3     |\n",
      "|    value_loss         | 64.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 5291000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.96\n",
      "Num timesteps: 5292000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.94\n",
      "Num timesteps: 5293000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.19\n",
      "Num timesteps: 5294000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.71\n",
      "Num timesteps: 5295000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.89\n",
      "Num timesteps: 5296000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.07\n",
      "Num timesteps: 5297000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.77\n",
      "Num timesteps: 5298000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.59\n",
      "Num timesteps: 5299000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.52\n",
      "Num timesteps: 5300000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 218      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 53000    |\n",
      "|    time_elapsed       | 3951     |\n",
      "|    total_timesteps    | 5300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.626   |\n",
      "|    explained_variance | 0.0146   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 52999    |\n",
      "|    policy_loss        | -4.86    |\n",
      "|    value_loss         | 73.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 5301000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.94\n",
      "Num timesteps: 5302000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.62\n",
      "Num timesteps: 5303000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.16\n",
      "Num timesteps: 5304000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.64\n",
      "Num timesteps: 5305000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.89\n",
      "Num timesteps: 5306000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.07\n",
      "Num timesteps: 5307000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.31\n",
      "Num timesteps: 5308000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.56\n",
      "Num timesteps: 5309000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.16\n",
      "Num timesteps: 5310000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 227      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 53100    |\n",
      "|    time_elapsed       | 3957     |\n",
      "|    total_timesteps    | 5310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.638   |\n",
      "|    explained_variance | -0.0841  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53099    |\n",
      "|    policy_loss        | -0.333   |\n",
      "|    value_loss         | 0.318    |\n",
      "------------------------------------\n",
      "Num timesteps: 5311000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.01\n",
      "Num timesteps: 5312000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.77\n",
      "Num timesteps: 5313000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.43\n",
      "Num timesteps: 5314000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.13\n",
      "Num timesteps: 5315000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.23\n",
      "Num timesteps: 5316000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.86\n",
      "Num timesteps: 5317000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.94\n",
      "Num timesteps: 5318000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.91\n",
      "Num timesteps: 5319000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.41\n",
      "Num timesteps: 5320000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 245      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 53200    |\n",
      "|    time_elapsed       | 3964     |\n",
      "|    total_timesteps    | 5320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.345   |\n",
      "|    explained_variance | 0.836    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53199    |\n",
      "|    policy_loss        | 4.59     |\n",
      "|    value_loss         | 313      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5321000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.47\n",
      "Num timesteps: 5322000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.52\n",
      "Num timesteps: 5323000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.00\n",
      "Num timesteps: 5324000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.19\n",
      "Num timesteps: 5325000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.52\n",
      "Num timesteps: 5326000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.49\n",
      "Num timesteps: 5327000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.57\n",
      "Num timesteps: 5328000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.66\n",
      "Num timesteps: 5329000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.86\n",
      "Num timesteps: 5330000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 238      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 53300    |\n",
      "|    time_elapsed       | 3969     |\n",
      "|    total_timesteps    | 5330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.432   |\n",
      "|    explained_variance | -0.0902  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53299    |\n",
      "|    policy_loss        | 6.23     |\n",
      "|    value_loss         | 210      |\n",
      "------------------------------------\n",
      "Num timesteps: 5331000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.60\n",
      "Num timesteps: 5332000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.99\n",
      "Num timesteps: 5333000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.32\n",
      "Num timesteps: 5334000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.52\n",
      "Num timesteps: 5335000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.78\n",
      "Num timesteps: 5336000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 5337000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.22\n",
      "Num timesteps: 5338000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.43\n",
      "Num timesteps: 5339000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.14\n",
      "Num timesteps: 5340000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.70\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 229      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 53400    |\n",
      "|    time_elapsed       | 3975     |\n",
      "|    total_timesteps    | 5340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.398   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53399    |\n",
      "|    policy_loss        | -2.81    |\n",
      "|    value_loss         | 93.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 5341000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.43\n",
      "Num timesteps: 5342000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.53\n",
      "Num timesteps: 5343000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.22\n",
      "Num timesteps: 5344000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.88\n",
      "Num timesteps: 5345000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.98\n",
      "Num timesteps: 5346000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.05\n",
      "Num timesteps: 5347000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.33\n",
      "Num timesteps: 5348000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.41\n",
      "Num timesteps: 5349000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.73\n",
      "Num timesteps: 5350000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 221      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 53500    |\n",
      "|    time_elapsed       | 3981     |\n",
      "|    total_timesteps    | 5350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53499    |\n",
      "|    policy_loss        | -23.2    |\n",
      "|    value_loss         | 5.78e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5351000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.75\n",
      "Num timesteps: 5352000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.82\n",
      "Num timesteps: 5353000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.47\n",
      "Num timesteps: 5354000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.91\n",
      "Num timesteps: 5355000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.44\n",
      "Num timesteps: 5356000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.21\n",
      "Num timesteps: 5357000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.06\n",
      "Num timesteps: 5358000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.72\n",
      "Num timesteps: 5359000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.94\n",
      "Num timesteps: 5360000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 197      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 53600    |\n",
      "|    time_elapsed       | 3986     |\n",
      "|    total_timesteps    | 5360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.494   |\n",
      "|    explained_variance | 0.00295  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53599    |\n",
      "|    policy_loss        | -0.909   |\n",
      "|    value_loss         | 6        |\n",
      "------------------------------------\n",
      "Num timesteps: 5361000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.94\n",
      "Num timesteps: 5362000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.83\n",
      "Num timesteps: 5363000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.96\n",
      "Num timesteps: 5364000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.31\n",
      "Num timesteps: 5365000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.13\n",
      "Num timesteps: 5366000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.40\n",
      "Num timesteps: 5367000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.86\n",
      "Num timesteps: 5368000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.30\n",
      "Num timesteps: 5369000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.86\n",
      "Num timesteps: 5370000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 193      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 53700    |\n",
      "|    time_elapsed       | 3993     |\n",
      "|    total_timesteps    | 5370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | 0.0291   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53699    |\n",
      "|    policy_loss        | 0.274    |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "Num timesteps: 5371000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.26\n",
      "Num timesteps: 5372000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.87\n",
      "Num timesteps: 5373000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.29\n",
      "Num timesteps: 5374000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.57\n",
      "Num timesteps: 5375000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.75\n",
      "Num timesteps: 5376000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.74\n",
      "Num timesteps: 5377000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.85\n",
      "Num timesteps: 5378000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.60\n",
      "Num timesteps: 5379000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5380000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 210      |\n",
      "|    ep_rew_mean        | 132      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 53800    |\n",
      "|    time_elapsed       | 3998     |\n",
      "|    total_timesteps    | 5380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.642   |\n",
      "|    explained_variance | -0.111   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53799    |\n",
      "|    policy_loss        | -0.161   |\n",
      "|    value_loss         | 0.21     |\n",
      "------------------------------------\n",
      "Num timesteps: 5381000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.76\n",
      "Num timesteps: 5382000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.62\n",
      "Num timesteps: 5383000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.20\n",
      "Num timesteps: 5384000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.83\n",
      "Num timesteps: 5385000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.88\n",
      "Num timesteps: 5386000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.70\n",
      "Num timesteps: 5387000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 118.14\n",
      "Num timesteps: 5388000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.32\n",
      "Num timesteps: 5389000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.19\n",
      "Num timesteps: 5390000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.91\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 219      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 53900    |\n",
      "|    time_elapsed       | 4004     |\n",
      "|    total_timesteps    | 5390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.273   |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53899    |\n",
      "|    policy_loss        | 7.21     |\n",
      "|    value_loss         | 485      |\n",
      "------------------------------------\n",
      "Num timesteps: 5391000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.12\n",
      "Num timesteps: 5392000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 115.68\n",
      "Num timesteps: 5393000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.17\n",
      "Num timesteps: 5394000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.15\n",
      "Num timesteps: 5395000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.46\n",
      "Num timesteps: 5396000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.51\n",
      "Num timesteps: 5397000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 115.65\n",
      "Num timesteps: 5398000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.18\n",
      "Num timesteps: 5399000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.18\n",
      "Num timesteps: 5400000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 210      |\n",
      "|    ep_rew_mean        | 127      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 54000    |\n",
      "|    time_elapsed       | 4009     |\n",
      "|    total_timesteps    | 5400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | -0.949   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 53999    |\n",
      "|    policy_loss        | -0.133   |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "Num timesteps: 5401000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.59\n",
      "Num timesteps: 5402000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.03\n",
      "Num timesteps: 5403000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.72\n",
      "Num timesteps: 5404000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.59\n",
      "Num timesteps: 5405000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.52\n",
      "Num timesteps: 5406000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.98\n",
      "Num timesteps: 5407000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.97\n",
      "Num timesteps: 5408000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.09\n",
      "Num timesteps: 5409000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.97\n",
      "Num timesteps: 5410000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 204      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 54100    |\n",
      "|    time_elapsed       | 4015     |\n",
      "|    total_timesteps    | 5410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.158    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54099    |\n",
      "|    policy_loss        | 0.631    |\n",
      "|    value_loss         | 537      |\n",
      "------------------------------------\n",
      "Num timesteps: 5411000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.89\n",
      "Num timesteps: 5412000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.51\n",
      "Num timesteps: 5413000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.88\n",
      "Num timesteps: 5414000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.56\n",
      "Num timesteps: 5415000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.90\n",
      "Num timesteps: 5416000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.89\n",
      "Num timesteps: 5417000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.02\n",
      "Num timesteps: 5418000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.21\n",
      "Num timesteps: 5419000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.39\n",
      "Num timesteps: 5420000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 217      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 54200    |\n",
      "|    time_elapsed       | 4020     |\n",
      "|    total_timesteps    | 5420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | -0.81    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54199    |\n",
      "|    policy_loss        | -0.764   |\n",
      "|    value_loss         | 1.83     |\n",
      "------------------------------------\n",
      "Num timesteps: 5421000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.30\n",
      "Num timesteps: 5422000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.94\n",
      "Num timesteps: 5423000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.46\n",
      "Num timesteps: 5424000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.26\n",
      "Num timesteps: 5425000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.94\n",
      "Num timesteps: 5426000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.63\n",
      "Num timesteps: 5427000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.47\n",
      "Num timesteps: 5428000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.11\n",
      "Num timesteps: 5429000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.32\n",
      "Num timesteps: 5430000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.15\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 241      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 54300    |\n",
      "|    time_elapsed       | 4027     |\n",
      "|    total_timesteps    | 5430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | 0.563    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54299    |\n",
      "|    policy_loss        | -0.126   |\n",
      "|    value_loss         | 0.0639   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5431000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.78\n",
      "Num timesteps: 5432000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.53\n",
      "Num timesteps: 5433000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.01\n",
      "Num timesteps: 5434000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.60\n",
      "Num timesteps: 5435000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.25\n",
      "Num timesteps: 5436000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.95\n",
      "Num timesteps: 5437000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.74\n",
      "Num timesteps: 5438000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.31\n",
      "Num timesteps: 5439000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.05\n",
      "Num timesteps: 5440000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 54400    |\n",
      "|    time_elapsed       | 4035     |\n",
      "|    total_timesteps    | 5440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.555   |\n",
      "|    explained_variance | -0.0837  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54399    |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0203   |\n",
      "------------------------------------\n",
      "Num timesteps: 5441000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.99\n",
      "Num timesteps: 5442000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.54\n",
      "Num timesteps: 5443000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.76\n",
      "Num timesteps: 5444000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.71\n",
      "Num timesteps: 5445000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.52\n",
      "Num timesteps: 5446000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.06\n",
      "Num timesteps: 5447000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.20\n",
      "Num timesteps: 5448000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.08\n",
      "Num timesteps: 5449000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.30\n",
      "Num timesteps: 5450000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 54500    |\n",
      "|    time_elapsed       | 4042     |\n",
      "|    total_timesteps    | 5450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.499   |\n",
      "|    explained_variance | -0.00904 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54499    |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 0.0912   |\n",
      "------------------------------------\n",
      "Num timesteps: 5451000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.95\n",
      "Num timesteps: 5452000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.78\n",
      "Num timesteps: 5453000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.13\n",
      "Num timesteps: 5454000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.51\n",
      "Num timesteps: 5455000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.17\n",
      "Num timesteps: 5456000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.23\n",
      "Num timesteps: 5457000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.02\n",
      "Num timesteps: 5458000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.51\n",
      "Num timesteps: 5459000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.72\n",
      "Num timesteps: 5460000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 54600    |\n",
      "|    time_elapsed       | 4048     |\n",
      "|    total_timesteps    | 5460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.615   |\n",
      "|    explained_variance | 0.0783   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54599    |\n",
      "|    policy_loss        | -0.00208 |\n",
      "|    value_loss         | 0.0568   |\n",
      "------------------------------------\n",
      "Num timesteps: 5461000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.70\n",
      "Num timesteps: 5462000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.30\n",
      "Num timesteps: 5463000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.68\n",
      "Num timesteps: 5464000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.01\n",
      "Num timesteps: 5465000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.73\n",
      "Num timesteps: 5466000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.60\n",
      "Num timesteps: 5467000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.26\n",
      "Num timesteps: 5468000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.29\n",
      "Num timesteps: 5469000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.31\n",
      "Num timesteps: 5470000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 258      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 54700    |\n",
      "|    time_elapsed       | 4054     |\n",
      "|    total_timesteps    | 5470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.483   |\n",
      "|    explained_variance | -0.0191  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54699    |\n",
      "|    policy_loss        | -1.87    |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 5471000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.43\n",
      "Num timesteps: 5472000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.35\n",
      "Num timesteps: 5473000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.04\n",
      "Num timesteps: 5474000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.21\n",
      "Num timesteps: 5475000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.70\n",
      "Num timesteps: 5476000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.39\n",
      "Num timesteps: 5477000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.36\n",
      "Num timesteps: 5478000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.56\n",
      "Num timesteps: 5479000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.80\n",
      "Num timesteps: 5480000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 254      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 54800    |\n",
      "|    time_elapsed       | 4061     |\n",
      "|    total_timesteps    | 5480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | -0.977   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54799    |\n",
      "|    policy_loss        | 0.0283   |\n",
      "|    value_loss         | 0.0649   |\n",
      "------------------------------------\n",
      "Num timesteps: 5481000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.31\n",
      "Num timesteps: 5482000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.12\n",
      "Num timesteps: 5483000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.51\n",
      "Num timesteps: 5484000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.02\n",
      "Num timesteps: 5485000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.50\n",
      "Num timesteps: 5486000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.66\n",
      "Num timesteps: 5487000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.35\n",
      "Num timesteps: 5488000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.28\n",
      "Num timesteps: 5489000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5490000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 259      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 54900    |\n",
      "|    time_elapsed       | 4067     |\n",
      "|    total_timesteps    | 5490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.498   |\n",
      "|    explained_variance | 0.534    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54899    |\n",
      "|    policy_loss        | -6.86    |\n",
      "|    value_loss         | 2.09e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5491000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.28\n",
      "Num timesteps: 5492000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.55\n",
      "Num timesteps: 5493000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.11\n",
      "Num timesteps: 5494000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.13\n",
      "Num timesteps: 5495000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.46\n",
      "Num timesteps: 5496000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.63\n",
      "Num timesteps: 5497000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.65\n",
      "Num timesteps: 5498000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.87\n",
      "Num timesteps: 5499000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.30\n",
      "Num timesteps: 5500000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 221      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 55000    |\n",
      "|    time_elapsed       | 4072     |\n",
      "|    total_timesteps    | 5500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.472   |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 54999    |\n",
      "|    policy_loss        | -11.2    |\n",
      "|    value_loss         | 2.24e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5501000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.28\n",
      "Num timesteps: 5502000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.15\n",
      "Num timesteps: 5503000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.57\n",
      "Num timesteps: 5504000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.67\n",
      "Num timesteps: 5505000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.67\n",
      "Num timesteps: 5506000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.48\n",
      "Num timesteps: 5507000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.91\n",
      "Num timesteps: 5508000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.35\n",
      "Num timesteps: 5509000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.64\n",
      "Num timesteps: 5510000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 203      |\n",
      "|    ep_rew_mean        | 136      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55100    |\n",
      "|    time_elapsed       | 4078     |\n",
      "|    total_timesteps    | 5510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.377   |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55099    |\n",
      "|    policy_loss        | 2.6      |\n",
      "|    value_loss         | 177      |\n",
      "------------------------------------\n",
      "Num timesteps: 5511000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.73\n",
      "Num timesteps: 5512000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.39\n",
      "Num timesteps: 5513000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.99\n",
      "Num timesteps: 5514000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.10\n",
      "Num timesteps: 5515000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.57\n",
      "Num timesteps: 5516000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.47\n",
      "Num timesteps: 5517000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.40\n",
      "Num timesteps: 5518000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.65\n",
      "Num timesteps: 5519000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.17\n",
      "Num timesteps: 5520000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 240      |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55200    |\n",
      "|    time_elapsed       | 4085     |\n",
      "|    total_timesteps    | 5520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.356   |\n",
      "|    explained_variance | 0.631    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55199    |\n",
      "|    policy_loss        | -13.6    |\n",
      "|    value_loss         | 3.52e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5521000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.49\n",
      "Num timesteps: 5522000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.26\n",
      "Num timesteps: 5523000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.70\n",
      "Num timesteps: 5524000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.67\n",
      "Num timesteps: 5525000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.70\n",
      "Num timesteps: 5526000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.87\n",
      "Num timesteps: 5527000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.90\n",
      "Num timesteps: 5528000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.32\n",
      "Num timesteps: 5529000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.02\n",
      "Num timesteps: 5530000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 271      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55300    |\n",
      "|    time_elapsed       | 4092     |\n",
      "|    total_timesteps    | 5530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.325   |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55299    |\n",
      "|    policy_loss        | -4.66    |\n",
      "|    value_loss         | 2.52e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5531000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.08\n",
      "Num timesteps: 5532000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.73\n",
      "Num timesteps: 5533000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.49\n",
      "Num timesteps: 5534000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.79\n",
      "Num timesteps: 5535000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.51\n",
      "Num timesteps: 5536000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.13\n",
      "Num timesteps: 5537000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.31\n",
      "Num timesteps: 5538000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.37\n",
      "Num timesteps: 5539000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.01\n",
      "Num timesteps: 5540000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55400    |\n",
      "|    time_elapsed       | 4099     |\n",
      "|    total_timesteps    | 5540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.672   |\n",
      "|    explained_variance | -1.49    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55399    |\n",
      "|    policy_loss        | -0.424   |\n",
      "|    value_loss         | 0.591    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5541000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.68\n",
      "Num timesteps: 5542000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.14\n",
      "Num timesteps: 5543000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.80\n",
      "Num timesteps: 5544000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.27\n",
      "Num timesteps: 5545000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.49\n",
      "Num timesteps: 5546000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.83\n",
      "Num timesteps: 5547000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.84\n",
      "Num timesteps: 5548000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.91\n",
      "Num timesteps: 5549000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.63\n",
      "Num timesteps: 5550000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.40\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 280      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55500    |\n",
      "|    time_elapsed       | 4105     |\n",
      "|    total_timesteps    | 5550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.461   |\n",
      "|    explained_variance | -0.00174 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55499    |\n",
      "|    policy_loss        | 0.0817   |\n",
      "|    value_loss         | 0.0351   |\n",
      "------------------------------------\n",
      "Num timesteps: 5551000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.58\n",
      "Num timesteps: 5552000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.07\n",
      "Num timesteps: 5553000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.24\n",
      "Num timesteps: 5554000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.90\n",
      "Num timesteps: 5555000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.22\n",
      "Num timesteps: 5556000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.38\n",
      "Num timesteps: 5557000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.78\n",
      "Num timesteps: 5558000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.35\n",
      "Num timesteps: 5559000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.25\n",
      "Num timesteps: 5560000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.28\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 295      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55600    |\n",
      "|    time_elapsed       | 4112     |\n",
      "|    total_timesteps    | 5560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.899   |\n",
      "|    explained_variance | -0.397   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55599    |\n",
      "|    policy_loss        | -5.02    |\n",
      "|    value_loss         | 41.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 5561000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.33\n",
      "Num timesteps: 5562000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.47\n",
      "Num timesteps: 5563000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.49\n",
      "Num timesteps: 5564000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.41\n",
      "Num timesteps: 5565000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.46\n",
      "Num timesteps: 5566000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.84\n",
      "Num timesteps: 5567000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.70\n",
      "Num timesteps: 5568000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.77\n",
      "Num timesteps: 5569000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.18\n",
      "Num timesteps: 5570000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55700    |\n",
      "|    time_elapsed       | 4120     |\n",
      "|    total_timesteps    | 5570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.924   |\n",
      "|    explained_variance | 0.231    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55699    |\n",
      "|    policy_loss        | -1.91    |\n",
      "|    value_loss         | 7.33     |\n",
      "------------------------------------\n",
      "Num timesteps: 5571000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.03\n",
      "Num timesteps: 5572000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.64\n",
      "Num timesteps: 5573000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.82\n",
      "Num timesteps: 5574000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.51\n",
      "Num timesteps: 5575000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.95\n",
      "Num timesteps: 5576000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.57\n",
      "Num timesteps: 5577000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.79\n",
      "Num timesteps: 5578000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.09\n",
      "Num timesteps: 5579000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.35\n",
      "Num timesteps: 5580000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 351      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 55800    |\n",
      "|    time_elapsed       | 4129     |\n",
      "|    total_timesteps    | 5580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55799    |\n",
      "|    policy_loss        | -1.54    |\n",
      "|    value_loss         | 24.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 5581000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.26\n",
      "Num timesteps: 5582000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.85\n",
      "Num timesteps: 5583000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.36\n",
      "Num timesteps: 5584000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.26\n",
      "Num timesteps: 5585000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.20\n",
      "Num timesteps: 5586000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.31\n",
      "Num timesteps: 5587000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.04\n",
      "Num timesteps: 5588000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.34\n",
      "Num timesteps: 5589000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.47\n",
      "Num timesteps: 5590000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 111.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 112      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 55900    |\n",
      "|    time_elapsed       | 4137     |\n",
      "|    total_timesteps    | 5590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | 0.0258   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55899    |\n",
      "|    policy_loss        | -2.58    |\n",
      "|    value_loss         | 18.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 5591000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.87\n",
      "Num timesteps: 5592000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 111.06\n",
      "Num timesteps: 5593000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.45\n",
      "Num timesteps: 5594000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 106.44\n",
      "Num timesteps: 5595000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.53\n",
      "Num timesteps: 5596000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 99.08\n",
      "Num timesteps: 5597000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 98.53\n",
      "Num timesteps: 5598000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.54\n",
      "Num timesteps: 5599000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 103.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5600000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 99.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 413      |\n",
      "|    ep_rew_mean        | 99.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 56000    |\n",
      "|    time_elapsed       | 4148     |\n",
      "|    total_timesteps    | 5600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.897   |\n",
      "|    explained_variance | 0.229    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 55999    |\n",
      "|    policy_loss        | -2.03    |\n",
      "|    value_loss         | 9.07     |\n",
      "------------------------------------\n",
      "Num timesteps: 5601000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.46\n",
      "Num timesteps: 5602000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.65\n",
      "Num timesteps: 5603000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 105.74\n",
      "Num timesteps: 5604000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 106.44\n",
      "Num timesteps: 5605000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.43\n",
      "Num timesteps: 5606000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.55\n",
      "Num timesteps: 5607000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.20\n",
      "Num timesteps: 5608000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.91\n",
      "Num timesteps: 5609000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.86\n",
      "Num timesteps: 5610000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 101.54\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 414      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56100    |\n",
      "|    time_elapsed       | 4157     |\n",
      "|    total_timesteps    | 5610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.68    |\n",
      "|    explained_variance | -1.34    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56099    |\n",
      "|    policy_loss        | 0.0583   |\n",
      "|    value_loss         | 0.0344   |\n",
      "------------------------------------\n",
      "Num timesteps: 5611000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.70\n",
      "Num timesteps: 5612000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 101.63\n",
      "Num timesteps: 5613000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 106.29\n",
      "Num timesteps: 5614000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.11\n",
      "Num timesteps: 5615000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.02\n",
      "Num timesteps: 5616000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 118.71\n",
      "Num timesteps: 5617000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 118.61\n",
      "Num timesteps: 5618000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.01\n",
      "Num timesteps: 5619000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.67\n",
      "Num timesteps: 5620000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 400      |\n",
      "|    ep_rew_mean        | 124      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56200    |\n",
      "|    time_elapsed       | 4165     |\n",
      "|    total_timesteps    | 5620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.483   |\n",
      "|    explained_variance | 0.743    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56199    |\n",
      "|    policy_loss        | -20.6    |\n",
      "|    value_loss         | 5.17e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5621000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.77\n",
      "Num timesteps: 5622000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.02\n",
      "Num timesteps: 5623000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.58\n",
      "Num timesteps: 5624000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.32\n",
      "Num timesteps: 5625000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.83\n",
      "Num timesteps: 5626000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.89\n",
      "Num timesteps: 5627000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.34\n",
      "Num timesteps: 5628000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.55\n",
      "Num timesteps: 5629000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.74\n",
      "Num timesteps: 5630000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 355      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 56300    |\n",
      "|    time_elapsed       | 4173     |\n",
      "|    total_timesteps    | 5630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.191   |\n",
      "|    explained_variance | -7.23    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56299    |\n",
      "|    policy_loss        | -2.52    |\n",
      "|    value_loss         | 3.98e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5631000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.39\n",
      "Num timesteps: 5632000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.43\n",
      "Num timesteps: 5633000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.42\n",
      "Num timesteps: 5634000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.77\n",
      "Num timesteps: 5635000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.58\n",
      "Num timesteps: 5636000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.46\n",
      "Num timesteps: 5637000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.00\n",
      "Num timesteps: 5638000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.27\n",
      "Num timesteps: 5639000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.41\n",
      "Num timesteps: 5640000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.49\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 291      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56400    |\n",
      "|    time_elapsed       | 4179     |\n",
      "|    total_timesteps    | 5640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.646   |\n",
      "|    explained_variance | -0.0607  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56399    |\n",
      "|    policy_loss        | -0.157   |\n",
      "|    value_loss         | 0.173    |\n",
      "------------------------------------\n",
      "Num timesteps: 5641000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.17\n",
      "Num timesteps: 5642000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.93\n",
      "Num timesteps: 5643000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.31\n",
      "Num timesteps: 5644000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.15\n",
      "Num timesteps: 5645000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.59\n",
      "Num timesteps: 5646000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.64\n",
      "Num timesteps: 5647000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.24\n",
      "Num timesteps: 5648000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.21\n",
      "Num timesteps: 5649000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.42\n",
      "Num timesteps: 5650000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.40\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 323      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56500    |\n",
      "|    time_elapsed       | 4187     |\n",
      "|    total_timesteps    | 5650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.808   |\n",
      "|    explained_variance | 0.416    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56499    |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 8.27     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5651000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.04\n",
      "Num timesteps: 5652000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.58\n",
      "Num timesteps: 5653000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.05\n",
      "Num timesteps: 5654000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.67\n",
      "Num timesteps: 5655000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.55\n",
      "Num timesteps: 5656000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.06\n",
      "Num timesteps: 5657000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.22\n",
      "Num timesteps: 5658000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.37\n",
      "Num timesteps: 5659000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.59\n",
      "Num timesteps: 5660000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.40\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56600    |\n",
      "|    time_elapsed       | 4193     |\n",
      "|    total_timesteps    | 5660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.369   |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56599    |\n",
      "|    policy_loss        | -41.6    |\n",
      "|    value_loss         | 8.34e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5661000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.10\n",
      "Num timesteps: 5662000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.25\n",
      "Num timesteps: 5663000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.70\n",
      "Num timesteps: 5664000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.89\n",
      "Num timesteps: 5665000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.44\n",
      "Num timesteps: 5666000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.50\n",
      "Num timesteps: 5667000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.04\n",
      "Num timesteps: 5668000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.43\n",
      "Num timesteps: 5669000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.37\n",
      "Num timesteps: 5670000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 56700    |\n",
      "|    time_elapsed       | 4200     |\n",
      "|    total_timesteps    | 5670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.327   |\n",
      "|    explained_variance | 0.953    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56699    |\n",
      "|    policy_loss        | 2.67     |\n",
      "|    value_loss         | 116      |\n",
      "------------------------------------\n",
      "Num timesteps: 5671000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.93\n",
      "Num timesteps: 5672000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.80\n",
      "Num timesteps: 5673000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.37\n",
      "Num timesteps: 5674000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.52\n",
      "Num timesteps: 5675000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.11\n",
      "Num timesteps: 5676000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.59\n",
      "Num timesteps: 5677000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.69\n",
      "Num timesteps: 5678000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 5679000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.82\n",
      "Num timesteps: 5680000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 244      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 56800    |\n",
      "|    time_elapsed       | 4207     |\n",
      "|    total_timesteps    | 5680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.752   |\n",
      "|    explained_variance | -5       |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56799    |\n",
      "|    policy_loss        | -22.4    |\n",
      "|    value_loss         | 902      |\n",
      "------------------------------------\n",
      "Num timesteps: 5681000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.75\n",
      "Num timesteps: 5682000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.71\n",
      "Num timesteps: 5683000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.77\n",
      "Num timesteps: 5684000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.92\n",
      "Num timesteps: 5685000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.60\n",
      "Num timesteps: 5686000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.24\n",
      "Num timesteps: 5687000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.46\n",
      "Num timesteps: 5688000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.67\n",
      "Num timesteps: 5689000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.41\n",
      "Num timesteps: 5690000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 257      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 56900    |\n",
      "|    time_elapsed       | 4213     |\n",
      "|    total_timesteps    | 5690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.486   |\n",
      "|    explained_variance | -3.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56899    |\n",
      "|    policy_loss        | 12.8     |\n",
      "|    value_loss         | 1.95e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5691000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.43\n",
      "Num timesteps: 5692000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.88\n",
      "Num timesteps: 5693000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.05\n",
      "Num timesteps: 5694000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.02\n",
      "Num timesteps: 5695000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.12\n",
      "Num timesteps: 5696000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.01\n",
      "Num timesteps: 5697000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.64\n",
      "Num timesteps: 5698000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.28\n",
      "Num timesteps: 5699000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.13\n",
      "Num timesteps: 5700000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 264      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 57000    |\n",
      "|    time_elapsed       | 4221     |\n",
      "|    total_timesteps    | 5700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | -0.2     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 56999    |\n",
      "|    policy_loss        | -2.13    |\n",
      "|    value_loss         | 16.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 5701000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.02\n",
      "Num timesteps: 5702000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.10\n",
      "Num timesteps: 5703000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.01\n",
      "Num timesteps: 5704000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.45\n",
      "Num timesteps: 5705000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.73\n",
      "Num timesteps: 5706000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.39\n",
      "Num timesteps: 5707000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.74\n",
      "Num timesteps: 5708000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.36\n",
      "Num timesteps: 5709000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5710000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 270      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 57100    |\n",
      "|    time_elapsed       | 4227     |\n",
      "|    total_timesteps    | 5710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.388   |\n",
      "|    explained_variance | 0.42     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57099    |\n",
      "|    policy_loss        | 10.2     |\n",
      "|    value_loss         | 687      |\n",
      "------------------------------------\n",
      "Num timesteps: 5711000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.20\n",
      "Num timesteps: 5712000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.55\n",
      "Num timesteps: 5713000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.64\n",
      "Num timesteps: 5714000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.96\n",
      "Num timesteps: 5715000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.74\n",
      "Num timesteps: 5716000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.82\n",
      "Num timesteps: 5717000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.81\n",
      "Num timesteps: 5718000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.22\n",
      "Num timesteps: 5719000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.62\n",
      "Num timesteps: 5720000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 300      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 57200    |\n",
      "|    time_elapsed       | 4237     |\n",
      "|    total_timesteps    | 5720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.884   |\n",
      "|    explained_variance | -0.112   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57199    |\n",
      "|    policy_loss        | -2.77    |\n",
      "|    value_loss         | 13.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 5721000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.23\n",
      "Num timesteps: 5722000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.65\n",
      "Num timesteps: 5723000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.68\n",
      "Num timesteps: 5724000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.78\n",
      "Num timesteps: 5725000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.01\n",
      "Num timesteps: 5726000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.40\n",
      "Num timesteps: 5727000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.82\n",
      "Num timesteps: 5728000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.64\n",
      "Num timesteps: 5729000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.02\n",
      "Num timesteps: 5730000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 314      |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 57300    |\n",
      "|    time_elapsed       | 4246     |\n",
      "|    total_timesteps    | 5730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 0.0869   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57299    |\n",
      "|    policy_loss        | -0.68    |\n",
      "|    value_loss         | 5.83     |\n",
      "------------------------------------\n",
      "Num timesteps: 5731000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.55\n",
      "Num timesteps: 5732000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.91\n",
      "Num timesteps: 5733000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.59\n",
      "Num timesteps: 5734000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.56\n",
      "Num timesteps: 5735000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.23\n",
      "Num timesteps: 5736000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.36\n",
      "Num timesteps: 5737000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.29\n",
      "Num timesteps: 5738000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.31\n",
      "Num timesteps: 5739000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.10\n",
      "Num timesteps: 5740000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 314      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 57400    |\n",
      "|    time_elapsed       | 4254     |\n",
      "|    total_timesteps    | 5740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | -0.0704  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57399    |\n",
      "|    policy_loss        | -3.12    |\n",
      "|    value_loss         | 19.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 5741000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.74\n",
      "Num timesteps: 5742000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.91\n",
      "Num timesteps: 5743000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.80\n",
      "Num timesteps: 5744000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.46\n",
      "Num timesteps: 5745000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.50\n",
      "Num timesteps: 5746000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.83\n",
      "Num timesteps: 5747000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.48\n",
      "Num timesteps: 5748000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.24\n",
      "Num timesteps: 5749000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.69\n",
      "Num timesteps: 5750000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 300      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 57500    |\n",
      "|    time_elapsed       | 4262     |\n",
      "|    total_timesteps    | 5750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.02    |\n",
      "|    explained_variance | -0.172   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57499    |\n",
      "|    policy_loss        | -0.322   |\n",
      "|    value_loss         | 5.01     |\n",
      "------------------------------------\n",
      "Num timesteps: 5751000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.52\n",
      "Num timesteps: 5752000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.16\n",
      "Num timesteps: 5753000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.42\n",
      "Num timesteps: 5754000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.39\n",
      "Num timesteps: 5755000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.04\n",
      "Num timesteps: 5756000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.18\n",
      "Num timesteps: 5757000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.94\n",
      "Num timesteps: 5758000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.55\n",
      "Num timesteps: 5759000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.03\n",
      "Num timesteps: 5760000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 286      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 57600    |\n",
      "|    time_elapsed       | 4272     |\n",
      "|    total_timesteps    | 5760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.334   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57599    |\n",
      "|    policy_loss        | 0.00877  |\n",
      "|    value_loss         | 49.5     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5761000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.70\n",
      "Num timesteps: 5762000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.96\n",
      "Num timesteps: 5763000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.22\n",
      "Num timesteps: 5764000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.44\n",
      "Num timesteps: 5765000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.36\n",
      "Num timesteps: 5766000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.69\n",
      "Num timesteps: 5767000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.34\n",
      "Num timesteps: 5768000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.88\n",
      "Num timesteps: 5769000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.26\n",
      "Num timesteps: 5770000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 57700    |\n",
      "|    time_elapsed       | 4282     |\n",
      "|    total_timesteps    | 5770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57699    |\n",
      "|    policy_loss        | 0.853    |\n",
      "|    value_loss         | 33.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 5771000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.47\n",
      "Num timesteps: 5772000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.37\n",
      "Num timesteps: 5773000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.87\n",
      "Num timesteps: 5774000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.97\n",
      "Num timesteps: 5775000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.48\n",
      "Num timesteps: 5776000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.27\n",
      "Num timesteps: 5777000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.70\n",
      "Num timesteps: 5778000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.15\n",
      "Num timesteps: 5779000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.25\n",
      "Num timesteps: 5780000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 337      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 57800    |\n",
      "|    time_elapsed       | 4292     |\n",
      "|    total_timesteps    | 5780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.0986   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57799    |\n",
      "|    policy_loss        | 0.0944   |\n",
      "|    value_loss         | 4.8      |\n",
      "------------------------------------\n",
      "Num timesteps: 5781000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.77\n",
      "Num timesteps: 5782000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.38\n",
      "Num timesteps: 5783000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.24\n",
      "Num timesteps: 5784000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.62\n",
      "Num timesteps: 5785000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 111.48\n",
      "Num timesteps: 5786000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.60\n",
      "Num timesteps: 5787000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.57\n",
      "Num timesteps: 5788000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.39\n",
      "Num timesteps: 5789000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 111.21\n",
      "Num timesteps: 5790000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 356      |\n",
      "|    ep_rew_mean        | 108      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 57900    |\n",
      "|    time_elapsed       | 4301     |\n",
      "|    total_timesteps    | 5790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.496   |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57899    |\n",
      "|    policy_loss        | -22.2    |\n",
      "|    value_loss         | 5.44e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5791000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.76\n",
      "Num timesteps: 5792000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 109.08\n",
      "Num timesteps: 5793000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.39\n",
      "Num timesteps: 5794000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.69\n",
      "Num timesteps: 5795000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.75\n",
      "Num timesteps: 5796000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.75\n",
      "Num timesteps: 5797000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 104.88\n",
      "Num timesteps: 5798000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 106.12\n",
      "Num timesteps: 5799000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 101.90\n",
      "Num timesteps: 5800000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 350      |\n",
      "|    ep_rew_mean        | 100      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 58000    |\n",
      "|    time_elapsed       | 4308     |\n",
      "|    total_timesteps    | 5800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.786   |\n",
      "|    explained_variance | 0.202    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 57999    |\n",
      "|    policy_loss        | -0.572   |\n",
      "|    value_loss         | 2.84     |\n",
      "------------------------------------\n",
      "Num timesteps: 5801000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 92.49\n",
      "Num timesteps: 5802000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 95.25\n",
      "Num timesteps: 5803000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 96.15\n",
      "Num timesteps: 5804000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 95.80\n",
      "Num timesteps: 5805000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 95.88\n",
      "Num timesteps: 5806000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 95.96\n",
      "Num timesteps: 5807000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 95.74\n",
      "Num timesteps: 5808000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.19\n",
      "Num timesteps: 5809000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 98.56\n",
      "Num timesteps: 5810000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 101.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 330      |\n",
      "|    ep_rew_mean        | 102      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 58100    |\n",
      "|    time_elapsed       | 4316     |\n",
      "|    total_timesteps    | 5810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.556   |\n",
      "|    explained_variance | 0.888    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58099    |\n",
      "|    policy_loss        | -0.256   |\n",
      "|    value_loss         | 106      |\n",
      "------------------------------------\n",
      "Num timesteps: 5811000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.89\n",
      "Num timesteps: 5812000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 107.69\n",
      "Num timesteps: 5813000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 105.80\n",
      "Num timesteps: 5814000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 102.87\n",
      "Num timesteps: 5815000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 102.02\n",
      "Num timesteps: 5816000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.24\n",
      "Num timesteps: 5817000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 100.28\n",
      "Num timesteps: 5818000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 103.40\n",
      "Num timesteps: 5819000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5820000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 113      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 58200    |\n",
      "|    time_elapsed       | 4324     |\n",
      "|    total_timesteps    | 5820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.406   |\n",
      "|    explained_variance | 0.978    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58199    |\n",
      "|    policy_loss        | 1.26     |\n",
      "|    value_loss         | 34       |\n",
      "------------------------------------\n",
      "Num timesteps: 5821000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.51\n",
      "Num timesteps: 5822000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.83\n",
      "Num timesteps: 5823000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.39\n",
      "Num timesteps: 5824000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.48\n",
      "Num timesteps: 5825000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.72\n",
      "Num timesteps: 5826000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.65\n",
      "Num timesteps: 5827000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.52\n",
      "Num timesteps: 5828000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.49\n",
      "Num timesteps: 5829000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.14\n",
      "Num timesteps: 5830000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 325      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 58300    |\n",
      "|    time_elapsed       | 4332     |\n",
      "|    total_timesteps    | 5830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58299    |\n",
      "|    policy_loss        | 2.04     |\n",
      "|    value_loss         | 44.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 5831000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.78\n",
      "Num timesteps: 5832000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.68\n",
      "Num timesteps: 5833000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.78\n",
      "Num timesteps: 5834000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.27\n",
      "Num timesteps: 5835000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.74\n",
      "Num timesteps: 5836000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.78\n",
      "Num timesteps: 5837000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.00\n",
      "Num timesteps: 5838000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.08\n",
      "Num timesteps: 5839000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.96\n",
      "Num timesteps: 5840000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.93\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 58400    |\n",
      "|    time_elapsed       | 4339     |\n",
      "|    total_timesteps    | 5840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.324   |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58399    |\n",
      "|    policy_loss        | 0.669    |\n",
      "|    value_loss         | 70.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 5841000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.02\n",
      "Num timesteps: 5842000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.18\n",
      "Num timesteps: 5843000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.34\n",
      "Num timesteps: 5844000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.22\n",
      "Num timesteps: 5845000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.24\n",
      "Num timesteps: 5846000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.63\n",
      "Num timesteps: 5847000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.35\n",
      "Num timesteps: 5848000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.55\n",
      "Num timesteps: 5849000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.73\n",
      "Num timesteps: 5850000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 58500    |\n",
      "|    time_elapsed       | 4348     |\n",
      "|    total_timesteps    | 5850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.895   |\n",
      "|    explained_variance | -0.108   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58499    |\n",
      "|    policy_loss        | 4.75     |\n",
      "|    value_loss         | 39.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 5851000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.31\n",
      "Num timesteps: 5852000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.61\n",
      "Num timesteps: 5853000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.05\n",
      "Num timesteps: 5854000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.80\n",
      "Num timesteps: 5855000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.38\n",
      "Num timesteps: 5856000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.14\n",
      "Num timesteps: 5857000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.28\n",
      "Num timesteps: 5858000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.66\n",
      "Num timesteps: 5859000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.20\n",
      "Num timesteps: 5860000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 360      |\n",
      "|    ep_rew_mean        | 128      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 58600    |\n",
      "|    time_elapsed       | 4359     |\n",
      "|    total_timesteps    | 5860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.586   |\n",
      "|    explained_variance | 0.504    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58599    |\n",
      "|    policy_loss        | -43.9    |\n",
      "|    value_loss         | 6.72e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5861000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.32\n",
      "Num timesteps: 5862000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.40\n",
      "Num timesteps: 5863000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.77\n",
      "Num timesteps: 5864000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.29\n",
      "Num timesteps: 5865000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.37\n",
      "Num timesteps: 5866000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.76\n",
      "Num timesteps: 5867000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.54\n",
      "Num timesteps: 5868000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.18\n",
      "Num timesteps: 5869000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.66\n",
      "Num timesteps: 5870000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 394      |\n",
      "|    ep_rew_mean        | 123      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 58700    |\n",
      "|    time_elapsed       | 4368     |\n",
      "|    total_timesteps    | 5870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.636   |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58699    |\n",
      "|    policy_loss        | -0.184   |\n",
      "|    value_loss         | 0.124    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5871000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.88\n",
      "Num timesteps: 5872000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.71\n",
      "Num timesteps: 5873000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.36\n",
      "Num timesteps: 5874000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.84\n",
      "Num timesteps: 5875000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.36\n",
      "Num timesteps: 5876000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.84\n",
      "Num timesteps: 5877000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.31\n",
      "Num timesteps: 5878000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.08\n",
      "Num timesteps: 5879000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.87\n",
      "Num timesteps: 5880000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 396      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 58800    |\n",
      "|    time_elapsed       | 4377     |\n",
      "|    total_timesteps    | 5880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.327   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58799    |\n",
      "|    policy_loss        | 3.07     |\n",
      "|    value_loss         | 83.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 5881000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.34\n",
      "Num timesteps: 5882000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.00\n",
      "Num timesteps: 5883000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.61\n",
      "Num timesteps: 5884000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.40\n",
      "Num timesteps: 5885000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.42\n",
      "Num timesteps: 5886000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.50\n",
      "Num timesteps: 5887000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.66\n",
      "Num timesteps: 5888000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.33\n",
      "Num timesteps: 5889000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.03\n",
      "Num timesteps: 5890000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.68\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 384      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 58900    |\n",
      "|    time_elapsed       | 4385     |\n",
      "|    total_timesteps    | 5890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58899    |\n",
      "|    policy_loss        | -1.04    |\n",
      "|    value_loss         | 24.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 5891000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.56\n",
      "Num timesteps: 5892000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.84\n",
      "Num timesteps: 5893000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.86\n",
      "Num timesteps: 5894000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.42\n",
      "Num timesteps: 5895000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.64\n",
      "Num timesteps: 5896000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.53\n",
      "Num timesteps: 5897000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.14\n",
      "Num timesteps: 5898000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.80\n",
      "Num timesteps: 5899000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.90\n",
      "Num timesteps: 5900000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.08\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 342      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 59000    |\n",
      "|    time_elapsed       | 4392     |\n",
      "|    total_timesteps    | 5900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.258   |\n",
      "|    explained_variance | -2.83    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 58999    |\n",
      "|    policy_loss        | 2.42     |\n",
      "|    value_loss         | 2.57e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 5901000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.02\n",
      "Num timesteps: 5902000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.15\n",
      "Num timesteps: 5903000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.83\n",
      "Num timesteps: 5904000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.95\n",
      "Num timesteps: 5905000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.31\n",
      "Num timesteps: 5906000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.94\n",
      "Num timesteps: 5907000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.39\n",
      "Num timesteps: 5908000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.29\n",
      "Num timesteps: 5909000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.55\n",
      "Num timesteps: 5910000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.72\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 348      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59100    |\n",
      "|    time_elapsed       | 4401     |\n",
      "|    total_timesteps    | 5910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.607   |\n",
      "|    explained_variance | -1.13    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59099    |\n",
      "|    policy_loss        | -0.212   |\n",
      "|    value_loss         | 0.177    |\n",
      "------------------------------------\n",
      "Num timesteps: 5911000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.19\n",
      "Num timesteps: 5912000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.72\n",
      "Num timesteps: 5913000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.89\n",
      "Num timesteps: 5914000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.97\n",
      "Num timesteps: 5915000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.37\n",
      "Num timesteps: 5916000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.14\n",
      "Num timesteps: 5917000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.32\n",
      "Num timesteps: 5918000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.84\n",
      "Num timesteps: 5919000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.11\n",
      "Num timesteps: 5920000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 366      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59200    |\n",
      "|    time_elapsed       | 4409     |\n",
      "|    total_timesteps    | 5920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.708   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59199    |\n",
      "|    policy_loss        | -0.884   |\n",
      "|    value_loss         | 22.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 5921000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.80\n",
      "Num timesteps: 5922000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.93\n",
      "Num timesteps: 5923000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.43\n",
      "Num timesteps: 5924000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.72\n",
      "Num timesteps: 5925000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.16\n",
      "Num timesteps: 5926000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.63\n",
      "Num timesteps: 5927000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.99\n",
      "Num timesteps: 5928000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.63\n",
      "Num timesteps: 5929000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5930000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 348      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59300    |\n",
      "|    time_elapsed       | 4416     |\n",
      "|    total_timesteps    | 5930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.717   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59299    |\n",
      "|    policy_loss        | 2.48     |\n",
      "|    value_loss         | 17       |\n",
      "------------------------------------\n",
      "Num timesteps: 5931000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.38\n",
      "Num timesteps: 5932000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.42\n",
      "Num timesteps: 5933000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.78\n",
      "Num timesteps: 5934000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.16\n",
      "Num timesteps: 5935000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.92\n",
      "Num timesteps: 5936000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.45\n",
      "Num timesteps: 5937000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.95\n",
      "Num timesteps: 5938000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.69\n",
      "Num timesteps: 5939000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.03\n",
      "Num timesteps: 5940000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 360      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59400    |\n",
      "|    time_elapsed       | 4425     |\n",
      "|    total_timesteps    | 5940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.264   |\n",
      "|    explained_variance | 0.729    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59399    |\n",
      "|    policy_loss        | 4.08     |\n",
      "|    value_loss         | 391      |\n",
      "------------------------------------\n",
      "Num timesteps: 5941000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.06\n",
      "Num timesteps: 5942000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.39\n",
      "Num timesteps: 5943000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.17\n",
      "Num timesteps: 5944000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.42\n",
      "Num timesteps: 5945000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.80\n",
      "Num timesteps: 5946000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.06\n",
      "Num timesteps: 5947000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.67\n",
      "Num timesteps: 5948000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.07\n",
      "Num timesteps: 5949000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.98\n",
      "Num timesteps: 5950000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 324      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59500    |\n",
      "|    time_elapsed       | 4432     |\n",
      "|    total_timesteps    | 5950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.888   |\n",
      "|    explained_variance | -0.0678  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59499    |\n",
      "|    policy_loss        | -2.2     |\n",
      "|    value_loss         | 23.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 5951000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.72\n",
      "Num timesteps: 5952000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.86\n",
      "Num timesteps: 5953000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.81\n",
      "Num timesteps: 5954000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.22\n",
      "Num timesteps: 5955000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.69\n",
      "Num timesteps: 5956000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.96\n",
      "Num timesteps: 5957000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.89\n",
      "Num timesteps: 5958000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.88\n",
      "Num timesteps: 5959000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.08\n",
      "Num timesteps: 5960000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 319      |\n",
      "|    ep_rew_mean        | 125      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59600    |\n",
      "|    time_elapsed       | 4440     |\n",
      "|    total_timesteps    | 5960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.624   |\n",
      "|    explained_variance | 0.355    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59599    |\n",
      "|    policy_loss        | 0.865    |\n",
      "|    value_loss         | 2.97     |\n",
      "------------------------------------\n",
      "Num timesteps: 5961000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.00\n",
      "Num timesteps: 5962000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.05\n",
      "Num timesteps: 5963000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.52\n",
      "Num timesteps: 5964000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.83\n",
      "Num timesteps: 5965000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.76\n",
      "Num timesteps: 5966000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.85\n",
      "Num timesteps: 5967000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.68\n",
      "Num timesteps: 5968000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.81\n",
      "Num timesteps: 5969000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.74\n",
      "Num timesteps: 5970000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 59700    |\n",
      "|    time_elapsed       | 4447     |\n",
      "|    total_timesteps    | 5970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | 0.166    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59699    |\n",
      "|    policy_loss        | 0.699    |\n",
      "|    value_loss         | 3.92     |\n",
      "------------------------------------\n",
      "Num timesteps: 5971000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.26\n",
      "Num timesteps: 5972000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.96\n",
      "Num timesteps: 5973000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.02\n",
      "Num timesteps: 5974000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.10\n",
      "Num timesteps: 5975000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.23\n",
      "Num timesteps: 5976000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.83\n",
      "Num timesteps: 5977000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.85\n",
      "Num timesteps: 5978000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.43\n",
      "Num timesteps: 5979000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.62\n",
      "Num timesteps: 5980000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 370      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 59800    |\n",
      "|    time_elapsed       | 4456     |\n",
      "|    total_timesteps    | 5980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.905   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59799    |\n",
      "|    policy_loss        | 1.72     |\n",
      "|    value_loss         | 14.6     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 5981000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.61\n",
      "Num timesteps: 5982000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.67\n",
      "Num timesteps: 5983000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.65\n",
      "Num timesteps: 5984000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.40\n",
      "Num timesteps: 5985000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.67\n",
      "Num timesteps: 5986000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.29\n",
      "Num timesteps: 5987000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.91\n",
      "Num timesteps: 5988000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.18\n",
      "Num timesteps: 5989000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.13\n",
      "Num timesteps: 5990000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 59900    |\n",
      "|    time_elapsed       | 4466     |\n",
      "|    total_timesteps    | 5990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59899    |\n",
      "|    policy_loss        | 1.28     |\n",
      "|    value_loss         | 10       |\n",
      "------------------------------------\n",
      "Num timesteps: 5991000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.14\n",
      "Num timesteps: 5992000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.12\n",
      "Num timesteps: 5993000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.15\n",
      "Num timesteps: 5994000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.33\n",
      "Num timesteps: 5995000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.30\n",
      "Num timesteps: 5996000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.59\n",
      "Num timesteps: 5997000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.96\n",
      "Num timesteps: 5998000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.83\n",
      "Num timesteps: 5999000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.46\n",
      "Num timesteps: 6000000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 373      |\n",
      "|    ep_rew_mean        | 146      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 60000    |\n",
      "|    time_elapsed       | 4474     |\n",
      "|    total_timesteps    | 6000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.978   |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 59999    |\n",
      "|    policy_loss        | -3.41    |\n",
      "|    value_loss         | 20.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6001000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.81\n",
      "Num timesteps: 6002000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.42\n",
      "Num timesteps: 6003000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.23\n",
      "Num timesteps: 6004000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.89\n",
      "Num timesteps: 6005000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.70\n",
      "Num timesteps: 6006000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.16\n",
      "Num timesteps: 6007000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.06\n",
      "Num timesteps: 6008000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.84\n",
      "Num timesteps: 6009000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.41\n",
      "Num timesteps: 6010000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 343      |\n",
      "|    ep_rew_mean        | 147      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60100    |\n",
      "|    time_elapsed       | 4480     |\n",
      "|    total_timesteps    | 6010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.418   |\n",
      "|    explained_variance | -0.183   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60099    |\n",
      "|    policy_loss        | -0.259   |\n",
      "|    value_loss         | 0.749    |\n",
      "------------------------------------\n",
      "Num timesteps: 6011000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.91\n",
      "Num timesteps: 6012000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.52\n",
      "Num timesteps: 6013000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.42\n",
      "Num timesteps: 6014000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.82\n",
      "Num timesteps: 6015000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.54\n",
      "Num timesteps: 6016000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.24\n",
      "Num timesteps: 6017000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.97\n",
      "Num timesteps: 6018000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.87\n",
      "Num timesteps: 6019000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.99\n",
      "Num timesteps: 6020000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 311      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60200    |\n",
      "|    time_elapsed       | 4488     |\n",
      "|    total_timesteps    | 6020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.819   |\n",
      "|    explained_variance | -2.31    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60199    |\n",
      "|    policy_loss        | 11.4     |\n",
      "|    value_loss         | 467      |\n",
      "------------------------------------\n",
      "Num timesteps: 6021000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.42\n",
      "Num timesteps: 6022000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.45\n",
      "Num timesteps: 6023000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.47\n",
      "Num timesteps: 6024000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.51\n",
      "Num timesteps: 6025000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.68\n",
      "Num timesteps: 6026000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.44\n",
      "Num timesteps: 6027000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.26\n",
      "Num timesteps: 6028000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.82\n",
      "Num timesteps: 6029000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.41\n",
      "Num timesteps: 6030000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.08\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 288      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60300    |\n",
      "|    time_elapsed       | 4495     |\n",
      "|    total_timesteps    | 6030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.262   |\n",
      "|    explained_variance | -0.684   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60299    |\n",
      "|    policy_loss        | -0.0454  |\n",
      "|    value_loss         | 1.37     |\n",
      "------------------------------------\n",
      "Num timesteps: 6031000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.69\n",
      "Num timesteps: 6032000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.75\n",
      "Num timesteps: 6033000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.47\n",
      "Num timesteps: 6034000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.64\n",
      "Num timesteps: 6035000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.08\n",
      "Num timesteps: 6036000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.87\n",
      "Num timesteps: 6037000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.60\n",
      "Num timesteps: 6038000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.53\n",
      "Num timesteps: 6039000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6040000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60400    |\n",
      "|    time_elapsed       | 4503     |\n",
      "|    total_timesteps    | 6040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.257    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60399    |\n",
      "|    policy_loss        | 3        |\n",
      "|    value_loss         | 19.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 6041000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.01\n",
      "Num timesteps: 6042000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.30\n",
      "Num timesteps: 6043000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.27\n",
      "Num timesteps: 6044000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.03\n",
      "Num timesteps: 6045000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.71\n",
      "Num timesteps: 6046000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.96\n",
      "Num timesteps: 6047000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.03\n",
      "Num timesteps: 6048000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.28\n",
      "Num timesteps: 6049000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.56\n",
      "Num timesteps: 6050000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.99\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 60500    |\n",
      "|    time_elapsed       | 4513     |\n",
      "|    total_timesteps    | 6050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | 0.166    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60499    |\n",
      "|    policy_loss        | -3.05    |\n",
      "|    value_loss         | 24.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6051000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.30\n",
      "Num timesteps: 6052000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.07\n",
      "Num timesteps: 6053000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.13\n",
      "Num timesteps: 6054000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.86\n",
      "Num timesteps: 6055000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.15\n",
      "Num timesteps: 6056000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.11\n",
      "Num timesteps: 6057000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.30\n",
      "Num timesteps: 6058000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.61\n",
      "Num timesteps: 6059000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.49\n",
      "Num timesteps: 6060000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 347      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 60600    |\n",
      "|    time_elapsed       | 4520     |\n",
      "|    total_timesteps    | 6060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | -1.09    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60599    |\n",
      "|    policy_loss        | 3.23     |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6061000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.31\n",
      "Num timesteps: 6062000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.28\n",
      "Num timesteps: 6063000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.87\n",
      "Num timesteps: 6064000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.11\n",
      "Num timesteps: 6065000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.36\n",
      "Num timesteps: 6066000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.28\n",
      "Num timesteps: 6067000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.29\n",
      "Num timesteps: 6068000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.51\n",
      "Num timesteps: 6069000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.57\n",
      "Num timesteps: 6070000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 321      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60700    |\n",
      "|    time_elapsed       | 4526     |\n",
      "|    total_timesteps    | 6070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | -0.879   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60699    |\n",
      "|    policy_loss        | -0.951   |\n",
      "|    value_loss         | 3        |\n",
      "------------------------------------\n",
      "Num timesteps: 6071000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.55\n",
      "Num timesteps: 6072000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.09\n",
      "Num timesteps: 6073000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.28\n",
      "Num timesteps: 6074000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.56\n",
      "Num timesteps: 6075000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.10\n",
      "Num timesteps: 6076000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.50\n",
      "Num timesteps: 6077000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.52\n",
      "Num timesteps: 6078000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.10\n",
      "Num timesteps: 6079000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.19\n",
      "Num timesteps: 6080000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 306      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60800    |\n",
      "|    time_elapsed       | 4533     |\n",
      "|    total_timesteps    | 6080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60799    |\n",
      "|    policy_loss        | 1.13     |\n",
      "|    value_loss         | 1.39e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6081000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.53\n",
      "Num timesteps: 6082000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.79\n",
      "Num timesteps: 6083000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.42\n",
      "Num timesteps: 6084000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.11\n",
      "Num timesteps: 6085000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.64\n",
      "Num timesteps: 6086000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.91\n",
      "Num timesteps: 6087000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.24\n",
      "Num timesteps: 6088000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.89\n",
      "Num timesteps: 6089000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.68\n",
      "Num timesteps: 6090000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 60900    |\n",
      "|    time_elapsed       | 4540     |\n",
      "|    total_timesteps    | 6090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | -0.292   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60899    |\n",
      "|    policy_loss        | 7.15     |\n",
      "|    value_loss         | 54.3     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6091000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.72\n",
      "Num timesteps: 6092000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.10\n",
      "Num timesteps: 6093000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.07\n",
      "Num timesteps: 6094000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.47\n",
      "Num timesteps: 6095000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.26\n",
      "Num timesteps: 6096000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.85\n",
      "Num timesteps: 6097000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.57\n",
      "Num timesteps: 6098000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.35\n",
      "Num timesteps: 6099000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.90\n",
      "Num timesteps: 6100000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 293      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 61000    |\n",
      "|    time_elapsed       | 4546     |\n",
      "|    total_timesteps    | 6100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 60999    |\n",
      "|    policy_loss        | 3.08     |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "Num timesteps: 6101000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.58\n",
      "Num timesteps: 6102000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.37\n",
      "Num timesteps: 6103000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.99\n",
      "Num timesteps: 6104000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.81\n",
      "Num timesteps: 6105000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.64\n",
      "Num timesteps: 6106000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.98\n",
      "Num timesteps: 6107000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.98\n",
      "Num timesteps: 6108000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.24\n",
      "Num timesteps: 6109000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.26\n",
      "Num timesteps: 6110000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 278      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 61100    |\n",
      "|    time_elapsed       | 4552     |\n",
      "|    total_timesteps    | 6110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.48    |\n",
      "|    explained_variance | 0.97     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61099    |\n",
      "|    policy_loss        | 3.47     |\n",
      "|    value_loss         | 89.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 6111000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.83\n",
      "Num timesteps: 6112000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.85\n",
      "Num timesteps: 6113000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.25\n",
      "Num timesteps: 6114000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.10\n",
      "Num timesteps: 6115000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.08\n",
      "Num timesteps: 6116000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.93\n",
      "Num timesteps: 6117000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.15\n",
      "Num timesteps: 6118000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.63\n",
      "Num timesteps: 6119000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.54\n",
      "Num timesteps: 6120000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 282      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 61200    |\n",
      "|    time_elapsed       | 4559     |\n",
      "|    total_timesteps    | 6120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | -3.05    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61199    |\n",
      "|    policy_loss        | 9.92     |\n",
      "|    value_loss         | 295      |\n",
      "------------------------------------\n",
      "Num timesteps: 6121000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.17\n",
      "Num timesteps: 6122000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.11\n",
      "Num timesteps: 6123000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.38\n",
      "Num timesteps: 6124000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.27\n",
      "Num timesteps: 6125000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.65\n",
      "Num timesteps: 6126000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.43\n",
      "Num timesteps: 6127000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.35\n",
      "Num timesteps: 6128000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.42\n",
      "Num timesteps: 6129000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.86\n",
      "Num timesteps: 6130000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 271      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 61300    |\n",
      "|    time_elapsed       | 4565     |\n",
      "|    total_timesteps    | 6130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.534   |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61299    |\n",
      "|    policy_loss        | 2.83     |\n",
      "|    value_loss         | 89.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 6131000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.81\n",
      "Num timesteps: 6132000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.47\n",
      "Num timesteps: 6133000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.36\n",
      "Num timesteps: 6134000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.80\n",
      "Num timesteps: 6135000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.04\n",
      "Num timesteps: 6136000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.95\n",
      "Num timesteps: 6137000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.00\n",
      "Num timesteps: 6138000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.46\n",
      "Num timesteps: 6139000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.86\n",
      "Num timesteps: 6140000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 292      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 61400    |\n",
      "|    time_elapsed       | 4571     |\n",
      "|    total_timesteps    | 6140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.348   |\n",
      "|    explained_variance | 0.0724   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61399    |\n",
      "|    policy_loss        | -24.3    |\n",
      "|    value_loss         | 7.48e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6141000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.96\n",
      "Num timesteps: 6142000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.63\n",
      "Num timesteps: 6143000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.87\n",
      "Num timesteps: 6144000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.16\n",
      "Num timesteps: 6145000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.39\n",
      "Num timesteps: 6146000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.20\n",
      "Num timesteps: 6147000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.47\n",
      "Num timesteps: 6148000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.40\n",
      "Num timesteps: 6149000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6150000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.53\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 299      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 61500    |\n",
      "|    time_elapsed       | 4579     |\n",
      "|    total_timesteps    | 6150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61499    |\n",
      "|    policy_loss        | -7.95    |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "Num timesteps: 6151000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.92\n",
      "Num timesteps: 6152000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.32\n",
      "Num timesteps: 6153000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.96\n",
      "Num timesteps: 6154000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.42\n",
      "Num timesteps: 6155000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.76\n",
      "Num timesteps: 6156000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.32\n",
      "Num timesteps: 6157000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.12\n",
      "Num timesteps: 6158000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.11\n",
      "Num timesteps: 6159000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.86\n",
      "Num timesteps: 6160000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 61600    |\n",
      "|    time_elapsed       | 4585     |\n",
      "|    total_timesteps    | 6160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.525   |\n",
      "|    explained_variance | -0.154   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61599    |\n",
      "|    policy_loss        | -0.345   |\n",
      "|    value_loss         | 0.549    |\n",
      "------------------------------------\n",
      "Num timesteps: 6161000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.30\n",
      "Num timesteps: 6162000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.78\n",
      "Num timesteps: 6163000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.10\n",
      "Num timesteps: 6164000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.81\n",
      "Num timesteps: 6165000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.42\n",
      "Num timesteps: 6166000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.63\n",
      "Num timesteps: 6167000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.17\n",
      "Num timesteps: 6168000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.04\n",
      "Num timesteps: 6169000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.15\n",
      "Num timesteps: 6170000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 277      |\n",
      "|    ep_rew_mean        | 184      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 61700    |\n",
      "|    time_elapsed       | 4591     |\n",
      "|    total_timesteps    | 6170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.705   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61699    |\n",
      "|    policy_loss        | -2.51    |\n",
      "|    value_loss         | 20.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 6171000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.58\n",
      "Num timesteps: 6172000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.04\n",
      "Num timesteps: 6173000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.66\n",
      "Num timesteps: 6174000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.74\n",
      "Num timesteps: 6175000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.07\n",
      "Num timesteps: 6176000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.57\n",
      "Num timesteps: 6177000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.13\n",
      "Num timesteps: 6178000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.70\n",
      "Num timesteps: 6179000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.06\n",
      "Num timesteps: 6180000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 274      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 61800    |\n",
      "|    time_elapsed       | 4597     |\n",
      "|    total_timesteps    | 6180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.476   |\n",
      "|    explained_variance | -7.56    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61799    |\n",
      "|    policy_loss        | -0.98    |\n",
      "|    value_loss         | 31.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 6181000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.01\n",
      "Num timesteps: 6182000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.72\n",
      "Num timesteps: 6183000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.95\n",
      "Num timesteps: 6184000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.58\n",
      "Num timesteps: 6185000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.80\n",
      "Num timesteps: 6186000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.71\n",
      "Num timesteps: 6187000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.27\n",
      "Num timesteps: 6188000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.89\n",
      "Num timesteps: 6189000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.58\n",
      "Num timesteps: 6190000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 285      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 61900    |\n",
      "|    time_elapsed       | 4604     |\n",
      "|    total_timesteps    | 6190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.247   |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61899    |\n",
      "|    policy_loss        | -1.51    |\n",
      "|    value_loss         | 95.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6191000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.05\n",
      "Num timesteps: 6192000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.33\n",
      "Num timesteps: 6193000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.96\n",
      "Num timesteps: 6194000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.61\n",
      "Num timesteps: 6195000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.30\n",
      "Num timesteps: 6196000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.25\n",
      "Num timesteps: 6197000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.02\n",
      "Num timesteps: 6198000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.80\n",
      "Num timesteps: 6199000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.21\n",
      "Num timesteps: 6200000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 191      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 62000    |\n",
      "|    time_elapsed       | 4609     |\n",
      "|    total_timesteps    | 6200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.542   |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 61999    |\n",
      "|    policy_loss        | 2.3      |\n",
      "|    value_loss         | 42.6     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6201000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.93\n",
      "Num timesteps: 6202000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.47\n",
      "Num timesteps: 6203000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.76\n",
      "Num timesteps: 6204000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.96\n",
      "Num timesteps: 6205000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.00\n",
      "Num timesteps: 6206000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.41\n",
      "Num timesteps: 6207000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.52\n",
      "Num timesteps: 6208000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.35\n",
      "Num timesteps: 6209000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.75\n",
      "Num timesteps: 6210000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.47\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 291      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 62100    |\n",
      "|    time_elapsed       | 4616     |\n",
      "|    total_timesteps    | 6210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | 0.577    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62099    |\n",
      "|    policy_loss        | -34.1    |\n",
      "|    value_loss         | 5.02e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6211000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.01\n",
      "Num timesteps: 6212000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.72\n",
      "Num timesteps: 6213000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.67\n",
      "Num timesteps: 6214000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.49\n",
      "Num timesteps: 6215000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.17\n",
      "Num timesteps: 6216000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.28\n",
      "Num timesteps: 6217000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.66\n",
      "Num timesteps: 6218000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.46\n",
      "Num timesteps: 6219000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.98\n",
      "Num timesteps: 6220000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.15\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 62200    |\n",
      "|    time_elapsed       | 4623     |\n",
      "|    total_timesteps    | 6220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | -0.0157  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62199    |\n",
      "|    policy_loss        | -1.55    |\n",
      "|    value_loss         | 6.08     |\n",
      "------------------------------------\n",
      "Num timesteps: 6221000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.20\n",
      "Num timesteps: 6222000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.01\n",
      "Num timesteps: 6223000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.36\n",
      "Num timesteps: 6224000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.76\n",
      "Num timesteps: 6225000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.55\n",
      "Num timesteps: 6226000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.51\n",
      "Num timesteps: 6227000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.40\n",
      "Num timesteps: 6228000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.32\n",
      "Num timesteps: 6229000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.84\n",
      "Num timesteps: 6230000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 62300    |\n",
      "|    time_elapsed       | 4629     |\n",
      "|    total_timesteps    | 6230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62299    |\n",
      "|    policy_loss        | 0.66     |\n",
      "|    value_loss         | 14.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6231000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.20\n",
      "Num timesteps: 6232000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.96\n",
      "Num timesteps: 6233000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.60\n",
      "Num timesteps: 6234000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.61\n",
      "Num timesteps: 6235000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.72\n",
      "Num timesteps: 6236000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.73\n",
      "Num timesteps: 6237000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.99\n",
      "Num timesteps: 6238000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.42\n",
      "Num timesteps: 6239000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.28\n",
      "Num timesteps: 6240000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.75\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 283      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 62400    |\n",
      "|    time_elapsed       | 4635     |\n",
      "|    total_timesteps    | 6240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.456   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62399    |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 92.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 6241000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.38\n",
      "Num timesteps: 6242000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.82\n",
      "Num timesteps: 6243000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.44\n",
      "Num timesteps: 6244000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.51\n",
      "Num timesteps: 6245000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.19\n",
      "Num timesteps: 6246000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.05\n",
      "Num timesteps: 6247000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.79\n",
      "Num timesteps: 6248000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.04\n",
      "Num timesteps: 6249000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.80\n",
      "Num timesteps: 6250000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 290      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 62500    |\n",
      "|    time_elapsed       | 4641     |\n",
      "|    total_timesteps    | 6250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.651   |\n",
      "|    explained_variance | -0.0148  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62499    |\n",
      "|    policy_loss        | -2.66    |\n",
      "|    value_loss         | 23.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6251000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.99\n",
      "Num timesteps: 6252000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.33\n",
      "Num timesteps: 6253000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.74\n",
      "Num timesteps: 6254000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.71\n",
      "Num timesteps: 6255000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.02\n",
      "Num timesteps: 6256000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.39\n",
      "Num timesteps: 6257000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.44\n",
      "Num timesteps: 6258000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.34\n",
      "Num timesteps: 6259000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6260000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 273      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 62600    |\n",
      "|    time_elapsed       | 4647     |\n",
      "|    total_timesteps    | 6260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.73    |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62599    |\n",
      "|    policy_loss        | -3.45    |\n",
      "|    value_loss         | 39.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6261000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.18\n",
      "Num timesteps: 6262000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.52\n",
      "Num timesteps: 6263000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.04\n",
      "Num timesteps: 6264000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.39\n",
      "Num timesteps: 6265000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.26\n",
      "Num timesteps: 6266000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.75\n",
      "Num timesteps: 6267000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.76\n",
      "Num timesteps: 6268000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.50\n",
      "Num timesteps: 6269000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.41\n",
      "Num timesteps: 6270000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 309      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 62700    |\n",
      "|    time_elapsed       | 4654     |\n",
      "|    total_timesteps    | 6270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.599   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62699    |\n",
      "|    policy_loss        | -2.64    |\n",
      "|    value_loss         | 17.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 6271000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.17\n",
      "Num timesteps: 6272000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.18\n",
      "Num timesteps: 6273000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.17\n",
      "Num timesteps: 6274000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.60\n",
      "Num timesteps: 6275000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.53\n",
      "Num timesteps: 6276000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.96\n",
      "Num timesteps: 6277000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.89\n",
      "Num timesteps: 6278000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.35\n",
      "Num timesteps: 6279000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.53\n",
      "Num timesteps: 6280000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 320      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 62800    |\n",
      "|    time_elapsed       | 4661     |\n",
      "|    total_timesteps    | 6280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | 0.994    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62799    |\n",
      "|    policy_loss        | 0.582    |\n",
      "|    value_loss         | 8.67     |\n",
      "------------------------------------\n",
      "Num timesteps: 6281000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.58\n",
      "Num timesteps: 6282000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.11\n",
      "Num timesteps: 6283000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.88\n",
      "Num timesteps: 6284000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.45\n",
      "Num timesteps: 6285000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.81\n",
      "Num timesteps: 6286000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.38\n",
      "Num timesteps: 6287000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.58\n",
      "Num timesteps: 6288000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.61\n",
      "Num timesteps: 6289000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.12\n",
      "Num timesteps: 6290000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 336      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 62900    |\n",
      "|    time_elapsed       | 4670     |\n",
      "|    total_timesteps    | 6290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 0.998    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62899    |\n",
      "|    policy_loss        | 0.364    |\n",
      "|    value_loss         | 4.37     |\n",
      "------------------------------------\n",
      "Num timesteps: 6291000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.39\n",
      "Num timesteps: 6292000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.12\n",
      "Num timesteps: 6293000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.75\n",
      "Num timesteps: 6294000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.24\n",
      "Num timesteps: 6295000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.39\n",
      "Num timesteps: 6296000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.32\n",
      "Num timesteps: 6297000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.17\n",
      "Num timesteps: 6298000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.09\n",
      "Num timesteps: 6299000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.55\n",
      "Num timesteps: 6300000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 360      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 63000    |\n",
      "|    time_elapsed       | 4679     |\n",
      "|    total_timesteps    | 6300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.378   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 62999    |\n",
      "|    policy_loss        | -0.31    |\n",
      "|    value_loss         | 49       |\n",
      "------------------------------------\n",
      "Num timesteps: 6301000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.61\n",
      "Num timesteps: 6302000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.27\n",
      "Num timesteps: 6303000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.74\n",
      "Num timesteps: 6304000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.06\n",
      "Num timesteps: 6305000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.97\n",
      "Num timesteps: 6306000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.21\n",
      "Num timesteps: 6307000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.30\n",
      "Num timesteps: 6308000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.74\n",
      "Num timesteps: 6309000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.16\n",
      "Num timesteps: 6310000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.39\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 380      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 63100    |\n",
      "|    time_elapsed       | 4687     |\n",
      "|    total_timesteps    | 6310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.715   |\n",
      "|    explained_variance | 0.747    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63099    |\n",
      "|    policy_loss        | -40.8    |\n",
      "|    value_loss         | 4.03e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6311000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.62\n",
      "Num timesteps: 6312000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.73\n",
      "Num timesteps: 6313000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.24\n",
      "Num timesteps: 6314000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.09\n",
      "Num timesteps: 6315000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.36\n",
      "Num timesteps: 6316000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.91\n",
      "Num timesteps: 6317000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.77\n",
      "Num timesteps: 6318000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.91\n",
      "Num timesteps: 6319000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.14\n",
      "Num timesteps: 6320000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 414      |\n",
      "|    ep_rew_mean        | 149      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 63200    |\n",
      "|    time_elapsed       | 4696     |\n",
      "|    total_timesteps    | 6320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | -1.04    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63199    |\n",
      "|    policy_loss        | -0.34    |\n",
      "|    value_loss         | 1.39     |\n",
      "------------------------------------\n",
      "Num timesteps: 6321000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.99\n",
      "Num timesteps: 6322000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.77\n",
      "Num timesteps: 6323000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.08\n",
      "Num timesteps: 6324000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.91\n",
      "Num timesteps: 6325000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.60\n",
      "Num timesteps: 6326000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.58\n",
      "Num timesteps: 6327000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.01\n",
      "Num timesteps: 6328000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.38\n",
      "Num timesteps: 6329000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.93\n",
      "Num timesteps: 6330000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 391      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 63300    |\n",
      "|    time_elapsed       | 4702     |\n",
      "|    total_timesteps    | 6330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.504   |\n",
      "|    explained_variance | 0.564    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63299    |\n",
      "|    policy_loss        | 10.3     |\n",
      "|    value_loss         | 348      |\n",
      "------------------------------------\n",
      "Num timesteps: 6331000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.75\n",
      "Num timesteps: 6332000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.40\n",
      "Num timesteps: 6333000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.34\n",
      "Num timesteps: 6334000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.51\n",
      "Num timesteps: 6335000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.03\n",
      "Num timesteps: 6336000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.08\n",
      "Num timesteps: 6337000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.50\n",
      "Num timesteps: 6338000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.59\n",
      "Num timesteps: 6339000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.17\n",
      "Num timesteps: 6340000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 63400    |\n",
      "|    time_elapsed       | 4711     |\n",
      "|    total_timesteps    | 6340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.631   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63399    |\n",
      "|    policy_loss        | 1.94     |\n",
      "|    value_loss         | 12.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 6341000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.58\n",
      "Num timesteps: 6342000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.40\n",
      "Num timesteps: 6343000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.21\n",
      "Num timesteps: 6344000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.06\n",
      "Num timesteps: 6345000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.49\n",
      "Num timesteps: 6346000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.40\n",
      "Num timesteps: 6347000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.42\n",
      "Num timesteps: 6348000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.29\n",
      "Num timesteps: 6349000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.21\n",
      "Num timesteps: 6350000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 352      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 63500    |\n",
      "|    time_elapsed       | 4718     |\n",
      "|    total_timesteps    | 6350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | -0.168   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63499    |\n",
      "|    policy_loss        | -0.224   |\n",
      "|    value_loss         | 28.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 6351000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.15\n",
      "Num timesteps: 6352000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.75\n",
      "Num timesteps: 6353000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.07\n",
      "Num timesteps: 6354000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.21\n",
      "Num timesteps: 6355000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.95\n",
      "Num timesteps: 6356000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.00\n",
      "Num timesteps: 6357000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.80\n",
      "Num timesteps: 6358000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.84\n",
      "Num timesteps: 6359000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.46\n",
      "Num timesteps: 6360000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 348      |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 63600    |\n",
      "|    time_elapsed       | 4726     |\n",
      "|    total_timesteps    | 6360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.318   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63599    |\n",
      "|    policy_loss        | 0.681    |\n",
      "|    value_loss         | 32.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 6361000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.42\n",
      "Num timesteps: 6362000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.35\n",
      "Num timesteps: 6363000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.93\n",
      "Num timesteps: 6364000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.93\n",
      "Num timesteps: 6365000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.38\n",
      "Num timesteps: 6366000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.87\n",
      "Num timesteps: 6367000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.19\n",
      "Num timesteps: 6368000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.30\n",
      "Num timesteps: 6369000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6370000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.10\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 353      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 63700    |\n",
      "|    time_elapsed       | 4734     |\n",
      "|    total_timesteps    | 6370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.18    |\n",
      "|    explained_variance | 0.222    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63699    |\n",
      "|    policy_loss        | 4.88     |\n",
      "|    value_loss         | 193      |\n",
      "------------------------------------\n",
      "Num timesteps: 6371000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.66\n",
      "Num timesteps: 6372000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.29\n",
      "Num timesteps: 6373000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.64\n",
      "Num timesteps: 6374000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.37\n",
      "Num timesteps: 6375000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.73\n",
      "Num timesteps: 6376000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.67\n",
      "Num timesteps: 6377000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.80\n",
      "Num timesteps: 6378000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.50\n",
      "Num timesteps: 6379000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.18\n",
      "Num timesteps: 6380000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 380      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1344     |\n",
      "|    iterations         | 63800    |\n",
      "|    time_elapsed       | 4744     |\n",
      "|    total_timesteps    | 6380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63799    |\n",
      "|    policy_loss        | 1.71     |\n",
      "|    value_loss         | 48       |\n",
      "------------------------------------\n",
      "Num timesteps: 6381000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.00\n",
      "Num timesteps: 6382000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.60\n",
      "Num timesteps: 6383000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.13\n",
      "Num timesteps: 6384000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.56\n",
      "Num timesteps: 6385000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.43\n",
      "Num timesteps: 6386000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.47\n",
      "Num timesteps: 6387000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.77\n",
      "Num timesteps: 6388000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.21\n",
      "Num timesteps: 6389000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.70\n",
      "Num timesteps: 6390000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 422      |\n",
      "|    ep_rew_mean        | 152      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1343     |\n",
      "|    iterations         | 63900    |\n",
      "|    time_elapsed       | 4756     |\n",
      "|    total_timesteps    | 6390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.264   |\n",
      "|    explained_variance | -0.945   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63899    |\n",
      "|    policy_loss        | 4.15     |\n",
      "|    value_loss         | 1.2e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 6391000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.59\n",
      "Num timesteps: 6392000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.26\n",
      "Num timesteps: 6393000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.63\n",
      "Num timesteps: 6394000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.09\n",
      "Num timesteps: 6395000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.17\n",
      "Num timesteps: 6396000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.40\n",
      "Num timesteps: 6397000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.82\n",
      "Num timesteps: 6398000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.05\n",
      "Num timesteps: 6399000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.94\n",
      "Num timesteps: 6400000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.74\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 455      |\n",
      "|    ep_rew_mean        | 129      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 64000    |\n",
      "|    time_elapsed       | 4768     |\n",
      "|    total_timesteps    | 6400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | -0.24    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 63999    |\n",
      "|    policy_loss        | 0.218    |\n",
      "|    value_loss         | 5.36     |\n",
      "------------------------------------\n",
      "Num timesteps: 6401000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.79\n",
      "Num timesteps: 6402000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.76\n",
      "Num timesteps: 6403000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.71\n",
      "Num timesteps: 6404000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.76\n",
      "Num timesteps: 6405000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.29\n",
      "Num timesteps: 6406000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.22\n",
      "Num timesteps: 6407000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.21\n",
      "Num timesteps: 6408000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.63\n",
      "Num timesteps: 6409000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.22\n",
      "Num timesteps: 6410000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 485      |\n",
      "|    ep_rew_mean        | 120      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 64100    |\n",
      "|    time_elapsed       | 4779     |\n",
      "|    total_timesteps    | 6410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.845   |\n",
      "|    explained_variance | -0.0661  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64099    |\n",
      "|    policy_loss        | 4.33     |\n",
      "|    value_loss         | 32.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 6411000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.95\n",
      "Num timesteps: 6412000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.41\n",
      "Num timesteps: 6413000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.51\n",
      "Num timesteps: 6414000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.58\n",
      "Num timesteps: 6415000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.50\n",
      "Num timesteps: 6416000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.87\n",
      "Num timesteps: 6417000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.08\n",
      "Num timesteps: 6418000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.63\n",
      "Num timesteps: 6419000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.16\n",
      "Num timesteps: 6420000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 495      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1339     |\n",
      "|    iterations         | 64200    |\n",
      "|    time_elapsed       | 4793     |\n",
      "|    total_timesteps    | 6420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.477   |\n",
      "|    explained_variance | 0.993    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64199    |\n",
      "|    policy_loss        | 0.434    |\n",
      "|    value_loss         | 8.23     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6421000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.65\n",
      "Num timesteps: 6422000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.46\n",
      "Num timesteps: 6423000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.06\n",
      "Num timesteps: 6424000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.94\n",
      "Num timesteps: 6425000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.88\n",
      "Num timesteps: 6426000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.39\n",
      "Num timesteps: 6427000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.50\n",
      "Num timesteps: 6428000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.10\n",
      "Num timesteps: 6429000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.49\n",
      "Num timesteps: 6430000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 509      |\n",
      "|    ep_rew_mean        | 122      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 64300    |\n",
      "|    time_elapsed       | 4804     |\n",
      "|    total_timesteps    | 6430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.693   |\n",
      "|    explained_variance | -0.0038  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64299    |\n",
      "|    policy_loss        | -1.25    |\n",
      "|    value_loss         | 4.14     |\n",
      "------------------------------------\n",
      "Num timesteps: 6431000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.21\n",
      "Num timesteps: 6432000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.99\n",
      "Num timesteps: 6433000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.98\n",
      "Num timesteps: 6434000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.43\n",
      "Num timesteps: 6435000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.49\n",
      "Num timesteps: 6436000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.25\n",
      "Num timesteps: 6437000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.79\n",
      "Num timesteps: 6438000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.56\n",
      "Num timesteps: 6439000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.17\n",
      "Num timesteps: 6440000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 481      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 64400    |\n",
      "|    time_elapsed       | 4812     |\n",
      "|    total_timesteps    | 6440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.705   |\n",
      "|    explained_variance | -0.105   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64399    |\n",
      "|    policy_loss        | -3.17    |\n",
      "|    value_loss         | 16       |\n",
      "------------------------------------\n",
      "Num timesteps: 6441000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.36\n",
      "Num timesteps: 6442000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.40\n",
      "Num timesteps: 6443000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.28\n",
      "Num timesteps: 6444000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.80\n",
      "Num timesteps: 6445000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.35\n",
      "Num timesteps: 6446000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.44\n",
      "Num timesteps: 6447000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.71\n",
      "Num timesteps: 6448000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.44\n",
      "Num timesteps: 6449000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.07\n",
      "Num timesteps: 6450000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 469      |\n",
      "|    ep_rew_mean        | 147      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 64500    |\n",
      "|    time_elapsed       | 4826     |\n",
      "|    total_timesteps    | 6450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.886   |\n",
      "|    explained_variance | -0.0799  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64499    |\n",
      "|    policy_loss        | 0.25     |\n",
      "|    value_loss         | 45.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6451000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.26\n",
      "Num timesteps: 6452000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.93\n",
      "Num timesteps: 6453000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.09\n",
      "Num timesteps: 6454000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.80\n",
      "Num timesteps: 6455000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.54\n",
      "Num timesteps: 6456000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.19\n",
      "Num timesteps: 6457000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.35\n",
      "Num timesteps: 6458000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.73\n",
      "Num timesteps: 6459000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.87\n",
      "Num timesteps: 6460000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 487      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 64600    |\n",
      "|    time_elapsed       | 4838     |\n",
      "|    total_timesteps    | 6460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.59    |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64599    |\n",
      "|    policy_loss        | -1.04    |\n",
      "|    value_loss         | 3.34     |\n",
      "------------------------------------\n",
      "Num timesteps: 6461000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.98\n",
      "Num timesteps: 6462000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.09\n",
      "Num timesteps: 6463000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.24\n",
      "Num timesteps: 6464000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.92\n",
      "Num timesteps: 6465000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.40\n",
      "Num timesteps: 6466000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.41\n",
      "Num timesteps: 6467000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.81\n",
      "Num timesteps: 6468000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.05\n",
      "Num timesteps: 6469000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.89\n",
      "Num timesteps: 6470000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 512      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 64700    |\n",
      "|    time_elapsed       | 4851     |\n",
      "|    total_timesteps    | 6470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.896   |\n",
      "|    explained_variance | -0.0703  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64699    |\n",
      "|    policy_loss        | -1.08    |\n",
      "|    value_loss         | 6.17     |\n",
      "------------------------------------\n",
      "Num timesteps: 6471000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.82\n",
      "Num timesteps: 6472000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.98\n",
      "Num timesteps: 6473000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.38\n",
      "Num timesteps: 6474000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.94\n",
      "Num timesteps: 6475000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.04\n",
      "Num timesteps: 6476000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.64\n",
      "Num timesteps: 6477000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.91\n",
      "Num timesteps: 6478000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.67\n",
      "Num timesteps: 6479000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6480000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 494      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 64800    |\n",
      "|    time_elapsed       | 4863     |\n",
      "|    total_timesteps    | 6480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.708   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64799    |\n",
      "|    policy_loss        | 3.87     |\n",
      "|    value_loss         | 37.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6481000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.00\n",
      "Num timesteps: 6482000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.30\n",
      "Num timesteps: 6483000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.64\n",
      "Num timesteps: 6484000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.90\n",
      "Num timesteps: 6485000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.21\n",
      "Num timesteps: 6486000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.07\n",
      "Num timesteps: 6487000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.92\n",
      "Num timesteps: 6488000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.91\n",
      "Num timesteps: 6489000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.16\n",
      "Num timesteps: 6490000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 498      |\n",
      "|    ep_rew_mean        | 140      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 64900    |\n",
      "|    time_elapsed       | 4872     |\n",
      "|    total_timesteps    | 6490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.465   |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64899    |\n",
      "|    policy_loss        | 2.07     |\n",
      "|    value_loss         | 1.17e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6491000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.93\n",
      "Num timesteps: 6492000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.45\n",
      "Num timesteps: 6493000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.84\n",
      "Num timesteps: 6494000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.56\n",
      "Num timesteps: 6495000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.78\n",
      "Num timesteps: 6496000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.01\n",
      "Num timesteps: 6497000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.93\n",
      "Num timesteps: 6498000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.30\n",
      "Num timesteps: 6499000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.55\n",
      "Num timesteps: 6500000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 425      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65000    |\n",
      "|    time_elapsed       | 4879     |\n",
      "|    total_timesteps    | 6500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.393    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 64999    |\n",
      "|    policy_loss        | -11.9    |\n",
      "|    value_loss         | 186      |\n",
      "------------------------------------\n",
      "Num timesteps: 6501000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.66\n",
      "Num timesteps: 6502000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.77\n",
      "Num timesteps: 6503000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.61\n",
      "Num timesteps: 6504000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.56\n",
      "Num timesteps: 6505000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.59\n",
      "Num timesteps: 6506000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.13\n",
      "Num timesteps: 6507000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.47\n",
      "Num timesteps: 6508000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.90\n",
      "Num timesteps: 6509000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.57\n",
      "Num timesteps: 6510000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 362      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65100    |\n",
      "|    time_elapsed       | 4886     |\n",
      "|    total_timesteps    | 6510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0796  |\n",
      "|    explained_variance | -1.65    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65099    |\n",
      "|    policy_loss        | -0.318   |\n",
      "|    value_loss         | 669      |\n",
      "------------------------------------\n",
      "Num timesteps: 6511000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.19\n",
      "Num timesteps: 6512000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.31\n",
      "Num timesteps: 6513000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.38\n",
      "Num timesteps: 6514000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.79\n",
      "Num timesteps: 6515000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.70\n",
      "Num timesteps: 6516000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.21\n",
      "Num timesteps: 6517000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.07\n",
      "Num timesteps: 6518000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.19\n",
      "Num timesteps: 6519000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.79\n",
      "Num timesteps: 6520000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.23\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 350      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65200    |\n",
      "|    time_elapsed       | 4894     |\n",
      "|    total_timesteps    | 6520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.53    |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65199    |\n",
      "|    policy_loss        | 1.82     |\n",
      "|    value_loss         | 53.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 6521000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.65\n",
      "Num timesteps: 6522000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.67\n",
      "Num timesteps: 6523000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.43\n",
      "Num timesteps: 6524000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.10\n",
      "Num timesteps: 6525000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.08\n",
      "Num timesteps: 6526000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.16\n",
      "Num timesteps: 6527000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.27\n",
      "Num timesteps: 6528000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.50\n",
      "Num timesteps: 6529000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.72\n",
      "Num timesteps: 6530000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 349      |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65300    |\n",
      "|    time_elapsed       | 4900     |\n",
      "|    total_timesteps    | 6530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.417   |\n",
      "|    explained_variance | 0.603    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65299    |\n",
      "|    policy_loss        | -20.9    |\n",
      "|    value_loss         | 3.32e+03 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6531000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.31\n",
      "Num timesteps: 6532000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.77\n",
      "Num timesteps: 6533000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.04\n",
      "Num timesteps: 6534000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.44\n",
      "Num timesteps: 6535000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.61\n",
      "Num timesteps: 6536000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.07\n",
      "Num timesteps: 6537000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.59\n",
      "Num timesteps: 6538000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.70\n",
      "Num timesteps: 6539000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.48\n",
      "Num timesteps: 6540000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.73\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65400    |\n",
      "|    time_elapsed       | 4908     |\n",
      "|    total_timesteps    | 6540000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65399    |\n",
      "|    policy_loss        | -2.32    |\n",
      "|    value_loss         | 25.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6541000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.59\n",
      "Num timesteps: 6542000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.69\n",
      "Num timesteps: 6543000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.90\n",
      "Num timesteps: 6544000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.23\n",
      "Num timesteps: 6545000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.83\n",
      "Num timesteps: 6546000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.24\n",
      "Num timesteps: 6547000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.13\n",
      "Num timesteps: 6548000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.47\n",
      "Num timesteps: 6549000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.73\n",
      "Num timesteps: 6550000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 351      |\n",
      "|    ep_rew_mean        | 158      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65500    |\n",
      "|    time_elapsed       | 4917     |\n",
      "|    total_timesteps    | 6550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | 0.132    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65499    |\n",
      "|    policy_loss        | -0.159   |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "Num timesteps: 6551000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.93\n",
      "Num timesteps: 6552000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.68\n",
      "Num timesteps: 6553000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.07\n",
      "Num timesteps: 6554000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.03\n",
      "Num timesteps: 6555000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.53\n",
      "Num timesteps: 6556000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.68\n",
      "Num timesteps: 6557000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.98\n",
      "Num timesteps: 6558000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.94\n",
      "Num timesteps: 6559000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.55\n",
      "Num timesteps: 6560000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 337      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65600    |\n",
      "|    time_elapsed       | 4924     |\n",
      "|    total_timesteps    | 6560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.319   |\n",
      "|    explained_variance | -0.843   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65599    |\n",
      "|    policy_loss        | 9.22     |\n",
      "|    value_loss         | 2.94e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6561000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.05\n",
      "Num timesteps: 6562000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.69\n",
      "Num timesteps: 6563000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.70\n",
      "Num timesteps: 6564000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.63\n",
      "Num timesteps: 6565000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.25\n",
      "Num timesteps: 6566000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.48\n",
      "Num timesteps: 6567000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.06\n",
      "Num timesteps: 6568000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.26\n",
      "Num timesteps: 6569000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.76\n",
      "Num timesteps: 6570000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65700    |\n",
      "|    time_elapsed       | 4930     |\n",
      "|    total_timesteps    | 6570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.277   |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65699    |\n",
      "|    policy_loss        | 0.755    |\n",
      "|    value_loss         | 38.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 6571000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.66\n",
      "Num timesteps: 6572000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.88\n",
      "Num timesteps: 6573000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.42\n",
      "Num timesteps: 6574000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.79\n",
      "Num timesteps: 6575000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 6576000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.35\n",
      "Num timesteps: 6577000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.84\n",
      "Num timesteps: 6578000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.13\n",
      "Num timesteps: 6579000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.30\n",
      "Num timesteps: 6580000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.78\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 313      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65800    |\n",
      "|    time_elapsed       | 4938     |\n",
      "|    total_timesteps    | 6580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.666   |\n",
      "|    explained_variance | -1.27    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65799    |\n",
      "|    policy_loss        | -0.727   |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Num timesteps: 6581000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.57\n",
      "Num timesteps: 6582000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.94\n",
      "Num timesteps: 6583000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 6584000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.84\n",
      "Num timesteps: 6585000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.06\n",
      "Num timesteps: 6586000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.99\n",
      "Num timesteps: 6587000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.89\n",
      "Num timesteps: 6588000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.46\n",
      "Num timesteps: 6589000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6590000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.86\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 369      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 65900    |\n",
      "|    time_elapsed       | 4947     |\n",
      "|    total_timesteps    | 6590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.326   |\n",
      "|    explained_variance | -2.19    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65899    |\n",
      "|    policy_loss        | 15.1     |\n",
      "|    value_loss         | 3.7e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 6591000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.72\n",
      "Num timesteps: 6592000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.21\n",
      "Num timesteps: 6593000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.53\n",
      "Num timesteps: 6594000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.27\n",
      "Num timesteps: 6595000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.45\n",
      "Num timesteps: 6596000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.19\n",
      "Num timesteps: 6597000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.39\n",
      "Num timesteps: 6598000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.15\n",
      "Num timesteps: 6599000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.74\n",
      "Num timesteps: 6600000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 386      |\n",
      "|    ep_rew_mean        | 150      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 66000    |\n",
      "|    time_elapsed       | 4956     |\n",
      "|    total_timesteps    | 6600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.57    |\n",
      "|    explained_variance | -12.4    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 65999    |\n",
      "|    policy_loss        | 20.5     |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6601000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.65\n",
      "Num timesteps: 6602000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.67\n",
      "Num timesteps: 6603000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.83\n",
      "Num timesteps: 6604000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.35\n",
      "Num timesteps: 6605000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.95\n",
      "Num timesteps: 6606000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.24\n",
      "Num timesteps: 6607000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.58\n",
      "Num timesteps: 6608000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.49\n",
      "Num timesteps: 6609000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.39\n",
      "Num timesteps: 6610000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 66100    |\n",
      "|    time_elapsed       | 4963     |\n",
      "|    total_timesteps    | 6610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.586   |\n",
      "|    explained_variance | -3.1     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66099    |\n",
      "|    policy_loss        | 12.2     |\n",
      "|    value_loss         | 368      |\n",
      "------------------------------------\n",
      "Num timesteps: 6611000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.61\n",
      "Num timesteps: 6612000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.71\n",
      "Num timesteps: 6613000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.32\n",
      "Num timesteps: 6614000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.44\n",
      "Num timesteps: 6615000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.88\n",
      "Num timesteps: 6616000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.34\n",
      "Num timesteps: 6617000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.47\n",
      "Num timesteps: 6618000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.52\n",
      "Num timesteps: 6619000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.98\n",
      "Num timesteps: 6620000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 66200    |\n",
      "|    time_elapsed       | 4970     |\n",
      "|    total_timesteps    | 6620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.423   |\n",
      "|    explained_variance | -0.0448  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66199    |\n",
      "|    policy_loss        | 7.82     |\n",
      "|    value_loss         | 641      |\n",
      "------------------------------------\n",
      "Num timesteps: 6621000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.40\n",
      "Num timesteps: 6622000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.20\n",
      "Num timesteps: 6623000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.31\n",
      "Num timesteps: 6624000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.06\n",
      "Num timesteps: 6625000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.49\n",
      "Num timesteps: 6626000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.15\n",
      "Num timesteps: 6627000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.96\n",
      "Num timesteps: 6628000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.18\n",
      "Num timesteps: 6629000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.70\n",
      "Num timesteps: 6630000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.42\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 66300    |\n",
      "|    time_elapsed       | 4978     |\n",
      "|    total_timesteps    | 6630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.676   |\n",
      "|    explained_variance | 0.169    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66299    |\n",
      "|    policy_loss        | 0.00957  |\n",
      "|    value_loss         | 0.0501   |\n",
      "------------------------------------\n",
      "Num timesteps: 6631000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.01\n",
      "Num timesteps: 6632000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.05\n",
      "Num timesteps: 6633000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.95\n",
      "Num timesteps: 6634000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.22\n",
      "Num timesteps: 6635000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.79\n",
      "Num timesteps: 6636000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.89\n",
      "Num timesteps: 6637000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.41\n",
      "Num timesteps: 6638000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.84\n",
      "Num timesteps: 6639000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.62\n",
      "Num timesteps: 6640000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.26\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 296      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 66400    |\n",
      "|    time_elapsed       | 4983     |\n",
      "|    total_timesteps    | 6640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66399    |\n",
      "|    policy_loss        | -0.337   |\n",
      "|    value_loss         | 18.3     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6641000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.69\n",
      "Num timesteps: 6642000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.74\n",
      "Num timesteps: 6643000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.64\n",
      "Num timesteps: 6644000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.21\n",
      "Num timesteps: 6645000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.85\n",
      "Num timesteps: 6646000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.94\n",
      "Num timesteps: 6647000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.55\n",
      "Num timesteps: 6648000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.09\n",
      "Num timesteps: 6649000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.21\n",
      "Num timesteps: 6650000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.34\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 312      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 66500    |\n",
      "|    time_elapsed       | 4992     |\n",
      "|    total_timesteps    | 6650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.612   |\n",
      "|    explained_variance | -0.102   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66499    |\n",
      "|    policy_loss        | -0.083   |\n",
      "|    value_loss         | 0.0504   |\n",
      "------------------------------------\n",
      "Num timesteps: 6651000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.46\n",
      "Num timesteps: 6652000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.66\n",
      "Num timesteps: 6653000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.24\n",
      "Num timesteps: 6654000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.30\n",
      "Num timesteps: 6655000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.94\n",
      "Num timesteps: 6656000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.65\n",
      "Num timesteps: 6657000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.17\n",
      "Num timesteps: 6658000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.55\n",
      "Num timesteps: 6659000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.57\n",
      "Num timesteps: 6660000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 274      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 66600    |\n",
      "|    time_elapsed       | 4997     |\n",
      "|    total_timesteps    | 6660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.773   |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66599    |\n",
      "|    policy_loss        | -13.3    |\n",
      "|    value_loss         | 386      |\n",
      "------------------------------------\n",
      "Num timesteps: 6661000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.95\n",
      "Num timesteps: 6662000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.66\n",
      "Num timesteps: 6663000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.34\n",
      "Num timesteps: 6664000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.59\n",
      "Num timesteps: 6665000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.60\n",
      "Num timesteps: 6666000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.15\n",
      "Num timesteps: 6667000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.82\n",
      "Num timesteps: 6668000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.55\n",
      "Num timesteps: 6669000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.45\n",
      "Num timesteps: 6670000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.22\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 246      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 66700    |\n",
      "|    time_elapsed       | 5003     |\n",
      "|    total_timesteps    | 6670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | -0.0819  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66699    |\n",
      "|    policy_loss        | -0.516   |\n",
      "|    value_loss         | 0.653    |\n",
      "------------------------------------\n",
      "Num timesteps: 6671000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.37\n",
      "Num timesteps: 6672000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.76\n",
      "Num timesteps: 6673000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.00\n",
      "Num timesteps: 6674000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.59\n",
      "Num timesteps: 6675000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.91\n",
      "Num timesteps: 6676000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.36\n",
      "Num timesteps: 6677000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.34\n",
      "Num timesteps: 6678000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.90\n",
      "Num timesteps: 6679000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.79\n",
      "Num timesteps: 6680000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 258      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 66800    |\n",
      "|    time_elapsed       | 5010     |\n",
      "|    total_timesteps    | 6680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.684   |\n",
      "|    explained_variance | -0.351   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66799    |\n",
      "|    policy_loss        | -0.14    |\n",
      "|    value_loss         | 0.15     |\n",
      "------------------------------------\n",
      "Num timesteps: 6681000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.15\n",
      "Num timesteps: 6682000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.32\n",
      "Num timesteps: 6683000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.66\n",
      "Num timesteps: 6684000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.67\n",
      "Num timesteps: 6685000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.29\n",
      "Num timesteps: 6686000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.37\n",
      "Num timesteps: 6687000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.99\n",
      "Num timesteps: 6688000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.60\n",
      "Num timesteps: 6689000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.98\n",
      "Num timesteps: 6690000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 293      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 66900    |\n",
      "|    time_elapsed       | 5017     |\n",
      "|    total_timesteps    | 6690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.618   |\n",
      "|    explained_variance | 0.0269   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 66899    |\n",
      "|    policy_loss        | -0.191   |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "Num timesteps: 6691000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.74\n",
      "Num timesteps: 6692000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.37\n",
      "Num timesteps: 6693000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.07\n",
      "Num timesteps: 6694000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.62\n",
      "Num timesteps: 6695000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.90\n",
      "Num timesteps: 6696000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.98\n",
      "Num timesteps: 6697000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.55\n",
      "Num timesteps: 6698000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.43\n",
      "Num timesteps: 6699000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6700000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.98\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 308       |\n",
      "|    ep_rew_mean        | 187       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1333      |\n",
      "|    iterations         | 67000     |\n",
      "|    time_elapsed       | 5023      |\n",
      "|    total_timesteps    | 6700000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.618    |\n",
      "|    explained_variance | -0.000982 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 66999     |\n",
      "|    policy_loss        | -8.96     |\n",
      "|    value_loss         | 299       |\n",
      "-------------------------------------\n",
      "Num timesteps: 6701000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.52\n",
      "Num timesteps: 6702000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.35\n",
      "Num timesteps: 6703000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.77\n",
      "Num timesteps: 6704000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.16\n",
      "Num timesteps: 6705000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.46\n",
      "Num timesteps: 6706000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.16\n",
      "Num timesteps: 6707000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.61\n",
      "Num timesteps: 6708000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.08\n",
      "Num timesteps: 6709000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.11\n",
      "Num timesteps: 6710000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 310      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 67100    |\n",
      "|    time_elapsed       | 5030     |\n",
      "|    total_timesteps    | 6710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.456   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67099    |\n",
      "|    policy_loss        | 1.88     |\n",
      "|    value_loss         | 197      |\n",
      "------------------------------------\n",
      "Num timesteps: 6711000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.73\n",
      "Num timesteps: 6712000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.41\n",
      "Num timesteps: 6713000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.75\n",
      "Num timesteps: 6714000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.58\n",
      "Num timesteps: 6715000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.63\n",
      "Num timesteps: 6716000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.90\n",
      "Num timesteps: 6717000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.42\n",
      "Num timesteps: 6718000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.01\n",
      "Num timesteps: 6719000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.93\n",
      "Num timesteps: 6720000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 277      |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 67200    |\n",
      "|    time_elapsed       | 5036     |\n",
      "|    total_timesteps    | 6720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.603   |\n",
      "|    explained_variance | 0.846    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67199    |\n",
      "|    policy_loss        | 6.49     |\n",
      "|    value_loss         | 112      |\n",
      "------------------------------------\n",
      "Num timesteps: 6721000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.98\n",
      "Num timesteps: 6722000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.15\n",
      "Num timesteps: 6723000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.44\n",
      "Num timesteps: 6724000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.67\n",
      "Num timesteps: 6725000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.05\n",
      "Num timesteps: 6726000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.76\n",
      "Num timesteps: 6727000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.68\n",
      "Num timesteps: 6728000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.32\n",
      "Num timesteps: 6729000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.66\n",
      "Num timesteps: 6730000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.14\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 232      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 67300    |\n",
      "|    time_elapsed       | 5041     |\n",
      "|    total_timesteps    | 6730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.162   |\n",
      "|    explained_variance | 0.511    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67299    |\n",
      "|    policy_loss        | -1.76    |\n",
      "|    value_loss         | 40.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 6731000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.23\n",
      "Num timesteps: 6732000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.12\n",
      "Num timesteps: 6733000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.81\n",
      "Num timesteps: 6734000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.10\n",
      "Num timesteps: 6735000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.46\n",
      "Num timesteps: 6736000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.18\n",
      "Num timesteps: 6737000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.15\n",
      "Num timesteps: 6738000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.61\n",
      "Num timesteps: 6739000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.78\n",
      "Num timesteps: 6740000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 246      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 67400    |\n",
      "|    time_elapsed       | 5047     |\n",
      "|    total_timesteps    | 6740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.691   |\n",
      "|    explained_variance | -0.391   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67399    |\n",
      "|    policy_loss        | -0.00884 |\n",
      "|    value_loss         | 0.0145   |\n",
      "------------------------------------\n",
      "Num timesteps: 6741000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.65\n",
      "Num timesteps: 6742000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.63\n",
      "Num timesteps: 6743000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.50\n",
      "Num timesteps: 6744000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.23\n",
      "Num timesteps: 6745000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.07\n",
      "Num timesteps: 6746000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.10\n",
      "Num timesteps: 6747000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.17\n",
      "Num timesteps: 6748000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.10\n",
      "Num timesteps: 6749000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.65\n",
      "Num timesteps: 6750000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 241      |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 67500    |\n",
      "|    time_elapsed       | 5052     |\n",
      "|    total_timesteps    | 6750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.557   |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67499    |\n",
      "|    policy_loss        | -11.5    |\n",
      "|    value_loss         | 248      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6751000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.58\n",
      "Num timesteps: 6752000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.30\n",
      "Num timesteps: 6753000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.05\n",
      "Num timesteps: 6754000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.73\n",
      "Num timesteps: 6755000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.87\n",
      "Num timesteps: 6756000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.85\n",
      "Num timesteps: 6757000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.02\n",
      "Num timesteps: 6758000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.93\n",
      "Num timesteps: 6759000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.64\n",
      "Num timesteps: 6760000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.30\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 240      |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 67600    |\n",
      "|    time_elapsed       | 5058     |\n",
      "|    total_timesteps    | 6760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.653   |\n",
      "|    explained_variance | -0.433   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67599    |\n",
      "|    policy_loss        | -0.652   |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "Num timesteps: 6761000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.95\n",
      "Num timesteps: 6762000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.28\n",
      "Num timesteps: 6763000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.13\n",
      "Num timesteps: 6764000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.55\n",
      "Num timesteps: 6765000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.10\n",
      "Num timesteps: 6766000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.57\n",
      "Num timesteps: 6767000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.66\n",
      "Num timesteps: 6768000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.03\n",
      "Num timesteps: 6769000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.00\n",
      "Num timesteps: 6770000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.18\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 266      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 67700    |\n",
      "|    time_elapsed       | 5066     |\n",
      "|    total_timesteps    | 6770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.718   |\n",
      "|    explained_variance | 0.987    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67699    |\n",
      "|    policy_loss        | -0.677   |\n",
      "|    value_loss         | 5.64     |\n",
      "------------------------------------\n",
      "Num timesteps: 6771000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.81\n",
      "Num timesteps: 6772000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.16\n",
      "Num timesteps: 6773000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.59\n",
      "Num timesteps: 6774000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.35\n",
      "Num timesteps: 6775000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.19\n",
      "Num timesteps: 6776000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.88\n",
      "Num timesteps: 6777000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.33\n",
      "Num timesteps: 6778000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.24\n",
      "Num timesteps: 6779000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.86\n",
      "Num timesteps: 6780000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.60\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 297      |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 67800    |\n",
      "|    time_elapsed       | 5072     |\n",
      "|    total_timesteps    | 6780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.431   |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67799    |\n",
      "|    policy_loss        | 2.27     |\n",
      "|    value_loss         | 118      |\n",
      "------------------------------------\n",
      "Num timesteps: 6781000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.62\n",
      "Num timesteps: 6782000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.83\n",
      "Num timesteps: 6783000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.78\n",
      "Num timesteps: 6784000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.45\n",
      "Num timesteps: 6785000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.48\n",
      "Num timesteps: 6786000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.17\n",
      "Num timesteps: 6787000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.41\n",
      "Num timesteps: 6788000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.74\n",
      "Num timesteps: 6789000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.35\n",
      "Num timesteps: 6790000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 334      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 67900    |\n",
      "|    time_elapsed       | 5080     |\n",
      "|    total_timesteps    | 6790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67899    |\n",
      "|    policy_loss        | -1.81    |\n",
      "|    value_loss         | 44.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 6791000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.87\n",
      "Num timesteps: 6792000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.55\n",
      "Num timesteps: 6793000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.57\n",
      "Num timesteps: 6794000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.80\n",
      "Num timesteps: 6795000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.84\n",
      "Num timesteps: 6796000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.51\n",
      "Num timesteps: 6797000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.63\n",
      "Num timesteps: 6798000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.06\n",
      "Num timesteps: 6799000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.85\n",
      "Num timesteps: 6800000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.64\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 339      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 68000    |\n",
      "|    time_elapsed       | 5088     |\n",
      "|    total_timesteps    | 6800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.846   |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 67999    |\n",
      "|    policy_loss        | 1.28     |\n",
      "|    value_loss         | 6.85     |\n",
      "------------------------------------\n",
      "Num timesteps: 6801000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.01\n",
      "Num timesteps: 6802000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.41\n",
      "Num timesteps: 6803000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.17\n",
      "Num timesteps: 6804000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.88\n",
      "Num timesteps: 6805000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.80\n",
      "Num timesteps: 6806000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.26\n",
      "Num timesteps: 6807000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.82\n",
      "Num timesteps: 6808000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.03\n",
      "Num timesteps: 6809000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6810000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.29\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 361      |\n",
      "|    ep_rew_mean        | 167      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 68100    |\n",
      "|    time_elapsed       | 5098     |\n",
      "|    total_timesteps    | 6810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.994   |\n",
      "|    explained_variance | -0.226   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68099    |\n",
      "|    policy_loss        | 2.36     |\n",
      "|    value_loss         | 18.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6811000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.58\n",
      "Num timesteps: 6812000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.27\n",
      "Num timesteps: 6813000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.03\n",
      "Num timesteps: 6814000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.70\n",
      "Num timesteps: 6815000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.28\n",
      "Num timesteps: 6816000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.40\n",
      "Num timesteps: 6817000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.58\n",
      "Num timesteps: 6818000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.41\n",
      "Num timesteps: 6819000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.75\n",
      "Num timesteps: 6820000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 358      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 68200    |\n",
      "|    time_elapsed       | 5106     |\n",
      "|    total_timesteps    | 6820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | 0.991    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68199    |\n",
      "|    policy_loss        | 3.1      |\n",
      "|    value_loss         | 36.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6821000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.56\n",
      "Num timesteps: 6822000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.01\n",
      "Num timesteps: 6823000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.98\n",
      "Num timesteps: 6824000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.82\n",
      "Num timesteps: 6825000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.42\n",
      "Num timesteps: 6826000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.28\n",
      "Num timesteps: 6827000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.48\n",
      "Num timesteps: 6828000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.99\n",
      "Num timesteps: 6829000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.48\n",
      "Num timesteps: 6830000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 367      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 68300    |\n",
      "|    time_elapsed       | 5115     |\n",
      "|    total_timesteps    | 6830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.442   |\n",
      "|    explained_variance | 0.0212   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68299    |\n",
      "|    policy_loss        | -1.6     |\n",
      "|    value_loss         | 19.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6831000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.09\n",
      "Num timesteps: 6832000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.33\n",
      "Num timesteps: 6833000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.24\n",
      "Num timesteps: 6834000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.59\n",
      "Num timesteps: 6835000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.25\n",
      "Num timesteps: 6836000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.94\n",
      "Num timesteps: 6837000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.41\n",
      "Num timesteps: 6838000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.44\n",
      "Num timesteps: 6839000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.13\n",
      "Num timesteps: 6840000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.35\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 359      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1335     |\n",
      "|    iterations         | 68400    |\n",
      "|    time_elapsed       | 5123     |\n",
      "|    total_timesteps    | 6840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.777   |\n",
      "|    explained_variance | -0.0545  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68399    |\n",
      "|    policy_loss        | 0.189    |\n",
      "|    value_loss         | 8.58     |\n",
      "------------------------------------\n",
      "Num timesteps: 6841000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.07\n",
      "Num timesteps: 6842000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.39\n",
      "Num timesteps: 6843000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.96\n",
      "Num timesteps: 6844000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.02\n",
      "Num timesteps: 6845000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.67\n",
      "Num timesteps: 6846000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.93\n",
      "Num timesteps: 6847000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.59\n",
      "Num timesteps: 6848000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.28\n",
      "Num timesteps: 6849000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.26\n",
      "Num timesteps: 6850000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 366      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 68500    |\n",
      "|    time_elapsed       | 5132     |\n",
      "|    total_timesteps    | 6850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.778   |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68499    |\n",
      "|    policy_loss        | 1.73     |\n",
      "|    value_loss         | 17.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 6851000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.35\n",
      "Num timesteps: 6852000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.41\n",
      "Num timesteps: 6853000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.53\n",
      "Num timesteps: 6854000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.40\n",
      "Num timesteps: 6855000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.06\n",
      "Num timesteps: 6856000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.97\n",
      "Num timesteps: 6857000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.06\n",
      "Num timesteps: 6858000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.01\n",
      "Num timesteps: 6859000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.17\n",
      "Num timesteps: 6860000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 397      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 68600    |\n",
      "|    time_elapsed       | 5142     |\n",
      "|    total_timesteps    | 6860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.873   |\n",
      "|    explained_variance | 0.373    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68599    |\n",
      "|    policy_loss        | 0.587    |\n",
      "|    value_loss         | 3.62     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6861000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.82\n",
      "Num timesteps: 6862000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.41\n",
      "Num timesteps: 6863000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.34\n",
      "Num timesteps: 6864000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.71\n",
      "Num timesteps: 6865000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.55\n",
      "Num timesteps: 6866000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.23\n",
      "Num timesteps: 6867000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.95\n",
      "Num timesteps: 6868000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.19\n",
      "Num timesteps: 6869000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.65\n",
      "Num timesteps: 6870000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.16\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 419      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 68700    |\n",
      "|    time_elapsed       | 5152     |\n",
      "|    total_timesteps    | 6870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.71    |\n",
      "|    explained_variance | 0.635    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68699    |\n",
      "|    policy_loss        | -15.2    |\n",
      "|    value_loss         | 355      |\n",
      "------------------------------------\n",
      "Num timesteps: 6871000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 6872000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.89\n",
      "Num timesteps: 6873000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.48\n",
      "Num timesteps: 6874000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.08\n",
      "Num timesteps: 6875000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.11\n",
      "Num timesteps: 6876000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.97\n",
      "Num timesteps: 6877000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.59\n",
      "Num timesteps: 6878000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.83\n",
      "Num timesteps: 6879000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.40\n",
      "Num timesteps: 6880000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.66\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 388      |\n",
      "|    ep_rew_mean        | 186      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 68800    |\n",
      "|    time_elapsed       | 5159     |\n",
      "|    total_timesteps    | 6880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.427   |\n",
      "|    explained_variance | -0.00176 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68799    |\n",
      "|    policy_loss        | -7.5     |\n",
      "|    value_loss         | 308      |\n",
      "------------------------------------\n",
      "Num timesteps: 6881000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.95\n",
      "Num timesteps: 6882000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.13\n",
      "Num timesteps: 6883000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.23\n",
      "Num timesteps: 6884000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.09\n",
      "Num timesteps: 6885000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.08\n",
      "Num timesteps: 6886000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.04\n",
      "Num timesteps: 6887000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.97\n",
      "Num timesteps: 6888000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.31\n",
      "Num timesteps: 6889000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.14\n",
      "Num timesteps: 6890000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 384      |\n",
      "|    ep_rew_mean        | 185      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 68900    |\n",
      "|    time_elapsed       | 5167     |\n",
      "|    total_timesteps    | 6890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.55    |\n",
      "|    explained_variance | 0.999    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68899    |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 4.17     |\n",
      "------------------------------------\n",
      "Num timesteps: 6891000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.48\n",
      "Num timesteps: 6892000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.63\n",
      "Num timesteps: 6893000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.33\n",
      "Num timesteps: 6894000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.50\n",
      "Num timesteps: 6895000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.71\n",
      "Num timesteps: 6896000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.47\n",
      "Num timesteps: 6897000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.18\n",
      "Num timesteps: 6898000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.48\n",
      "Num timesteps: 6899000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.71\n",
      "Num timesteps: 6900000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.03\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 349      |\n",
      "|    ep_rew_mean        | 184      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 69000    |\n",
      "|    time_elapsed       | 5174     |\n",
      "|    total_timesteps    | 6900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.803   |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 68999    |\n",
      "|    policy_loss        | -2.51    |\n",
      "|    value_loss         | 42.7     |\n",
      "------------------------------------\n",
      "Num timesteps: 6901000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.59\n",
      "Num timesteps: 6902000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.76\n",
      "Num timesteps: 6903000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.00\n",
      "Num timesteps: 6904000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.30\n",
      "Num timesteps: 6905000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.01\n",
      "Num timesteps: 6906000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.48\n",
      "Num timesteps: 6907000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.29\n",
      "Num timesteps: 6908000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.54\n",
      "Num timesteps: 6909000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.63\n",
      "Num timesteps: 6910000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.65\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 354      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 69100    |\n",
      "|    time_elapsed       | 5183     |\n",
      "|    total_timesteps    | 6910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.319   |\n",
      "|    explained_variance | -0.757   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69099    |\n",
      "|    policy_loss        | 4.55     |\n",
      "|    value_loss         | 1.28e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6911000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.54\n",
      "Num timesteps: 6912000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.17\n",
      "Num timesteps: 6913000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.63\n",
      "Num timesteps: 6914000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.84\n",
      "Num timesteps: 6915000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.75\n",
      "Num timesteps: 6916000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.35\n",
      "Num timesteps: 6917000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.92\n",
      "Num timesteps: 6918000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.26\n",
      "Num timesteps: 6919000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6920000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 339      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 69200    |\n",
      "|    time_elapsed       | 5191     |\n",
      "|    total_timesteps    | 6920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0247  |\n",
      "|    explained_variance | -5.19    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69199    |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 6.86e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6921000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.56\n",
      "Num timesteps: 6922000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.97\n",
      "Num timesteps: 6923000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.77\n",
      "Num timesteps: 6924000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.85\n",
      "Num timesteps: 6925000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.62\n",
      "Num timesteps: 6926000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.25\n",
      "Num timesteps: 6927000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.33\n",
      "Num timesteps: 6928000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.89\n",
      "Num timesteps: 6929000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.83\n",
      "Num timesteps: 6930000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.13\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 177      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 69300    |\n",
      "|    time_elapsed       | 5198     |\n",
      "|    total_timesteps    | 6930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.796   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69299    |\n",
      "|    policy_loss        | -1.5     |\n",
      "|    value_loss         | 5.11     |\n",
      "------------------------------------\n",
      "Num timesteps: 6931000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.20\n",
      "Num timesteps: 6932000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.89\n",
      "Num timesteps: 6933000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.94\n",
      "Num timesteps: 6934000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.73\n",
      "Num timesteps: 6935000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.06\n",
      "Num timesteps: 6936000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.66\n",
      "Num timesteps: 6937000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.83\n",
      "Num timesteps: 6938000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.61\n",
      "Num timesteps: 6939000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.82\n",
      "Num timesteps: 6940000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.31\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 69400    |\n",
      "|    time_elapsed       | 5206     |\n",
      "|    total_timesteps    | 6940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.274   |\n",
      "|    explained_variance | -5.37    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69399    |\n",
      "|    policy_loss        | 5.13     |\n",
      "|    value_loss         | 3.21e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 6941000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.32\n",
      "Num timesteps: 6942000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 6943000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.68\n",
      "Num timesteps: 6944000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.28\n",
      "Num timesteps: 6945000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.24\n",
      "Num timesteps: 6946000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.32\n",
      "Num timesteps: 6947000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.95\n",
      "Num timesteps: 6948000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.75\n",
      "Num timesteps: 6949000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.39\n",
      "Num timesteps: 6950000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 351      |\n",
      "|    ep_rew_mean        | 153      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 69500    |\n",
      "|    time_elapsed       | 5216     |\n",
      "|    total_timesteps    | 6950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.781   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69499    |\n",
      "|    policy_loss        | 0.23     |\n",
      "|    value_loss         | 23.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6951000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.73\n",
      "Num timesteps: 6952000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.31\n",
      "Num timesteps: 6953000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.67\n",
      "Num timesteps: 6954000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.92\n",
      "Num timesteps: 6955000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.90\n",
      "Num timesteps: 6956000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.39\n",
      "Num timesteps: 6957000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.55\n",
      "Num timesteps: 6958000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.20\n",
      "Num timesteps: 6959000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.27\n",
      "Num timesteps: 6960000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.24\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 360      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 69600    |\n",
      "|    time_elapsed       | 5225     |\n",
      "|    total_timesteps    | 6960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.857   |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69599    |\n",
      "|    policy_loss        | 1.98     |\n",
      "|    value_loss         | 14.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 6961000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.64\n",
      "Num timesteps: 6962000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.60\n",
      "Num timesteps: 6963000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.02\n",
      "Num timesteps: 6964000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.63\n",
      "Num timesteps: 6965000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.06\n",
      "Num timesteps: 6966000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.08\n",
      "Num timesteps: 6967000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.78\n",
      "Num timesteps: 6968000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.81\n",
      "Num timesteps: 6969000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.25\n",
      "Num timesteps: 6970000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 382      |\n",
      "|    ep_rew_mean        | 142      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 69700    |\n",
      "|    time_elapsed       | 5234     |\n",
      "|    total_timesteps    | 6970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0.116    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69699    |\n",
      "|    policy_loss        | 0.00838  |\n",
      "|    value_loss         | 0.0609   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 6971000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.30\n",
      "Num timesteps: 6972000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.89\n",
      "Num timesteps: 6973000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.06\n",
      "Num timesteps: 6974000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.15\n",
      "Num timesteps: 6975000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.93\n",
      "Num timesteps: 6976000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.26\n",
      "Num timesteps: 6977000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.55\n",
      "Num timesteps: 6978000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.83\n",
      "Num timesteps: 6979000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.89\n",
      "Num timesteps: 6980000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 431      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 69800    |\n",
      "|    time_elapsed       | 5245     |\n",
      "|    total_timesteps    | 6980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.759   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69799    |\n",
      "|    policy_loss        | -4.9     |\n",
      "|    value_loss         | 74.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 6981000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.12\n",
      "Num timesteps: 6982000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.82\n",
      "Num timesteps: 6983000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.93\n",
      "Num timesteps: 6984000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.43\n",
      "Num timesteps: 6985000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.65\n",
      "Num timesteps: 6986000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.46\n",
      "Num timesteps: 6987000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.36\n",
      "Num timesteps: 6988000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.08\n",
      "Num timesteps: 6989000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.52\n",
      "Num timesteps: 6990000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 468      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 69900    |\n",
      "|    time_elapsed       | 5258     |\n",
      "|    total_timesteps    | 6990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.939   |\n",
      "|    explained_variance | -0.0154  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69899    |\n",
      "|    policy_loss        | -0.328   |\n",
      "|    value_loss         | 11.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 6991000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.78\n",
      "Num timesteps: 6992000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.78\n",
      "Num timesteps: 6993000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.87\n",
      "Num timesteps: 6994000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.11\n",
      "Num timesteps: 6995000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.86\n",
      "Num timesteps: 6996000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.37\n",
      "Num timesteps: 6997000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.74\n",
      "Num timesteps: 6998000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.40\n",
      "Num timesteps: 6999000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.59\n",
      "Num timesteps: 7000000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | 121      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 70000    |\n",
      "|    time_elapsed       | 5269     |\n",
      "|    total_timesteps    | 7000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.536   |\n",
      "|    explained_variance | 0.758    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 69999    |\n",
      "|    policy_loss        | -21.9    |\n",
      "|    value_loss         | 2.58e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7001000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.10\n",
      "Num timesteps: 7002000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.70\n",
      "Num timesteps: 7003000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.72\n",
      "Num timesteps: 7004000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 122.39\n",
      "Num timesteps: 7005000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 120.32\n",
      "Num timesteps: 7006000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.93\n",
      "Num timesteps: 7007000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.89\n",
      "Num timesteps: 7008000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.27\n",
      "Num timesteps: 7009000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.26\n",
      "Num timesteps: 7010000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 131      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 70100    |\n",
      "|    time_elapsed       | 5279     |\n",
      "|    total_timesteps    | 7010000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | -0.00112 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70099    |\n",
      "|    policy_loss        | -2.35    |\n",
      "|    value_loss         | 42.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 7011000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.05\n",
      "Num timesteps: 7012000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.05\n",
      "Num timesteps: 7013000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.93\n",
      "Num timesteps: 7014000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.31\n",
      "Num timesteps: 7015000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.65\n",
      "Num timesteps: 7016000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.45\n",
      "Num timesteps: 7017000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.01\n",
      "Num timesteps: 7018000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.93\n",
      "Num timesteps: 7019000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.91\n",
      "Num timesteps: 7020000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 451      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 70200    |\n",
      "|    time_elapsed       | 5290     |\n",
      "|    total_timesteps    | 7020000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.536   |\n",
      "|    explained_variance | -0.419   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70199    |\n",
      "|    policy_loss        | -2.68    |\n",
      "|    value_loss         | 777      |\n",
      "------------------------------------\n",
      "Num timesteps: 7021000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.77\n",
      "Num timesteps: 7022000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.16\n",
      "Num timesteps: 7023000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.40\n",
      "Num timesteps: 7024000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.05\n",
      "Num timesteps: 7025000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.10\n",
      "Num timesteps: 7026000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.36\n",
      "Num timesteps: 7027000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.54\n",
      "Num timesteps: 7028000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.14\n",
      "Num timesteps: 7029000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7030000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 434      |\n",
      "|    ep_rew_mean        | 145      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 70300    |\n",
      "|    time_elapsed       | 5301     |\n",
      "|    total_timesteps    | 7030000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.714   |\n",
      "|    explained_variance | -1.46    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70299    |\n",
      "|    policy_loss        | 16.8     |\n",
      "|    value_loss         | 765      |\n",
      "------------------------------------\n",
      "Num timesteps: 7031000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.39\n",
      "Num timesteps: 7032000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.01\n",
      "Num timesteps: 7033000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.17\n",
      "Num timesteps: 7034000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.27\n",
      "Num timesteps: 7035000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.90\n",
      "Num timesteps: 7036000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.34\n",
      "Num timesteps: 7037000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.82\n",
      "Num timesteps: 7038000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.28\n",
      "Num timesteps: 7039000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.71\n",
      "Num timesteps: 7040000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.48\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 443      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 70400    |\n",
      "|    time_elapsed       | 5312     |\n",
      "|    total_timesteps    | 7040000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.863   |\n",
      "|    explained_variance | -0.0189  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70399    |\n",
      "|    policy_loss        | -1.78    |\n",
      "|    value_loss         | 19       |\n",
      "------------------------------------\n",
      "Num timesteps: 7041000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.20\n",
      "Num timesteps: 7042000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.71\n",
      "Num timesteps: 7043000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.65\n",
      "Num timesteps: 7044000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.06\n",
      "Num timesteps: 7045000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.15\n",
      "Num timesteps: 7046000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.19\n",
      "Num timesteps: 7047000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.07\n",
      "Num timesteps: 7048000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.14\n",
      "Num timesteps: 7049000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.49\n",
      "Num timesteps: 7050000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 498      |\n",
      "|    ep_rew_mean        | 139      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 70500    |\n",
      "|    time_elapsed       | 5325     |\n",
      "|    total_timesteps    | 7050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.604   |\n",
      "|    explained_variance | -1.16    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70499    |\n",
      "|    policy_loss        | 11.8     |\n",
      "|    value_loss         | 302      |\n",
      "------------------------------------\n",
      "Num timesteps: 7051000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.86\n",
      "Num timesteps: 7052000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.99\n",
      "Num timesteps: 7053000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.41\n",
      "Num timesteps: 7054000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.87\n",
      "Num timesteps: 7055000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.15\n",
      "Num timesteps: 7056000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.56\n",
      "Num timesteps: 7057000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.05\n",
      "Num timesteps: 7058000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.64\n",
      "Num timesteps: 7059000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.84\n",
      "Num timesteps: 7060000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.81\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 515      |\n",
      "|    ep_rew_mean        | 133      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 70600    |\n",
      "|    time_elapsed       | 5334     |\n",
      "|    total_timesteps    | 7060000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.585   |\n",
      "|    explained_variance | 0.992    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70599    |\n",
      "|    policy_loss        | 4.04     |\n",
      "|    value_loss         | 53.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 7061000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.80\n",
      "Num timesteps: 7062000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.11\n",
      "Num timesteps: 7063000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 131.33\n",
      "Num timesteps: 7064000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.96\n",
      "Num timesteps: 7065000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.10\n",
      "Num timesteps: 7066000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.54\n",
      "Num timesteps: 7067000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.96\n",
      "Num timesteps: 7068000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 123.47\n",
      "Num timesteps: 7069000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 127.08\n",
      "Num timesteps: 7070000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 526      |\n",
      "|    ep_rew_mean        | 126      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 70700    |\n",
      "|    time_elapsed       | 5345     |\n",
      "|    total_timesteps    | 7070000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.769   |\n",
      "|    explained_variance | 0.619    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70699    |\n",
      "|    policy_loss        | 1.3      |\n",
      "|    value_loss         | 16.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 7071000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.69\n",
      "Num timesteps: 7072000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 118.76\n",
      "Num timesteps: 7073000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 119.49\n",
      "Num timesteps: 7074000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.71\n",
      "Num timesteps: 7075000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.01\n",
      "Num timesteps: 7076000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.16\n",
      "Num timesteps: 7077000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.07\n",
      "Num timesteps: 7078000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.75\n",
      "Num timesteps: 7079000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.26\n",
      "Num timesteps: 7080000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 557      |\n",
      "|    ep_rew_mean        | 116      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 70800    |\n",
      "|    time_elapsed       | 5358     |\n",
      "|    total_timesteps    | 7080000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.489   |\n",
      "|    explained_variance | -0.0229  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70799    |\n",
      "|    policy_loss        | -6.83    |\n",
      "|    value_loss         | 269      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7081000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 116.23\n",
      "Num timesteps: 7082000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 113.43\n",
      "Num timesteps: 7083000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.36\n",
      "Num timesteps: 7084000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.96\n",
      "Num timesteps: 7085000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.48\n",
      "Num timesteps: 7086000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.76\n",
      "Num timesteps: 7087000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.41\n",
      "Num timesteps: 7088000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 109.72\n",
      "Num timesteps: 7089000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 112.54\n",
      "Num timesteps: 7090000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 109.06\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 572      |\n",
      "|    ep_rew_mean        | 109      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 70900    |\n",
      "|    time_elapsed       | 5369     |\n",
      "|    total_timesteps    | 7090000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | -0.00262 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70899    |\n",
      "|    policy_loss        | -1.74    |\n",
      "|    value_loss         | 12.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 7091000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 110.99\n",
      "Num timesteps: 7092000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 108.18\n",
      "Num timesteps: 7093000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 105.23\n",
      "Num timesteps: 7094000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 107.87\n",
      "Num timesteps: 7095000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 105.84\n",
      "Num timesteps: 7096000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 103.14\n",
      "Num timesteps: 7097000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 107.83\n",
      "Num timesteps: 7098000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 103.96\n",
      "Num timesteps: 7099000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 114.54\n",
      "Num timesteps: 7100000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 117.93\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 549      |\n",
      "|    ep_rew_mean        | 118      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 71000    |\n",
      "|    time_elapsed       | 5379     |\n",
      "|    total_timesteps    | 7100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.721   |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 70999    |\n",
      "|    policy_loss        | 0.41     |\n",
      "|    value_loss         | 1.92     |\n",
      "------------------------------------\n",
      "Num timesteps: 7101000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 118.57\n",
      "Num timesteps: 7102000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.07\n",
      "Num timesteps: 7103000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 121.37\n",
      "Num timesteps: 7104000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.84\n",
      "Num timesteps: 7105000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 125.09\n",
      "Num timesteps: 7106000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 124.41\n",
      "Num timesteps: 7107000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.36\n",
      "Num timesteps: 7108000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.19\n",
      "Num timesteps: 7109000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.85\n",
      "Num timesteps: 7110000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.77\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 505      |\n",
      "|    ep_rew_mean        | 130      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 71100    |\n",
      "|    time_elapsed       | 5388     |\n",
      "|    total_timesteps    | 7110000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.641   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71099    |\n",
      "|    policy_loss        | -2.25    |\n",
      "|    value_loss         | 22.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 7111000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 126.44\n",
      "Num timesteps: 7112000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.53\n",
      "Num timesteps: 7113000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.68\n",
      "Num timesteps: 7114000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 128.77\n",
      "Num timesteps: 7115000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.50\n",
      "Num timesteps: 7116000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.21\n",
      "Num timesteps: 7117000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.69\n",
      "Num timesteps: 7118000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.27\n",
      "Num timesteps: 7119000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.06\n",
      "Num timesteps: 7120000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.70\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 495      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 71200    |\n",
      "|    time_elapsed       | 5397     |\n",
      "|    total_timesteps    | 7120000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.78    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71199    |\n",
      "|    policy_loss        | -5.49    |\n",
      "|    value_loss         | 57.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 7121000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.90\n",
      "Num timesteps: 7122000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.71\n",
      "Num timesteps: 7123000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.93\n",
      "Num timesteps: 7124000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.61\n",
      "Num timesteps: 7125000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.71\n",
      "Num timesteps: 7126000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.59\n",
      "Num timesteps: 7127000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.57\n",
      "Num timesteps: 7128000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.54\n",
      "Num timesteps: 7129000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.88\n",
      "Num timesteps: 7130000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 441      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 71300    |\n",
      "|    time_elapsed       | 5406     |\n",
      "|    total_timesteps    | 7130000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.622   |\n",
      "|    explained_variance | 0.0482   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71299    |\n",
      "|    policy_loss        | -6.15    |\n",
      "|    value_loss         | 126      |\n",
      "------------------------------------\n",
      "Num timesteps: 7131000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.37\n",
      "Num timesteps: 7132000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.34\n",
      "Num timesteps: 7133000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.04\n",
      "Num timesteps: 7134000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.04\n",
      "Num timesteps: 7135000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.28\n",
      "Num timesteps: 7136000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.06\n",
      "Num timesteps: 7137000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.93\n",
      "Num timesteps: 7138000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.28\n",
      "Num timesteps: 7139000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7140000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.96\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 410      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 71400    |\n",
      "|    time_elapsed       | 5415     |\n",
      "|    total_timesteps    | 7140000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.875   |\n",
      "|    explained_variance | 0.0429   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71399    |\n",
      "|    policy_loss        | -1.08    |\n",
      "|    value_loss         | 4.99     |\n",
      "------------------------------------\n",
      "Num timesteps: 7141000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.43\n",
      "Num timesteps: 7142000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.22\n",
      "Num timesteps: 7143000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.24\n",
      "Num timesteps: 7144000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.76\n",
      "Num timesteps: 7145000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.89\n",
      "Num timesteps: 7146000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.10\n",
      "Num timesteps: 7147000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.80\n",
      "Num timesteps: 7148000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.05\n",
      "Num timesteps: 7149000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.31\n",
      "Num timesteps: 7150000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 419      |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 71500    |\n",
      "|    time_elapsed       | 5425     |\n",
      "|    total_timesteps    | 7150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.386   |\n",
      "|    explained_variance | -1.57    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71499    |\n",
      "|    policy_loss        | 23.6     |\n",
      "|    value_loss         | 3.98e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7151000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.68\n",
      "Num timesteps: 7152000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.59\n",
      "Num timesteps: 7153000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.95\n",
      "Num timesteps: 7154000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.92\n",
      "Num timesteps: 7155000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.80\n",
      "Num timesteps: 7156000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.99\n",
      "Num timesteps: 7157000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.26\n",
      "Num timesteps: 7158000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.57\n",
      "Num timesteps: 7159000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.51\n",
      "Num timesteps: 7160000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.95\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 440      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 71600    |\n",
      "|    time_elapsed       | 5435     |\n",
      "|    total_timesteps    | 7160000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.633   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71599    |\n",
      "|    policy_loss        | -2.16    |\n",
      "|    value_loss         | 11.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 7161000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.29\n",
      "Num timesteps: 7162000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.29\n",
      "Num timesteps: 7163000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.61\n",
      "Num timesteps: 7164000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.07\n",
      "Num timesteps: 7165000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.16\n",
      "Num timesteps: 7166000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.95\n",
      "Num timesteps: 7167000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.81\n",
      "Num timesteps: 7168000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.90\n",
      "Num timesteps: 7169000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.38\n",
      "Num timesteps: 7170000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 466      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 71700    |\n",
      "|    time_elapsed       | 5448     |\n",
      "|    total_timesteps    | 7170000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.593   |\n",
      "|    explained_variance | -0.753   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71699    |\n",
      "|    policy_loss        | 12.9     |\n",
      "|    value_loss         | 409      |\n",
      "------------------------------------\n",
      "Num timesteps: 7171000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.06\n",
      "Num timesteps: 7172000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.20\n",
      "Num timesteps: 7173000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.40\n",
      "Num timesteps: 7174000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.36\n",
      "Num timesteps: 7175000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.47\n",
      "Num timesteps: 7176000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.90\n",
      "Num timesteps: 7177000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.69\n",
      "Num timesteps: 7178000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.58\n",
      "Num timesteps: 7179000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.55\n",
      "Num timesteps: 7180000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 489      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 71800    |\n",
      "|    time_elapsed       | 5461     |\n",
      "|    total_timesteps    | 7180000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.91    |\n",
      "|    explained_variance | 0.274    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71799    |\n",
      "|    policy_loss        | -0.751   |\n",
      "|    value_loss         | 2.39     |\n",
      "------------------------------------\n",
      "Num timesteps: 7181000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.50\n",
      "Num timesteps: 7182000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.47\n",
      "Num timesteps: 7183000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.41\n",
      "Num timesteps: 7184000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.19\n",
      "Num timesteps: 7185000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.62\n",
      "Num timesteps: 7186000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.55\n",
      "Num timesteps: 7187000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.04\n",
      "Num timesteps: 7188000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.04\n",
      "Num timesteps: 7189000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.14\n",
      "Num timesteps: 7190000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.50\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 498      |\n",
      "|    ep_rew_mean        | 159      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 71900    |\n",
      "|    time_elapsed       | 5471     |\n",
      "|    total_timesteps    | 7190000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.633   |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71899    |\n",
      "|    policy_loss        | 0.448    |\n",
      "|    value_loss         | 26.8     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7191000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.51\n",
      "Num timesteps: 7192000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.95\n",
      "Num timesteps: 7193000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.41\n",
      "Num timesteps: 7194000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.30\n",
      "Num timesteps: 7195000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.34\n",
      "Num timesteps: 7196000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.29\n",
      "Num timesteps: 7197000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.31\n",
      "Num timesteps: 7198000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.94\n",
      "Num timesteps: 7199000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.37\n",
      "Num timesteps: 7200000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.21\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 497      |\n",
      "|    ep_rew_mean        | 163      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 72000    |\n",
      "|    time_elapsed       | 5479     |\n",
      "|    total_timesteps    | 7200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.461   |\n",
      "|    explained_variance | 0.369    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 71999    |\n",
      "|    policy_loss        | 4.05     |\n",
      "|    value_loss         | 226      |\n",
      "------------------------------------\n",
      "Num timesteps: 7201000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.91\n",
      "Num timesteps: 7202000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.47\n",
      "Num timesteps: 7203000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.11\n",
      "Num timesteps: 7204000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.25\n",
      "Num timesteps: 7205000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.56\n",
      "Num timesteps: 7206000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.44\n",
      "Num timesteps: 7207000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.19\n",
      "Num timesteps: 7208000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.96\n",
      "Num timesteps: 7209000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.30\n",
      "Num timesteps: 7210000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 509      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 72100    |\n",
      "|    time_elapsed       | 5489     |\n",
      "|    total_timesteps    | 7210000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.822   |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72099    |\n",
      "|    policy_loss        | 2.67     |\n",
      "|    value_loss         | 18.3     |\n",
      "------------------------------------\n",
      "Num timesteps: 7211000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.55\n",
      "Num timesteps: 7212000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.61\n",
      "Num timesteps: 7213000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.38\n",
      "Num timesteps: 7214000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.57\n",
      "Num timesteps: 7215000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.65\n",
      "Num timesteps: 7216000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.59\n",
      "Num timesteps: 7217000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.98\n",
      "Num timesteps: 7218000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.16\n",
      "Num timesteps: 7219000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.81\n",
      "Num timesteps: 7220000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.97\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 460      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 72200    |\n",
      "|    time_elapsed       | 5499     |\n",
      "|    total_timesteps    | 7220000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | -0.115   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72199    |\n",
      "|    policy_loss        | 1.99     |\n",
      "|    value_loss         | 28.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 7221000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.53\n",
      "Num timesteps: 7222000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.61\n",
      "Num timesteps: 7223000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.55\n",
      "Num timesteps: 7224000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.57\n",
      "Num timesteps: 7225000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.31\n",
      "Num timesteps: 7226000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.98\n",
      "Num timesteps: 7227000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.41\n",
      "Num timesteps: 7228000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.52\n",
      "Num timesteps: 7229000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.87\n",
      "Num timesteps: 7230000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.40\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 416      |\n",
      "|    ep_rew_mean        | 185      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 72300    |\n",
      "|    time_elapsed       | 5506     |\n",
      "|    total_timesteps    | 7230000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.572   |\n",
      "|    explained_variance | -6.73    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72299    |\n",
      "|    policy_loss        | -0.705   |\n",
      "|    value_loss         | 137      |\n",
      "------------------------------------\n",
      "Num timesteps: 7231000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.41\n",
      "Num timesteps: 7232000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.63\n",
      "Num timesteps: 7233000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.66\n",
      "Num timesteps: 7234000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.31\n",
      "Num timesteps: 7235000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.26\n",
      "Num timesteps: 7236000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.95\n",
      "Num timesteps: 7237000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.49\n",
      "Num timesteps: 7238000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.78\n",
      "Num timesteps: 7239000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.20\n",
      "Num timesteps: 7240000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 406      |\n",
      "|    ep_rew_mean        | 183      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 72400    |\n",
      "|    time_elapsed       | 5517     |\n",
      "|    total_timesteps    | 7240000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.866   |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72399    |\n",
      "|    policy_loss        | -2.95    |\n",
      "|    value_loss         | 28.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 7241000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.48\n",
      "Num timesteps: 7242000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.85\n",
      "Num timesteps: 7243000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.50\n",
      "Num timesteps: 7244000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.04\n",
      "Num timesteps: 7245000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.26\n",
      "Num timesteps: 7246000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.98\n",
      "Num timesteps: 7247000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.48\n",
      "Num timesteps: 7248000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.85\n",
      "Num timesteps: 7249000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7250000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.72\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 381      |\n",
      "|    ep_rew_mean        | 166      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 72500    |\n",
      "|    time_elapsed       | 5525     |\n",
      "|    total_timesteps    | 7250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.552    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72499    |\n",
      "|    policy_loss        | 1.59     |\n",
      "|    value_loss         | 358      |\n",
      "------------------------------------\n",
      "Num timesteps: 7251000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.69\n",
      "Num timesteps: 7252000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.19\n",
      "Num timesteps: 7253000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.03\n",
      "Num timesteps: 7254000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.78\n",
      "Num timesteps: 7255000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.08\n",
      "Num timesteps: 7256000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.11\n",
      "Num timesteps: 7257000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.39\n",
      "Num timesteps: 7258000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.76\n",
      "Num timesteps: 7259000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.65\n",
      "Num timesteps: 7260000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 72600    |\n",
      "|    time_elapsed       | 5535     |\n",
      "|    total_timesteps    | 7260000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.482   |\n",
      "|    explained_variance | -0.0956  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72599    |\n",
      "|    policy_loss        | 4.02     |\n",
      "|    value_loss         | 95.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 7261000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.81\n",
      "Num timesteps: 7262000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.78\n",
      "Num timesteps: 7263000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.33\n",
      "Num timesteps: 7264000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.11\n",
      "Num timesteps: 7265000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.70\n",
      "Num timesteps: 7266000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.56\n",
      "Num timesteps: 7267000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.17\n",
      "Num timesteps: 7268000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.52\n",
      "Num timesteps: 7269000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.97\n",
      "Num timesteps: 7270000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.51\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 374      |\n",
      "|    ep_rew_mean        | 157      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 72700    |\n",
      "|    time_elapsed       | 5543     |\n",
      "|    total_timesteps    | 7270000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.144   |\n",
      "|    explained_variance | -2.66    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72699    |\n",
      "|    policy_loss        | -0.363   |\n",
      "|    value_loss         | 3.53e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7271000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.77\n",
      "Num timesteps: 7272000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.05\n",
      "Num timesteps: 7273000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.61\n",
      "Num timesteps: 7274000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.95\n",
      "Num timesteps: 7275000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.11\n",
      "Num timesteps: 7276000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.37\n",
      "Num timesteps: 7277000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.92\n",
      "Num timesteps: 7278000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.41\n",
      "Num timesteps: 7279000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.47\n",
      "Num timesteps: 7280000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.20\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 402      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 72800    |\n",
      "|    time_elapsed       | 5552     |\n",
      "|    total_timesteps    | 7280000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.678   |\n",
      "|    explained_variance | -3.6     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72799    |\n",
      "|    policy_loss        | -0.251   |\n",
      "|    value_loss         | 0.837    |\n",
      "------------------------------------\n",
      "Num timesteps: 7281000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.25\n",
      "Num timesteps: 7282000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.91\n",
      "Num timesteps: 7283000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.14\n",
      "Num timesteps: 7284000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.74\n",
      "Num timesteps: 7285000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.58\n",
      "Num timesteps: 7286000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.65\n",
      "Num timesteps: 7287000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.37\n",
      "Num timesteps: 7288000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.57\n",
      "Num timesteps: 7289000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.37\n",
      "Num timesteps: 7290000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.46\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 383      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 72900    |\n",
      "|    time_elapsed       | 5558     |\n",
      "|    total_timesteps    | 7290000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.375   |\n",
      "|    explained_variance | 0.665    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72899    |\n",
      "|    policy_loss        | -30.8    |\n",
      "|    value_loss         | 5.24e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7291000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.07\n",
      "Num timesteps: 7292000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.11\n",
      "Num timesteps: 7293000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.45\n",
      "Num timesteps: 7294000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.54\n",
      "Num timesteps: 7295000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.39\n",
      "Num timesteps: 7296000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.61\n",
      "Num timesteps: 7297000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.19\n",
      "Num timesteps: 7298000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.98\n",
      "Num timesteps: 7299000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.67\n",
      "Num timesteps: 7300000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 181      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73000    |\n",
      "|    time_elapsed       | 5567     |\n",
      "|    total_timesteps    | 7300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 0.0621   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 72999    |\n",
      "|    policy_loss        | 4.41     |\n",
      "|    value_loss         | 113      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7301000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.36\n",
      "Num timesteps: 7302000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.16\n",
      "Num timesteps: 7303000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.70\n",
      "Num timesteps: 7304000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.93\n",
      "Num timesteps: 7305000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.66\n",
      "Num timesteps: 7306000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.14\n",
      "Num timesteps: 7307000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.96\n",
      "Num timesteps: 7308000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.87\n",
      "Num timesteps: 7309000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.00\n",
      "Num timesteps: 7310000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 351      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73100    |\n",
      "|    time_elapsed       | 5574     |\n",
      "|    total_timesteps    | 7310000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.639   |\n",
      "|    explained_variance | 0.153    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73099    |\n",
      "|    policy_loss        | -1.22    |\n",
      "|    value_loss         | 3.33     |\n",
      "------------------------------------\n",
      "Num timesteps: 7311000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.59\n",
      "Num timesteps: 7312000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.54\n",
      "Num timesteps: 7313000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.19\n",
      "Num timesteps: 7314000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.99\n",
      "Num timesteps: 7315000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.63\n",
      "Num timesteps: 7316000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.96\n",
      "Num timesteps: 7317000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.40\n",
      "Num timesteps: 7318000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 199.44\n",
      "Num timesteps: 7319000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.60\n",
      "Num timesteps: 7320000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 328      |\n",
      "|    ep_rew_mean        | 201      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73200    |\n",
      "|    time_elapsed       | 5581     |\n",
      "|    total_timesteps    | 7320000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.836   |\n",
      "|    explained_variance | -0.00654 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73199    |\n",
      "|    policy_loss        | -3.05    |\n",
      "|    value_loss         | 16       |\n",
      "------------------------------------\n",
      "Num timesteps: 7321000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.83\n",
      "Num timesteps: 7322000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.41\n",
      "Num timesteps: 7323000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.25\n",
      "Num timesteps: 7324000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.68\n",
      "Num timesteps: 7325000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.04\n",
      "Num timesteps: 7326000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.31\n",
      "Num timesteps: 7327000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.95\n",
      "Num timesteps: 7328000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.46\n",
      "Num timesteps: 7329000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.92\n",
      "Num timesteps: 7330000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.76\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73300    |\n",
      "|    time_elapsed       | 5588     |\n",
      "|    total_timesteps    | 7330000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | -1.14    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73299    |\n",
      "|    policy_loss        | 8.69     |\n",
      "|    value_loss         | 1.36e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7331000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.96\n",
      "Num timesteps: 7332000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.25\n",
      "Num timesteps: 7333000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.50\n",
      "Num timesteps: 7334000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.85\n",
      "Num timesteps: 7335000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.50\n",
      "Num timesteps: 7336000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.65\n",
      "Num timesteps: 7337000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.90\n",
      "Num timesteps: 7338000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.45\n",
      "Num timesteps: 7339000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.21\n",
      "Num timesteps: 7340000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73400    |\n",
      "|    time_elapsed       | 5595     |\n",
      "|    total_timesteps    | 7340000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | -0.893   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73399    |\n",
      "|    policy_loss        | -0.125   |\n",
      "|    value_loss         | 0.0913   |\n",
      "------------------------------------\n",
      "Num timesteps: 7341000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.82\n",
      "Num timesteps: 7342000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.86\n",
      "Num timesteps: 7343000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.21\n",
      "Num timesteps: 7344000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.95\n",
      "Num timesteps: 7345000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.21\n",
      "Num timesteps: 7346000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.68\n",
      "Num timesteps: 7347000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.90\n",
      "Num timesteps: 7348000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.64\n",
      "Num timesteps: 7349000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.33\n",
      "Num timesteps: 7350000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 338      |\n",
      "|    ep_rew_mean        | 184      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73500    |\n",
      "|    time_elapsed       | 5603     |\n",
      "|    total_timesteps    | 7350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.181   |\n",
      "|    explained_variance | -0.54    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73499    |\n",
      "|    policy_loss        | -0.539   |\n",
      "|    value_loss         | 851      |\n",
      "------------------------------------\n",
      "Num timesteps: 7351000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.53\n",
      "Num timesteps: 7352000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.24\n",
      "Num timesteps: 7353000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.36\n",
      "Num timesteps: 7354000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.12\n",
      "Num timesteps: 7355000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.35\n",
      "Num timesteps: 7356000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.30\n",
      "Num timesteps: 7357000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.88\n",
      "Num timesteps: 7358000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.83\n",
      "Num timesteps: 7359000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7360000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73600    |\n",
      "|    time_elapsed       | 5613     |\n",
      "|    total_timesteps    | 7360000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.599   |\n",
      "|    explained_variance | -0.202   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73599    |\n",
      "|    policy_loss        | 4.72     |\n",
      "|    value_loss         | 96.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 7361000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.95\n",
      "Num timesteps: 7362000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.46\n",
      "Num timesteps: 7363000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.49\n",
      "Num timesteps: 7364000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.41\n",
      "Num timesteps: 7365000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.84\n",
      "Num timesteps: 7366000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.20\n",
      "Num timesteps: 7367000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.02\n",
      "Num timesteps: 7368000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.93\n",
      "Num timesteps: 7369000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.35\n",
      "Num timesteps: 7370000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.43\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73700    |\n",
      "|    time_elapsed       | 5619     |\n",
      "|    total_timesteps    | 7370000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.748   |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73699    |\n",
      "|    policy_loss        | 1.27     |\n",
      "|    value_loss         | 5.54     |\n",
      "------------------------------------\n",
      "Num timesteps: 7371000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.67\n",
      "Num timesteps: 7372000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.44\n",
      "Num timesteps: 7373000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.39\n",
      "Num timesteps: 7374000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.37\n",
      "Num timesteps: 7375000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.44\n",
      "Num timesteps: 7376000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.31\n",
      "Num timesteps: 7377000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.45\n",
      "Num timesteps: 7378000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.34\n",
      "Num timesteps: 7379000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.07\n",
      "Num timesteps: 7380000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.02\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 376      |\n",
      "|    ep_rew_mean        | 160      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 73800    |\n",
      "|    time_elapsed       | 5628     |\n",
      "|    total_timesteps    | 7380000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73799    |\n",
      "|    policy_loss        | -1.01    |\n",
      "|    value_loss         | 11.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 7381000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.96\n",
      "Num timesteps: 7382000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.58\n",
      "Num timesteps: 7383000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.99\n",
      "Num timesteps: 7384000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.75\n",
      "Num timesteps: 7385000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.94\n",
      "Num timesteps: 7386000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.95\n",
      "Num timesteps: 7387000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.81\n",
      "Num timesteps: 7388000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.48\n",
      "Num timesteps: 7389000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.63\n",
      "Num timesteps: 7390000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 73900    |\n",
      "|    time_elapsed       | 5637     |\n",
      "|    total_timesteps    | 7390000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.542   |\n",
      "|    explained_variance | -0.0268  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73899    |\n",
      "|    policy_loss        | -3.2     |\n",
      "|    value_loss         | 56.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 7391000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.78\n",
      "Num timesteps: 7392000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.08\n",
      "Num timesteps: 7393000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.94\n",
      "Num timesteps: 7394000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.98\n",
      "Num timesteps: 7395000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.98\n",
      "Num timesteps: 7396000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.21\n",
      "Num timesteps: 7397000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.26\n",
      "Num timesteps: 7398000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.30\n",
      "Num timesteps: 7399000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.59\n",
      "Num timesteps: 7400000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 437      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74000    |\n",
      "|    time_elapsed       | 5645     |\n",
      "|    total_timesteps    | 7400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.476   |\n",
      "|    explained_variance | 0.996    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 73999    |\n",
      "|    policy_loss        | 0.36     |\n",
      "|    value_loss         | 4.51     |\n",
      "------------------------------------\n",
      "Num timesteps: 7401000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.63\n",
      "Num timesteps: 7402000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.48\n",
      "Num timesteps: 7403000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.40\n",
      "Num timesteps: 7404000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.18\n",
      "Num timesteps: 7405000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.60\n",
      "Num timesteps: 7406000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.04\n",
      "Num timesteps: 7407000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.98\n",
      "Num timesteps: 7408000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.55\n",
      "Num timesteps: 7409000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.22\n",
      "Num timesteps: 7410000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.11\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 411      |\n",
      "|    ep_rew_mean        | 168      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74100    |\n",
      "|    time_elapsed       | 5653     |\n",
      "|    total_timesteps    | 7410000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.914   |\n",
      "|    explained_variance | 0.00895  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74099    |\n",
      "|    policy_loss        | 11.3     |\n",
      "|    value_loss         | 198      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7411000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.32\n",
      "Num timesteps: 7412000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.03\n",
      "Num timesteps: 7413000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.31\n",
      "Num timesteps: 7414000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.38\n",
      "Num timesteps: 7415000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.61\n",
      "Num timesteps: 7416000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.43\n",
      "Num timesteps: 7417000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.13\n",
      "Num timesteps: 7418000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.95\n",
      "Num timesteps: 7419000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.60\n",
      "Num timesteps: 7420000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.19\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 365      |\n",
      "|    ep_rew_mean        | 180      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74200    |\n",
      "|    time_elapsed       | 5660     |\n",
      "|    total_timesteps    | 7420000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.334   |\n",
      "|    explained_variance | 0.155    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74199    |\n",
      "|    policy_loss        | 3.3      |\n",
      "|    value_loss         | 520      |\n",
      "------------------------------------\n",
      "Num timesteps: 7421000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.05\n",
      "Num timesteps: 7422000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.14\n",
      "Num timesteps: 7423000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.37\n",
      "Num timesteps: 7424000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.82\n",
      "Num timesteps: 7425000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.50\n",
      "Num timesteps: 7426000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.16\n",
      "Num timesteps: 7427000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.84\n",
      "Num timesteps: 7428000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.82\n",
      "Num timesteps: 7429000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.87\n",
      "Num timesteps: 7430000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 341      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74300    |\n",
      "|    time_elapsed       | 5668     |\n",
      "|    total_timesteps    | 7430000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.199   |\n",
      "|    explained_variance | -0.736   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74299    |\n",
      "|    policy_loss        | -0.4     |\n",
      "|    value_loss         | 733      |\n",
      "------------------------------------\n",
      "Num timesteps: 7431000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.10\n",
      "Num timesteps: 7432000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.86\n",
      "Num timesteps: 7433000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.41\n",
      "Num timesteps: 7434000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.59\n",
      "Num timesteps: 7435000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.22\n",
      "Num timesteps: 7436000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.73\n",
      "Num timesteps: 7437000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.81\n",
      "Num timesteps: 7438000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.57\n",
      "Num timesteps: 7439000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.23\n",
      "Num timesteps: 7440000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74400    |\n",
      "|    time_elapsed       | 5675     |\n",
      "|    total_timesteps    | 7440000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74399    |\n",
      "|    policy_loss        | -29.9    |\n",
      "|    value_loss         | 4.21e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7441000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.58\n",
      "Num timesteps: 7442000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.61\n",
      "Num timesteps: 7443000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 158.91\n",
      "Num timesteps: 7444000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.79\n",
      "Num timesteps: 7445000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.78\n",
      "Num timesteps: 7446000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.33\n",
      "Num timesteps: 7447000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.56\n",
      "Num timesteps: 7448000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.47\n",
      "Num timesteps: 7449000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.74\n",
      "Num timesteps: 7450000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 320      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74500    |\n",
      "|    time_elapsed       | 5683     |\n",
      "|    total_timesteps    | 7450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | -0.00767 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74499    |\n",
      "|    policy_loss        | -2.83    |\n",
      "|    value_loss         | 23.1     |\n",
      "------------------------------------\n",
      "Num timesteps: 7451000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.00\n",
      "Num timesteps: 7452000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.70\n",
      "Num timesteps: 7453000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.89\n",
      "Num timesteps: 7454000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.78\n",
      "Num timesteps: 7455000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.45\n",
      "Num timesteps: 7456000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.23\n",
      "Num timesteps: 7457000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.85\n",
      "Num timesteps: 7458000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.02\n",
      "Num timesteps: 7459000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.01\n",
      "Num timesteps: 7460000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.69\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 346      |\n",
      "|    ep_rew_mean        | 141      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74600    |\n",
      "|    time_elapsed       | 5692     |\n",
      "|    total_timesteps    | 7460000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.903   |\n",
      "|    explained_variance | -0.138   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74599    |\n",
      "|    policy_loss        | 0.592    |\n",
      "|    value_loss         | 4.02     |\n",
      "------------------------------------\n",
      "Num timesteps: 7461000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.15\n",
      "Num timesteps: 7462000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.45\n",
      "Num timesteps: 7463000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.06\n",
      "Num timesteps: 7464000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.49\n",
      "Num timesteps: 7465000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.99\n",
      "Num timesteps: 7466000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.33\n",
      "Num timesteps: 7467000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.60\n",
      "Num timesteps: 7468000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.14\n",
      "Num timesteps: 7469000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7470000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 359      |\n",
      "|    ep_rew_mean        | 135      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74700    |\n",
      "|    time_elapsed       | 5700     |\n",
      "|    total_timesteps    | 7470000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.413   |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74699    |\n",
      "|    policy_loss        | 0.237    |\n",
      "|    value_loss         | 10.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 7471000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.25\n",
      "Num timesteps: 7472000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.39\n",
      "Num timesteps: 7473000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.51\n",
      "Num timesteps: 7474000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.23\n",
      "Num timesteps: 7475000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.39\n",
      "Num timesteps: 7476000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.15\n",
      "Num timesteps: 7477000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.83\n",
      "Num timesteps: 7478000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.26\n",
      "Num timesteps: 7479000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.88\n",
      "Num timesteps: 7480000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.82\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 401      |\n",
      "|    ep_rew_mean        | 147      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 74800    |\n",
      "|    time_elapsed       | 5710     |\n",
      "|    total_timesteps    | 7480000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.667   |\n",
      "|    explained_variance | 0.972    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74799    |\n",
      "|    policy_loss        | 3.88     |\n",
      "|    value_loss         | 64.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 7481000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.83\n",
      "Num timesteps: 7482000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.90\n",
      "Num timesteps: 7483000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.10\n",
      "Num timesteps: 7484000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.65\n",
      "Num timesteps: 7485000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.71\n",
      "Num timesteps: 7486000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.68\n",
      "Num timesteps: 7487000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.40\n",
      "Num timesteps: 7488000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.71\n",
      "Num timesteps: 7489000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.91\n",
      "Num timesteps: 7490000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 396      |\n",
      "|    ep_rew_mean        | 155      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 74900    |\n",
      "|    time_elapsed       | 5717     |\n",
      "|    total_timesteps    | 7490000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.313   |\n",
      "|    explained_variance | 0.404    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74899    |\n",
      "|    policy_loss        | 1.15     |\n",
      "|    value_loss         | 503      |\n",
      "------------------------------------\n",
      "Num timesteps: 7491000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 157.86\n",
      "Num timesteps: 7492000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.18\n",
      "Num timesteps: 7493000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.03\n",
      "Num timesteps: 7494000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.09\n",
      "Num timesteps: 7495000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.29\n",
      "Num timesteps: 7496000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.59\n",
      "Num timesteps: 7497000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.71\n",
      "Num timesteps: 7498000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.24\n",
      "Num timesteps: 7499000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.12\n",
      "Num timesteps: 7500000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.32\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 363      |\n",
      "|    ep_rew_mean        | 156      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75000    |\n",
      "|    time_elapsed       | 5724     |\n",
      "|    total_timesteps    | 7500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.325   |\n",
      "|    explained_variance | -0.721   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 74999    |\n",
      "|    policy_loss        | 1.65     |\n",
      "|    value_loss         | 741      |\n",
      "------------------------------------\n",
      "Num timesteps: 7501000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.96\n",
      "Num timesteps: 7502000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.58\n",
      "Num timesteps: 7503000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.75\n",
      "Num timesteps: 7504000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.94\n",
      "Num timesteps: 7505000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.05\n",
      "Num timesteps: 7506000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.80\n",
      "Num timesteps: 7507000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.45\n",
      "Num timesteps: 7508000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.60\n",
      "Num timesteps: 7509000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.91\n",
      "Num timesteps: 7510000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 348      |\n",
      "|    ep_rew_mean        | 162      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75100    |\n",
      "|    time_elapsed       | 5731     |\n",
      "|    total_timesteps    | 7510000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.545   |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75099    |\n",
      "|    policy_loss        | -0.193   |\n",
      "|    value_loss         | 651      |\n",
      "------------------------------------\n",
      "Num timesteps: 7511000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.31\n",
      "Num timesteps: 7512000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 160.91\n",
      "Num timesteps: 7513000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.43\n",
      "Num timesteps: 7514000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.50\n",
      "Num timesteps: 7515000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.75\n",
      "Num timesteps: 7516000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.73\n",
      "Num timesteps: 7517000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.77\n",
      "Num timesteps: 7518000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.58\n",
      "Num timesteps: 7519000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.78\n",
      "Num timesteps: 7520000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.91\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 318      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75200    |\n",
      "|    time_elapsed       | 5738     |\n",
      "|    total_timesteps    | 7520000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.767   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75199    |\n",
      "|    policy_loss        | 1.38     |\n",
      "|    value_loss         | 13.2     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7521000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.02\n",
      "Num timesteps: 7522000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.75\n",
      "Num timesteps: 7523000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.45\n",
      "Num timesteps: 7524000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.81\n",
      "Num timesteps: 7525000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.61\n",
      "Num timesteps: 7526000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.55\n",
      "Num timesteps: 7527000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.88\n",
      "Num timesteps: 7528000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.60\n",
      "Num timesteps: 7529000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.31\n",
      "Num timesteps: 7530000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.12\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 318      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75300    |\n",
      "|    time_elapsed       | 5746     |\n",
      "|    total_timesteps    | 7530000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.532   |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75299    |\n",
      "|    policy_loss        | -2.42    |\n",
      "|    value_loss         | 83.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 7531000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.28\n",
      "Num timesteps: 7532000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.95\n",
      "Num timesteps: 7533000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.03\n",
      "Num timesteps: 7534000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.75\n",
      "Num timesteps: 7535000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.69\n",
      "Num timesteps: 7536000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.79\n",
      "Num timesteps: 7537000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.92\n",
      "Num timesteps: 7538000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.88\n",
      "Num timesteps: 7539000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.56\n",
      "Num timesteps: 7540000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.38\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 322       |\n",
      "|    ep_rew_mean        | 169       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 75400     |\n",
      "|    time_elapsed       | 5753      |\n",
      "|    total_timesteps    | 7540000   |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.692    |\n",
      "|    explained_variance | -0.000928 |\n",
      "|    learning_rate      | 0.0003    |\n",
      "|    n_updates          | 75399     |\n",
      "|    policy_loss        | 1.13      |\n",
      "|    value_loss         | 3.34      |\n",
      "-------------------------------------\n",
      "Num timesteps: 7541000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.78\n",
      "Num timesteps: 7542000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.00\n",
      "Num timesteps: 7543000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.43\n",
      "Num timesteps: 7544000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.65\n",
      "Num timesteps: 7545000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.01\n",
      "Num timesteps: 7546000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.59\n",
      "Num timesteps: 7547000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.35\n",
      "Num timesteps: 7548000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.76\n",
      "Num timesteps: 7549000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.76\n",
      "Num timesteps: 7550000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.79\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 151      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75500    |\n",
      "|    time_elapsed       | 5759     |\n",
      "|    total_timesteps    | 7550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | -0.173   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75499    |\n",
      "|    policy_loss        | -1.16    |\n",
      "|    value_loss         | 3.35     |\n",
      "------------------------------------\n",
      "Num timesteps: 7551000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.25\n",
      "Num timesteps: 7552000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.94\n",
      "Num timesteps: 7553000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.23\n",
      "Num timesteps: 7554000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.31\n",
      "Num timesteps: 7555000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.33\n",
      "Num timesteps: 7556000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.84\n",
      "Num timesteps: 7557000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.85\n",
      "Num timesteps: 7558000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.90\n",
      "Num timesteps: 7559000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.18\n",
      "Num timesteps: 7560000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 316      |\n",
      "|    ep_rew_mean        | 144      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75600    |\n",
      "|    time_elapsed       | 5768     |\n",
      "|    total_timesteps    | 7560000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.338    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75599    |\n",
      "|    policy_loss        | 3.95     |\n",
      "|    value_loss         | 103      |\n",
      "------------------------------------\n",
      "Num timesteps: 7561000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.22\n",
      "Num timesteps: 7562000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.46\n",
      "Num timesteps: 7563000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.28\n",
      "Num timesteps: 7564000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.60\n",
      "Num timesteps: 7565000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 143.37\n",
      "Num timesteps: 7566000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.72\n",
      "Num timesteps: 7567000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.25\n",
      "Num timesteps: 7568000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.86\n",
      "Num timesteps: 7569000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.05\n",
      "Num timesteps: 7570000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.75\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 326      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75700    |\n",
      "|    time_elapsed       | 5774     |\n",
      "|    total_timesteps    | 7570000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.314   |\n",
      "|    explained_variance | -3.09    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75699    |\n",
      "|    policy_loss        | 13.4     |\n",
      "|    value_loss         | 3.11e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7571000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.82\n",
      "Num timesteps: 7572000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.28\n",
      "Num timesteps: 7573000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 132.78\n",
      "Num timesteps: 7574000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.65\n",
      "Num timesteps: 7575000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 130.25\n",
      "Num timesteps: 7576000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.19\n",
      "Num timesteps: 7577000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.97\n",
      "Num timesteps: 7578000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7579000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.62\n",
      "Num timesteps: 7580000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.94\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 348      |\n",
      "|    ep_rew_mean        | 137      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75800    |\n",
      "|    time_elapsed       | 5784     |\n",
      "|    total_timesteps    | 7580000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.42    |\n",
      "|    explained_variance | -0.00406 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75799    |\n",
      "|    policy_loss        | -8.6     |\n",
      "|    value_loss         | 556      |\n",
      "------------------------------------\n",
      "Num timesteps: 7581000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 136.45\n",
      "Num timesteps: 7582000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.24\n",
      "Num timesteps: 7583000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.01\n",
      "Num timesteps: 7584000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.44\n",
      "Num timesteps: 7585000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.09\n",
      "Num timesteps: 7586000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 148.48\n",
      "Num timesteps: 7587000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.86\n",
      "Num timesteps: 7588000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.53\n",
      "Num timesteps: 7589000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.38\n",
      "Num timesteps: 7590000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.87\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 385      |\n",
      "|    ep_rew_mean        | 154      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 75900    |\n",
      "|    time_elapsed       | 5793     |\n",
      "|    total_timesteps    | 7590000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0.127    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75899    |\n",
      "|    policy_loss        | 11.2     |\n",
      "|    value_loss         | 802      |\n",
      "------------------------------------\n",
      "Num timesteps: 7591000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 153.48\n",
      "Num timesteps: 7592000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.08\n",
      "Num timesteps: 7593000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.39\n",
      "Num timesteps: 7594000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.02\n",
      "Num timesteps: 7595000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.75\n",
      "Num timesteps: 7596000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.09\n",
      "Num timesteps: 7597000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.17\n",
      "Num timesteps: 7598000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 152.11\n",
      "Num timesteps: 7599000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.31\n",
      "Num timesteps: 7600000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.92\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 395      |\n",
      "|    ep_rew_mean        | 148      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 76000    |\n",
      "|    time_elapsed       | 5802     |\n",
      "|    total_timesteps    | 7600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.764   |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 75999    |\n",
      "|    policy_loss        | -6.18    |\n",
      "|    value_loss         | 119      |\n",
      "------------------------------------\n",
      "Num timesteps: 7601000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 146.07\n",
      "Num timesteps: 7602000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 145.24\n",
      "Num timesteps: 7603000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.53\n",
      "Num timesteps: 7604000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.81\n",
      "Num timesteps: 7605000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.41\n",
      "Num timesteps: 7606000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.66\n",
      "Num timesteps: 7607000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.73\n",
      "Num timesteps: 7608000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.03\n",
      "Num timesteps: 7609000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.27\n",
      "Num timesteps: 7610000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.59\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 418      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76100    |\n",
      "|    time_elapsed       | 5814     |\n",
      "|    total_timesteps    | 7610000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0.182    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76099    |\n",
      "|    policy_loss        | 0.185    |\n",
      "|    value_loss         | 0.401    |\n",
      "------------------------------------\n",
      "Num timesteps: 7611000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 144.41\n",
      "Num timesteps: 7612000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.00\n",
      "Num timesteps: 7613000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 150.47\n",
      "Num timesteps: 7614000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.56\n",
      "Num timesteps: 7615000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 151.38\n",
      "Num timesteps: 7616000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.24\n",
      "Num timesteps: 7617000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 147.12\n",
      "Num timesteps: 7618000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.67\n",
      "Num timesteps: 7619000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.43\n",
      "Num timesteps: 7620000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 424      |\n",
      "|    ep_rew_mean        | 138      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76200    |\n",
      "|    time_elapsed       | 5822     |\n",
      "|    total_timesteps    | 7620000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.689   |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76199    |\n",
      "|    policy_loss        | 1.79     |\n",
      "|    value_loss         | 14.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 7621000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.86\n",
      "Num timesteps: 7622000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.98\n",
      "Num timesteps: 7623000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.73\n",
      "Num timesteps: 7624000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.86\n",
      "Num timesteps: 7625000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.35\n",
      "Num timesteps: 7626000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 133.44\n",
      "Num timesteps: 7627000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.13\n",
      "Num timesteps: 7628000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 129.50\n",
      "Num timesteps: 7629000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.52\n",
      "Num timesteps: 7630000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.33\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 436      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76300    |\n",
      "|    time_elapsed       | 5830     |\n",
      "|    total_timesteps    | 7630000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.744   |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76299    |\n",
      "|    policy_loss        | 3.83     |\n",
      "|    value_loss         | 44.9     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7631000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 134.57\n",
      "Num timesteps: 7632000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 139.22\n",
      "Num timesteps: 7633000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.82\n",
      "Num timesteps: 7634000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 140.07\n",
      "Num timesteps: 7635000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.32\n",
      "Num timesteps: 7636000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 135.88\n",
      "Num timesteps: 7637000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 138.95\n",
      "Num timesteps: 7638000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 141.52\n",
      "Num timesteps: 7639000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 137.61\n",
      "Num timesteps: 7640000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 142.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 417      |\n",
      "|    ep_rew_mean        | 143      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76400    |\n",
      "|    time_elapsed       | 5838     |\n",
      "|    total_timesteps    | 7640000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.593   |\n",
      "|    explained_variance | 0.315    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76399    |\n",
      "|    policy_loss        | 5.54     |\n",
      "|    value_loss         | 85.4     |\n",
      "------------------------------------\n",
      "Num timesteps: 7641000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 149.79\n",
      "Num timesteps: 7642000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.20\n",
      "Num timesteps: 7643000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.24\n",
      "Num timesteps: 7644000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.83\n",
      "Num timesteps: 7645000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 159.57\n",
      "Num timesteps: 7646000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 156.88\n",
      "Num timesteps: 7647000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.36\n",
      "Num timesteps: 7648000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 154.22\n",
      "Num timesteps: 7649000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 155.64\n",
      "Num timesteps: 7650000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 407      |\n",
      "|    ep_rew_mean        | 161      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76500    |\n",
      "|    time_elapsed       | 5846     |\n",
      "|    total_timesteps    | 7650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | -0.537   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76499    |\n",
      "|    policy_loss        | -0.761   |\n",
      "|    value_loss         | 8.72     |\n",
      "------------------------------------\n",
      "Num timesteps: 7651000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.60\n",
      "Num timesteps: 7652000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.38\n",
      "Num timesteps: 7653000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.48\n",
      "Num timesteps: 7654000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.50\n",
      "Num timesteps: 7655000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.45\n",
      "Num timesteps: 7656000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.35\n",
      "Num timesteps: 7657000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.89\n",
      "Num timesteps: 7658000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.67\n",
      "Num timesteps: 7659000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.42\n",
      "Num timesteps: 7660000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.61\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 346      |\n",
      "|    ep_rew_mean        | 187      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76600    |\n",
      "|    time_elapsed       | 5853     |\n",
      "|    total_timesteps    | 7660000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.695   |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76599    |\n",
      "|    policy_loss        | -1.4     |\n",
      "|    value_loss         | 6.2      |\n",
      "------------------------------------\n",
      "Num timesteps: 7661000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.89\n",
      "Num timesteps: 7662000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.16\n",
      "Num timesteps: 7663000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.89\n",
      "Num timesteps: 7664000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.20\n",
      "Num timesteps: 7665000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.18\n",
      "Num timesteps: 7666000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.80\n",
      "Num timesteps: 7667000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.44\n",
      "Num timesteps: 7668000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.24\n",
      "Num timesteps: 7669000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.72\n",
      "Num timesteps: 7670000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.80\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 377      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 76700    |\n",
      "|    time_elapsed       | 5862     |\n",
      "|    total_timesteps    | 7670000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.21    |\n",
      "|    explained_variance | 0.28     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76699    |\n",
      "|    policy_loss        | -0.945   |\n",
      "|    value_loss         | 11.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 7671000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.85\n",
      "Num timesteps: 7672000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.78\n",
      "Num timesteps: 7673000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.30\n",
      "Num timesteps: 7674000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.21\n",
      "Num timesteps: 7675000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.44\n",
      "Num timesteps: 7676000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.54\n",
      "Num timesteps: 7677000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.55\n",
      "Num timesteps: 7678000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.64\n",
      "Num timesteps: 7679000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.40\n",
      "Num timesteps: 7680000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.88\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 389      |\n",
      "|    ep_rew_mean        | 189      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 76800    |\n",
      "|    time_elapsed       | 5871     |\n",
      "|    total_timesteps    | 7680000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.749   |\n",
      "|    explained_variance | -0.0636  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76799    |\n",
      "|    policy_loss        | 1.31     |\n",
      "|    value_loss         | 12.2     |\n",
      "------------------------------------\n",
      "Num timesteps: 7681000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.02\n",
      "Num timesteps: 7682000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.59\n",
      "Num timesteps: 7683000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.21\n",
      "Num timesteps: 7684000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.60\n",
      "Num timesteps: 7685000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.79\n",
      "Num timesteps: 7686000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.13\n",
      "Num timesteps: 7687000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.23\n",
      "Num timesteps: 7688000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.47\n",
      "Num timesteps: 7689000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7690000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 430      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 76900    |\n",
      "|    time_elapsed       | 5883     |\n",
      "|    total_timesteps    | 7690000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.362   |\n",
      "|    explained_variance | -0.27    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76899    |\n",
      "|    policy_loss        | 0.31     |\n",
      "|    value_loss         | 714      |\n",
      "------------------------------------\n",
      "Num timesteps: 7691000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.10\n",
      "Num timesteps: 7692000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.52\n",
      "Num timesteps: 7693000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.85\n",
      "Num timesteps: 7694000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.15\n",
      "Num timesteps: 7695000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.50\n",
      "Num timesteps: 7696000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.69\n",
      "Num timesteps: 7697000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.76\n",
      "Num timesteps: 7698000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.71\n",
      "Num timesteps: 7699000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.34\n",
      "Num timesteps: 7700000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.55\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 444      |\n",
      "|    ep_rew_mean        | 165      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 77000    |\n",
      "|    time_elapsed       | 5891     |\n",
      "|    total_timesteps    | 7700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 76999    |\n",
      "|    policy_loss        | -34.9    |\n",
      "|    value_loss         | 4.52e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7701000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.13\n",
      "Num timesteps: 7702000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.60\n",
      "Num timesteps: 7703000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.08\n",
      "Num timesteps: 7704000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.78\n",
      "Num timesteps: 7705000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.13\n",
      "Num timesteps: 7706000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.14\n",
      "Num timesteps: 7707000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.57\n",
      "Num timesteps: 7708000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.24\n",
      "Num timesteps: 7709000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.56\n",
      "Num timesteps: 7710000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.90\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 436      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 77100    |\n",
      "|    time_elapsed       | 5899     |\n",
      "|    total_timesteps    | 7710000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.641   |\n",
      "|    explained_variance | 0.203    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77099    |\n",
      "|    policy_loss        | -8.31    |\n",
      "|    value_loss         | 120      |\n",
      "------------------------------------\n",
      "Num timesteps: 7711000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.07\n",
      "Num timesteps: 7712000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.54\n",
      "Num timesteps: 7713000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.65\n",
      "Num timesteps: 7714000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.64\n",
      "Num timesteps: 7715000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.41\n",
      "Num timesteps: 7716000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.42\n",
      "Num timesteps: 7717000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.83\n",
      "Num timesteps: 7718000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.05\n",
      "Num timesteps: 7719000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.08\n",
      "Num timesteps: 7720000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.75\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 456      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 77200    |\n",
      "|    time_elapsed       | 5909     |\n",
      "|    total_timesteps    | 7720000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.841   |\n",
      "|    explained_variance | 0.182    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77199    |\n",
      "|    policy_loss        | -0.224   |\n",
      "|    value_loss         | 2.34     |\n",
      "------------------------------------\n",
      "Num timesteps: 7721000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.05\n",
      "Num timesteps: 7722000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.34\n",
      "Num timesteps: 7723000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.32\n",
      "Num timesteps: 7724000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.44\n",
      "Num timesteps: 7725000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.21\n",
      "Num timesteps: 7726000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 162.35\n",
      "Num timesteps: 7727000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 161.72\n",
      "Num timesteps: 7728000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.21\n",
      "Num timesteps: 7729000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.25\n",
      "Num timesteps: 7730000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 429      |\n",
      "|    ep_rew_mean        | 179      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 77300    |\n",
      "|    time_elapsed       | 5920     |\n",
      "|    total_timesteps    | 7730000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.722   |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77299    |\n",
      "|    policy_loss        | 0.346    |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "Num timesteps: 7731000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.31\n",
      "Num timesteps: 7732000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.04\n",
      "Num timesteps: 7733000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.32\n",
      "Num timesteps: 7734000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.41\n",
      "Num timesteps: 7735000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.34\n",
      "Num timesteps: 7736000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.76\n",
      "Num timesteps: 7737000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.24\n",
      "Num timesteps: 7738000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.34\n",
      "Num timesteps: 7739000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.29\n",
      "Num timesteps: 7740000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.67\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 455      |\n",
      "|    ep_rew_mean        | 176      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 77400    |\n",
      "|    time_elapsed       | 5929     |\n",
      "|    total_timesteps    | 7740000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.818   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77399    |\n",
      "|    policy_loss        | 0.236    |\n",
      "|    value_loss         | 2.43     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7741000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.26\n",
      "Num timesteps: 7742000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.28\n",
      "Num timesteps: 7743000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.72\n",
      "Num timesteps: 7744000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.03\n",
      "Num timesteps: 7745000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.91\n",
      "Num timesteps: 7746000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.19\n",
      "Num timesteps: 7747000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.63\n",
      "Num timesteps: 7748000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.11\n",
      "Num timesteps: 7749000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.80\n",
      "Num timesteps: 7750000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 471      |\n",
      "|    ep_rew_mean        | 174      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 77500    |\n",
      "|    time_elapsed       | 5938     |\n",
      "|    total_timesteps    | 7750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.307   |\n",
      "|    explained_variance | -0.133   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77499    |\n",
      "|    policy_loss        | 0.112    |\n",
      "|    value_loss         | 0.333    |\n",
      "------------------------------------\n",
      "Num timesteps: 7751000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.00\n",
      "Num timesteps: 7752000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.54\n",
      "Num timesteps: 7753000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.48\n",
      "Num timesteps: 7754000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.49\n",
      "Num timesteps: 7755000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.99\n",
      "Num timesteps: 7756000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.01\n",
      "Num timesteps: 7757000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.65\n",
      "Num timesteps: 7758000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 167.82\n",
      "Num timesteps: 7759000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.80\n",
      "Num timesteps: 7760000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.85\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 481      |\n",
      "|    ep_rew_mean        | 169      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 77600    |\n",
      "|    time_elapsed       | 5947     |\n",
      "|    total_timesteps    | 7760000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.862   |\n",
      "|    explained_variance | -0.305   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77599    |\n",
      "|    policy_loss        | -7.23    |\n",
      "|    value_loss         | 94.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 7761000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.31\n",
      "Num timesteps: 7762000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.75\n",
      "Num timesteps: 7763000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.82\n",
      "Num timesteps: 7764000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.68\n",
      "Num timesteps: 7765000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.88\n",
      "Num timesteps: 7766000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.75\n",
      "Num timesteps: 7767000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.01\n",
      "Num timesteps: 7768000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.26\n",
      "Num timesteps: 7769000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.80\n",
      "Num timesteps: 7770000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.52\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 465      |\n",
      "|    ep_rew_mean        | 170      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 77700    |\n",
      "|    time_elapsed       | 5959     |\n",
      "|    total_timesteps    | 7770000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.591   |\n",
      "|    explained_variance | -13.1    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77699    |\n",
      "|    policy_loss        | -0.841   |\n",
      "|    value_loss         | 78.9     |\n",
      "------------------------------------\n",
      "Num timesteps: 7771000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.06\n",
      "Num timesteps: 7772000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.88\n",
      "Num timesteps: 7773000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.55\n",
      "Num timesteps: 7774000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.58\n",
      "Num timesteps: 7775000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.11\n",
      "Num timesteps: 7776000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.49\n",
      "Num timesteps: 7777000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 165.16\n",
      "Num timesteps: 7778000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.50\n",
      "Num timesteps: 7779000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.49\n",
      "Num timesteps: 7780000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 163.84\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 485      |\n",
      "|    ep_rew_mean        | 164      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 77800    |\n",
      "|    time_elapsed       | 5969     |\n",
      "|    total_timesteps    | 7780000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.437   |\n",
      "|    explained_variance | -19.3    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77799    |\n",
      "|    policy_loss        | 15.7     |\n",
      "|    value_loss         | 2.48e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7781000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.77\n",
      "Num timesteps: 7782000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 164.20\n",
      "Num timesteps: 7783000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.40\n",
      "Num timesteps: 7784000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.67\n",
      "Num timesteps: 7785000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.64\n",
      "Num timesteps: 7786000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.58\n",
      "Num timesteps: 7787000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.64\n",
      "Num timesteps: 7788000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.56\n",
      "Num timesteps: 7789000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.00\n",
      "Num timesteps: 7790000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.17\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 436      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 77900    |\n",
      "|    time_elapsed       | 5976     |\n",
      "|    total_timesteps    | 7790000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0541  |\n",
      "|    explained_variance | -2.36    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77899    |\n",
      "|    policy_loss        | 0.132    |\n",
      "|    value_loss         | 4.43e+03 |\n",
      "------------------------------------\n",
      "Num timesteps: 7791000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.27\n",
      "Num timesteps: 7792000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.95\n",
      "Num timesteps: 7793000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.08\n",
      "Num timesteps: 7794000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.67\n",
      "Num timesteps: 7795000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.76\n",
      "Num timesteps: 7796000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.38\n",
      "Num timesteps: 7797000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.32\n",
      "Num timesteps: 7798000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.37\n",
      "Num timesteps: 7799000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7800000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.09\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 431      |\n",
      "|    ep_rew_mean        | 175      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 78000    |\n",
      "|    time_elapsed       | 5984     |\n",
      "|    total_timesteps    | 7800000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | -1.07    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 77999    |\n",
      "|    policy_loss        | -3.92    |\n",
      "|    value_loss         | 67.5     |\n",
      "------------------------------------\n",
      "Num timesteps: 7801000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.51\n",
      "Num timesteps: 7802000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.57\n",
      "Num timesteps: 7803000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.52\n",
      "Num timesteps: 7804000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.99\n",
      "Num timesteps: 7805000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.12\n",
      "Num timesteps: 7806000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.97\n",
      "Num timesteps: 7807000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.07\n",
      "Num timesteps: 7808000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.38\n",
      "Num timesteps: 7809000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.51\n",
      "Num timesteps: 7810000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.71\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 423      |\n",
      "|    ep_rew_mean        | 173      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 78100    |\n",
      "|    time_elapsed       | 5993     |\n",
      "|    total_timesteps    | 7810000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | 0.074    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78099    |\n",
      "|    policy_loss        | 0.484    |\n",
      "|    value_loss         | 3.78     |\n",
      "------------------------------------\n",
      "Num timesteps: 7811000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.88\n",
      "Num timesteps: 7812000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 175.81\n",
      "Num timesteps: 7813000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.91\n",
      "Num timesteps: 7814000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.31\n",
      "Num timesteps: 7815000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.78\n",
      "Num timesteps: 7816000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.72\n",
      "Num timesteps: 7817000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.50\n",
      "Num timesteps: 7818000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.54\n",
      "Num timesteps: 7819000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.32\n",
      "Num timesteps: 7820000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.65\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 421      |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1302     |\n",
      "|    iterations         | 78200    |\n",
      "|    time_elapsed       | 6002     |\n",
      "|    total_timesteps    | 7820000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.871   |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78199    |\n",
      "|    policy_loss        | -0.571   |\n",
      "|    value_loss         | 2.52     |\n",
      "------------------------------------\n",
      "Num timesteps: 7821000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.79\n",
      "Num timesteps: 7822000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.05\n",
      "Num timesteps: 7823000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.36\n",
      "Num timesteps: 7824000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.40\n",
      "Num timesteps: 7825000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.37\n",
      "Num timesteps: 7826000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.13\n",
      "Num timesteps: 7827000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.46\n",
      "Num timesteps: 7828000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.67\n",
      "Num timesteps: 7829000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.69\n",
      "Num timesteps: 7830000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.01\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 438      |\n",
      "|    ep_rew_mean        | 194      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1302     |\n",
      "|    iterations         | 78300    |\n",
      "|    time_elapsed       | 6011     |\n",
      "|    total_timesteps    | 7830000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.757   |\n",
      "|    explained_variance | -0.0335  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78299    |\n",
      "|    policy_loss        | 14.1     |\n",
      "|    value_loss         | 499      |\n",
      "------------------------------------\n",
      "Num timesteps: 7831000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.44\n",
      "Num timesteps: 7832000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.79\n",
      "Num timesteps: 7833000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.78\n",
      "Num timesteps: 7834000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.86\n",
      "Num timesteps: 7835000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.60\n",
      "Num timesteps: 7836000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.87\n",
      "Num timesteps: 7837000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.06\n",
      "Num timesteps: 7838000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.10\n",
      "Num timesteps: 7839000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.75\n",
      "Num timesteps: 7840000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.62\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 452      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1302     |\n",
      "|    iterations         | 78400    |\n",
      "|    time_elapsed       | 6021     |\n",
      "|    total_timesteps    | 7840000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.892   |\n",
      "|    explained_variance | -1.47    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78399    |\n",
      "|    policy_loss        | 1.61     |\n",
      "|    value_loss         | 39.6     |\n",
      "------------------------------------\n",
      "Num timesteps: 7841000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.46\n",
      "Num timesteps: 7842000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.74\n",
      "Num timesteps: 7843000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.32\n",
      "Num timesteps: 7844000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.07\n",
      "Num timesteps: 7845000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.54\n",
      "Num timesteps: 7846000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.69\n",
      "Num timesteps: 7847000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.98\n",
      "Num timesteps: 7848000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.15\n",
      "Num timesteps: 7849000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 203.99\n",
      "Num timesteps: 7850000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.00\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 436      |\n",
      "|    ep_rew_mean        | 200      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 78500    |\n",
      "|    time_elapsed       | 6029     |\n",
      "|    total_timesteps    | 7850000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.598   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78499    |\n",
      "|    policy_loss        | -2.55    |\n",
      "|    value_loss         | 22.1     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7851000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 199.69\n",
      "Num timesteps: 7852000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.06\n",
      "Num timesteps: 7853000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.03\n",
      "Num timesteps: 7854000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.77\n",
      "Num timesteps: 7855000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 202.07\n",
      "Num timesteps: 7856000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.58\n",
      "Num timesteps: 7857000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 199.73\n",
      "Num timesteps: 7858000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 199.88\n",
      "Num timesteps: 7859000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.99\n",
      "Num timesteps: 7860000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.45\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 426      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 78600    |\n",
      "|    time_elapsed       | 6038     |\n",
      "|    total_timesteps    | 7860000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.934   |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78599    |\n",
      "|    policy_loss        | -13.8    |\n",
      "|    value_loss         | 285      |\n",
      "------------------------------------\n",
      "Num timesteps: 7861000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.89\n",
      "Num timesteps: 7862000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.40\n",
      "Num timesteps: 7863000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.09\n",
      "Num timesteps: 7864000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.95\n",
      "Num timesteps: 7865000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.68\n",
      "Num timesteps: 7866000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.76\n",
      "Num timesteps: 7867000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.33\n",
      "Num timesteps: 7868000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.07\n",
      "Num timesteps: 7869000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.73\n",
      "Num timesteps: 7870000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.44\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 434      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 78700    |\n",
      "|    time_elapsed       | 6048     |\n",
      "|    total_timesteps    | 7870000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | -0.00735 |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78699    |\n",
      "|    policy_loss        | -6.51    |\n",
      "|    value_loss         | 359      |\n",
      "------------------------------------\n",
      "Num timesteps: 7871000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.03\n",
      "Num timesteps: 7872000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.62\n",
      "Num timesteps: 7873000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.64\n",
      "Num timesteps: 7874000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 199.98\n",
      "Num timesteps: 7875000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 202.76\n",
      "Num timesteps: 7876000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.73\n",
      "Num timesteps: 7877000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 204.26\n",
      "Num timesteps: 7878000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 206.86\n",
      "Num timesteps: 7879000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 204.17\n",
      "Num timesteps: 7880000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.04\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 415      |\n",
      "|    ep_rew_mean        | 201      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 78800    |\n",
      "|    time_elapsed       | 6055     |\n",
      "|    total_timesteps    | 7880000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.659   |\n",
      "|    explained_variance | -0.107   |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78799    |\n",
      "|    policy_loss        | -1.57    |\n",
      "|    value_loss         | 7.27     |\n",
      "------------------------------------\n",
      "Num timesteps: 7881000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 200.79\n",
      "Num timesteps: 7882000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 201.44\n",
      "Num timesteps: 7883000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.03\n",
      "Num timesteps: 7884000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.23\n",
      "Num timesteps: 7885000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.25\n",
      "Num timesteps: 7886000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.88\n",
      "Num timesteps: 7887000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.74\n",
      "Num timesteps: 7888000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.73\n",
      "Num timesteps: 7889000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.29\n",
      "Num timesteps: 7890000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.41\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 403      |\n",
      "|    ep_rew_mean        | 194      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 78900    |\n",
      "|    time_elapsed       | 6063     |\n",
      "|    total_timesteps    | 7890000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.784   |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78899    |\n",
      "|    policy_loss        | 3.25     |\n",
      "|    value_loss         | 33       |\n",
      "------------------------------------\n",
      "Num timesteps: 7891000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.59\n",
      "Num timesteps: 7892000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.62\n",
      "Num timesteps: 7893000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.79\n",
      "Num timesteps: 7894000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.83\n",
      "Num timesteps: 7895000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.65\n",
      "Num timesteps: 7896000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.49\n",
      "Num timesteps: 7897000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.33\n",
      "Num timesteps: 7898000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.46\n",
      "Num timesteps: 7899000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.45\n",
      "Num timesteps: 7900000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 198.05\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 399      |\n",
      "|    ep_rew_mean        | 198      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 79000    |\n",
      "|    time_elapsed       | 6071     |\n",
      "|    total_timesteps    | 7900000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.344   |\n",
      "|    explained_variance | 0.539    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 78999    |\n",
      "|    policy_loss        | 1.36     |\n",
      "|    value_loss         | 110      |\n",
      "------------------------------------\n",
      "Num timesteps: 7901000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 197.28\n",
      "Num timesteps: 7902000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.54\n",
      "Num timesteps: 7903000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.84\n",
      "Num timesteps: 7904000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.80\n",
      "Num timesteps: 7905000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.80\n",
      "Num timesteps: 7906000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.82\n",
      "Num timesteps: 7907000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.46\n",
      "Num timesteps: 7908000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.81\n",
      "Num timesteps: 7909000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7910000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 392      |\n",
      "|    ep_rew_mean        | 188      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79100    |\n",
      "|    time_elapsed       | 6079     |\n",
      "|    total_timesteps    | 7910000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.259   |\n",
      "|    explained_variance | -1.55    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79099    |\n",
      "|    policy_loss        | 1.87     |\n",
      "|    value_loss         | 753      |\n",
      "------------------------------------\n",
      "Num timesteps: 7911000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.12\n",
      "Num timesteps: 7912000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.31\n",
      "Num timesteps: 7913000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.46\n",
      "Num timesteps: 7914000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 187.67\n",
      "Num timesteps: 7915000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.22\n",
      "Num timesteps: 7916000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.81\n",
      "Num timesteps: 7917000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.58\n",
      "Num timesteps: 7918000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 184.36\n",
      "Num timesteps: 7919000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.16\n",
      "Num timesteps: 7920000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.58\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 404      |\n",
      "|    ep_rew_mean        | 190      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79200    |\n",
      "|    time_elapsed       | 6088     |\n",
      "|    total_timesteps    | 7920000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.457   |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79199    |\n",
      "|    policy_loss        | 0.021    |\n",
      "|    value_loss         | 28       |\n",
      "------------------------------------\n",
      "Num timesteps: 7921000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.43\n",
      "Num timesteps: 7922000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 188.81\n",
      "Num timesteps: 7923000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.56\n",
      "Num timesteps: 7924000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.87\n",
      "Num timesteps: 7925000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.04\n",
      "Num timesteps: 7926000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.53\n",
      "Num timesteps: 7927000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.53\n",
      "Num timesteps: 7928000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.51\n",
      "Num timesteps: 7929000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.08\n",
      "Num timesteps: 7930000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.36\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 409      |\n",
      "|    ep_rew_mean        | 195      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79300    |\n",
      "|    time_elapsed       | 6096     |\n",
      "|    total_timesteps    | 7930000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.724   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79299    |\n",
      "|    policy_loss        | 0.308    |\n",
      "|    value_loss         | 7.73     |\n",
      "------------------------------------\n",
      "Num timesteps: 7931000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 195.23\n",
      "Num timesteps: 7932000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.87\n",
      "Num timesteps: 7933000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.10\n",
      "Num timesteps: 7934000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.50\n",
      "Num timesteps: 7935000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.07\n",
      "Num timesteps: 7936000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.58\n",
      "Num timesteps: 7937000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.03\n",
      "Num timesteps: 7938000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 196.40\n",
      "Num timesteps: 7939000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.15\n",
      "Num timesteps: 7940000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.56\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 419      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79400    |\n",
      "|    time_elapsed       | 6105     |\n",
      "|    total_timesteps    | 7940000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.695   |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79399    |\n",
      "|    policy_loss        | -2.74    |\n",
      "|    value_loss         | 28.8     |\n",
      "------------------------------------\n",
      "Num timesteps: 7941000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.22\n",
      "Num timesteps: 7942000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.10\n",
      "Num timesteps: 7943000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.00\n",
      "Num timesteps: 7944000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.67\n",
      "Num timesteps: 7945000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.13\n",
      "Num timesteps: 7946000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.84\n",
      "Num timesteps: 7947000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.53\n",
      "Num timesteps: 7948000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.88\n",
      "Num timesteps: 7949000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.70\n",
      "Num timesteps: 7950000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.83\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 387      |\n",
      "|    ep_rew_mean        | 192      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79500    |\n",
      "|    time_elapsed       | 6112     |\n",
      "|    total_timesteps    | 7950000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.416   |\n",
      "|    explained_variance | -1.61    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79499    |\n",
      "|    policy_loss        | 4.63     |\n",
      "|    value_loss         | 326      |\n",
      "------------------------------------\n",
      "Num timesteps: 7951000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.70\n",
      "Num timesteps: 7952000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.42\n",
      "Num timesteps: 7953000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.65\n",
      "Num timesteps: 7954000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.69\n",
      "Num timesteps: 7955000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.87\n",
      "Num timesteps: 7956000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 190.94\n",
      "Num timesteps: 7957000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 191.97\n",
      "Num timesteps: 7958000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 194.01\n",
      "Num timesteps: 7959000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.56\n",
      "Num timesteps: 7960000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 193.27\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 400      |\n",
      "|    ep_rew_mean        | 193      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79600    |\n",
      "|    time_elapsed       | 6121     |\n",
      "|    total_timesteps    | 7960000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.877   |\n",
      "|    explained_variance | 0.338    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79599    |\n",
      "|    policy_loss        | 3.8      |\n",
      "|    value_loss         | 26.7     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 7961000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.23\n",
      "Num timesteps: 7962000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 192.02\n",
      "Num timesteps: 7963000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 189.94\n",
      "Num timesteps: 7964000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.81\n",
      "Num timesteps: 7965000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 186.43\n",
      "Num timesteps: 7966000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 185.33\n",
      "Num timesteps: 7967000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.25\n",
      "Num timesteps: 7968000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.94\n",
      "Num timesteps: 7969000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.44\n",
      "Num timesteps: 7970000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.37\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 409      |\n",
      "|    ep_rew_mean        | 182      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 79700    |\n",
      "|    time_elapsed       | 6129     |\n",
      "|    total_timesteps    | 7970000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.647   |\n",
      "|    explained_variance | -0.27    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79699    |\n",
      "|    policy_loss        | 17.5     |\n",
      "|    value_loss         | 645      |\n",
      "------------------------------------\n",
      "Num timesteps: 7971000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.52\n",
      "Num timesteps: 7972000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 183.01\n",
      "Num timesteps: 7973000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.32\n",
      "Num timesteps: 7974000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 181.78\n",
      "Num timesteps: 7975000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.53\n",
      "Num timesteps: 7976000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 182.05\n",
      "Num timesteps: 7977000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.72\n",
      "Num timesteps: 7978000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 180.53\n",
      "Num timesteps: 7979000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.00\n",
      "Num timesteps: 7980000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 178.38\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 422      |\n",
      "|    ep_rew_mean        | 178      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 79800    |\n",
      "|    time_elapsed       | 6139     |\n",
      "|    total_timesteps    | 7980000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.63    |\n",
      "|    explained_variance | 0.547    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79799    |\n",
      "|    policy_loss        | -0.547   |\n",
      "|    value_loss         | 1.13     |\n",
      "------------------------------------\n",
      "Num timesteps: 7981000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.23\n",
      "Num timesteps: 7982000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.26\n",
      "Num timesteps: 7983000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.13\n",
      "Num timesteps: 7984000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 169.26\n",
      "Num timesteps: 7985000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 166.03\n",
      "Num timesteps: 7986000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.68\n",
      "Num timesteps: 7987000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 168.13\n",
      "Num timesteps: 7988000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.69\n",
      "Num timesteps: 7989000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.08\n",
      "Num timesteps: 7990000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.07\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 439      |\n",
      "|    ep_rew_mean        | 172      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 79900    |\n",
      "|    time_elapsed       | 6148     |\n",
      "|    total_timesteps    | 7990000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | -4.27    |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79899    |\n",
      "|    policy_loss        | 0.649    |\n",
      "|    value_loss         | 2.1e+03  |\n",
      "------------------------------------\n",
      "Num timesteps: 7991000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.16\n",
      "Num timesteps: 7992000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 172.45\n",
      "Num timesteps: 7993000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 174.53\n",
      "Num timesteps: 7994000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 177.03\n",
      "Num timesteps: 7995000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 179.22\n",
      "Num timesteps: 7996000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 176.35\n",
      "Num timesteps: 7997000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.82\n",
      "Num timesteps: 7998000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 173.22\n",
      "Num timesteps: 7999000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 171.93\n",
      "Num timesteps: 8000000\n",
      "Best mean reward: 223.66 - Last mean reward per episode: 170.81\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 464      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 80000    |\n",
      "|    time_elapsed       | 6157     |\n",
      "|    total_timesteps    | 8000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.559   |\n",
      "|    explained_variance | -0.0562  |\n",
      "|    learning_rate      | 0.0003   |\n",
      "|    n_updates          | 79999    |\n",
      "|    policy_loss        | 0.35     |\n",
      "|    value_loss         | 0.578    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x10391a550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_A2C_8/\")\n",
    "\n",
    "policy = \"MlpPolicy\"\n",
    "n_steps = 100\n",
    "learning_rate = 0.0003\n",
    "batch_size = 256\n",
    "n_epochs = 10\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=\"log_dir_A2C_8/\")\n",
    "# instantiate the agent \n",
    "model = A2C(policy, env, learning_rate = learning_rate, n_steps = n_steps,tensorboard_log=\"./TensorBoardLog/\", verbose=1)\n",
    "# train the agent\n",
    "model.learn(total_timesteps=8000000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dc8fa",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366b80a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=100)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "    fig = plt.figure(title, figsize=(12,5))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed A2C after 80,00,000 Timesteps\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(\"log_dir_A2C_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f2a81",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815393a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=1,monitor_dir=\"log_dir_A2C_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63611e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(path=\"log_dir_A2C_8/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06e1a3",
   "metadata": {},
   "source": [
    "#### Stable Baseline 3 Evaluation Function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4fd326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean & Std Reward after 100 max run is 263.24761090000004 & 16.51837880754401\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env,n_eval_episodes=10, render=True, deterministic=True)\n",
    "print(\"Mean & Std Reward after {} max run is {} & {}\".format(10,mean_reward, std_reward)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c5168",
   "metadata": {},
   "source": [
    "# GIF of a Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60cc63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "model = A2C.load(path=\"log_dir_A2C_8/best_model.zip\")\n",
    "\n",
    "images = []\n",
    "obs = env.reset()\n",
    "img = env.render(mode=\"rgb_array\")\n",
    "for i in range(350):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _ ,_ = env.step(action)\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "\n",
    "imageio.mimsave(\"lunar lander_A2C_8.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc060b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
